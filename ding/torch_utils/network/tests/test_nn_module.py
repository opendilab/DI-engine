import pytest
import torch
from torch.testing import assert_allclose

from ding.torch_utils import build_activation
from ding.torch_utils.network.nn_module import MLP, conv1d_block, conv2d_block, fc_block, deconv2d_block, \
    ChannelShuffle, one_hot, NearestUpsample, BilinearUpsample, binary_encode, weight_init_, NaiveFlatten, \
    normed_linear, normed_conv2d

batch_size = 2
in_channels = 2
hidden_channels = 3
out_channels = 3
H = 2
W = 3
kernel_size = 2
stride = 1
padding = 0
dilation = 1
groups = 1
init_type = ['xavier', 'kaiming', 'orthogonal']
act = build_activation('relu')
norm_type = 'BN'


@pytest.mark.unittest
class TestNnModule:

    def run_model(self, input, model):
        output = model(input)
        loss = output.mean()
        loss.backward()
        assert isinstance(
            input.grad,
            torch.Tensor,
        )
        return output

    def test_weight_init(self):
        weight = torch.zeros(2, 3)
        for init_type in ['xavier', 'orthogonal']:
            weight_init_(weight, init_type)
        for act in [torch.nn.LeakyReLU(), torch.nn.ReLU()]:
            weight_init_(weight, 'kaiming', act)
        with pytest.raises(KeyError):
            weight_init_(weight, 'xxx')

    def test_mlp(self):
        layer_num = 3
        input_tensor = torch.rand(batch_size, in_channels).requires_grad_(True)

        for output_activation in [True, False]:
            for output_norm in [True, False]:
                for activation in [torch.nn.ReLU(), torch.nn.LeakyReLU(), torch.nn.Tanh(), None]:
                    for norm_type in ["LN", "BN", None]:
                        # Test case 1: MLP without last linear layer initialized to 0.
                        model = MLP(
                            in_channels,
                            hidden_channels,
                            out_channels,
                            layer_num,
                            activation=activation,
                            norm_type=norm_type,
                            output_activation=output_activation,
                            output_norm=output_norm
                        )
                        output_tensor = self.run_model(input_tensor, model)
                        assert output_tensor.shape == (batch_size, out_channels)

                        # Test case 2: MLP with last linear layer initialized to 0.
                        model = MLP(
                            in_channels,
                            hidden_channels,
                            out_channels,
                            layer_num,
                            activation=activation,
                            norm_type=norm_type,
                            output_activation=output_activation,
                            output_norm=output_norm,
                            last_linear_layer_init_zero=True
                        )
                        output_tensor = self.run_model(input_tensor, model)
                        assert output_tensor.shape == (batch_size, out_channels)
                        last_linear_layer = None
                        for layer in reversed(model):
                            if isinstance(layer, torch.nn.Linear):
                                last_linear_layer = layer
                                break
                        assert_allclose(last_linear_layer.weight, torch.zeros_like(last_linear_layer.weight))
                        assert_allclose(last_linear_layer.bias, torch.zeros_like(last_linear_layer.bias))

    def test_conv1d_block(self):
        length = 2
        input = torch.rand(batch_size, in_channels, length).requires_grad_(True)
        block = conv1d_block(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            activation=act,
            norm_type=norm_type
        )
        output = self.run_model(input, block)
        output_length = (length - kernel_size + 2 * padding // stride) + 1
        assert output.shape == (batch_size, out_channels, output_length)

    def test_conv2d_block(self):
        input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True)
        for pad_type in ['zero', 'reflect', 'replication']:
            block = conv2d_block(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                dilation=dilation,
                groups=groups,
                pad_type=pad_type,
                activation=act,
                norm_type=norm_type
            )
            output = self.run_model(input, block)
            output_H = (H - kernel_size + 2 * padding // stride) + 1
            output_W = (W - kernel_size + 2 * padding // stride) + 1
            assert output.shape == (batch_size, out_channels, output_H, output_W)

    def test_deconv2d_block(self):
        input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True)
        output_padding = 0
        block = deconv2d_block(
            in_channels,
            out_channels,
            kernel_size,
            stride=1,
            padding=0,
            output_padding=output_padding,
            groups=1,
            activation=act,
            norm_type=norm_type
        )
        output = self.run_model(input, block)
        output_H = (H - 1) * stride + output_padding - 2 * padding + kernel_size
        output_W = (W - 1) * stride + output_padding - 2 * padding + kernel_size
        assert output.shape == (batch_size, out_channels, output_H, output_W)

    def test_fc_block(self):
        input = torch.rand(batch_size, in_channels).requires_grad_(True)
        for use_dropout in [True, False]:
            block = fc_block(
                in_channels,
                out_channels,
                activation=act,
                norm_type=norm_type,
                use_dropout=use_dropout,
                dropout_probability=0.5
            )
            output = self.run_model(input, block)
            assert output.shape == (batch_size, out_channels)

    def test_normed_linear(self):
        input = torch.rand(batch_size, in_channels).requires_grad_(True)
        block = normed_linear(in_channels, out_channels, scale=1)
        r = block.weight.norm(dim=None, keepdim=False) * block.weight.norm(dim=None, keepdim=False)
        assert r.item() < out_channels + 0.01
        assert r.item() > out_channels - 0.01
        output = self.run_model(input, block)
        assert output.shape == (batch_size, out_channels)

    def test_normed_conv2d(self):
        input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True)
        block = normed_conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            scale=1
        )
        r = block.weight.norm(dim=(1, 2, 3), p=2, keepdim=True)[0, 0, 0, 0]
        assert r.item() < 1.01
        assert r.item() > 0.99
        output = self.run_model(input, block)
        output_H = (H - kernel_size + 2 * padding // stride) + 1
        output_W = (W - kernel_size + 2 * padding // stride) + 1
        assert output.shape == (batch_size, out_channels, output_H, output_W)

    def test_channel_shuffle(self):
        group_num = 2
        input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True)
        channel_shuffle = ChannelShuffle(group_num)
        output = self.run_model(input, channel_shuffle)
        assert output.shape == input.shape

    def test_one_hot(self):
        M = 2
        N = 2
        max_num = 3
        input = torch.ones(M, N).long()
        output = one_hot(input, max_num, num_first=False)
        assert output.sum() == input.numel()
        assert output.shape == (M, N, max_num)
        output = one_hot(input, max_num, num_first=True)
        assert output.sum() == input.numel()
        assert output.shape == (max_num, M, N)
        with pytest.raises(RuntimeError):
            _ = one_hot(torch.arange(0, max_num), max_num - 1)

    def test_upsample(self):
        scale_factor = 2
        input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True)
        model = NearestUpsample(scale_factor)
        output = self.run_model(input, model)
        assert output.shape == (batch_size, in_channels, 2 * H, 2 * W)
        model = BilinearUpsample(scale_factor)
        output = self.run_model(input, model)
        assert output.shape == (batch_size, in_channels, 2 * H, 2 * W)

    def test_binary_encode(self):
        input = torch.tensor([4])
        max_val = torch.tensor(8)
        output = binary_encode(input, max_val)
        assert torch.equal(output, torch.tensor([[0, 1, 0, 0]]))

    @pytest.mark.tmp
    def test_flatten(self):
        inputs = torch.randn(4, 3, 8, 8)
        model1 = NaiveFlatten()
        output1 = model1(inputs)
        assert output1.shape == (4, 3 * 8 * 8)
        model2 = NaiveFlatten(1, 2)
        output2 = model2(inputs)
        assert output2.shape == (4, 3 * 8, 8)
        model3 = NaiveFlatten(1, 3)
        output3 = model2(inputs)
        assert output1.shape == (4, 3 * 8 * 8)
