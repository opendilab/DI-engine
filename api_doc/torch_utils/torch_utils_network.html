

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>network.activation &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Utils" href="../utils/index.html" />
    <link rel="prev" title="loss.cross_entropy_loss" href="torch_utils_loss.html" />
    <link href="../../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hands_on/index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Doc</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../config/index.html">Config</a></li>
<li class="toctree-l2"><a class="reference internal" href="../env/index.html">Env</a></li>
<li class="toctree-l2"><a class="reference internal" href="../policy/index.html">Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model/index.html">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reward_model/index.html">Reward Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../league/index.html">League</a></li>
<li class="toctree-l2"><a class="reference internal" href="../framework/index.html">Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/learner/index.html">Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/collector/index.html">Collector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/replay_buffer/index.html">Replay Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/coordinator/index.html">Coordinator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/buffer/index.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rl_utils/index.html">RL Utils</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Torch Utils</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="checkpoint_helper.html">checkpoint_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_helper.html">data_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="distribution.html">distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="metric.html">metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn_test_helper.html">nn_test_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="optimizer_helper.html">optimizer_helper</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch_utils_loss.html">loss.cross_entropy_loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch_utils_loss.html#loss-multi-logits-loss">loss.multi_logits_loss</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">network.activation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#glu">GLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.activation.build_activation">build_activation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-nn-module">network.nn_module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.weight_init_">weight_init</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.sequential_pack">sequential_pack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.conv1d_block">conv1d_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.conv2d_block">conv2d_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.deconv2d_block">deconv2d_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.fc_block">fc_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.MLP">MLP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.one_hot">one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.binary_encode">binary_encode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.nn_module.noise_block">noise_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#channelshuffle">ChannelShuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nearestupsample">NearestUpsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bilinearupsample">BilinearUpsample</a></li>
<li class="toctree-l4"><a class="reference internal" href="#noiselinearlayer">NoiseLinearLayer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-normalization">network.normalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.normalization.build_normalization">build_normalization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-res-block">network.res_block</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resblock">ResBlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resfcblock">ResFCBlock</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-rnn">network.rnn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lstmforwardwrapper">LSTMForwardWrapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lstm">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pytorchlstm">PytorchLSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-ding.torch_utils.network.rnn.get_lstm">get_lstm</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-scatter-connection">network.scatter_connection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scatterconnection">ScatterConnection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-soft-argmax">network.soft_argmax</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#softargmax">SoftArgmax</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#network-transformer">network.transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transformerlayer">TransformerLayer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transformer">Transformer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../utils/index.html">Utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interaction/index.html">Interaction</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">API Doc</a> &raquo;</li>
        
          <li><a href="index.html">Torch Utils</a> &raquo;</li>
        
      <li>network.activation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/api_doc/torch_utils/torch_utils_network.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="network-activation">
<h1>network.activation<a class="headerlink" href="#network-activation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="glu">
<h2>GLU<a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">GLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'fc'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/activation.html#GLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GLU" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Gating Linear Unit.
This class does a thing like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inputs: input, context, output_size</span>
<span class="c1"># The gate value is a learnt function of the input.</span>
<span class="n">gate</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">)(</span><span class="n">context</span><span class="p">))</span>
<span class="c1"># Gate the input and return an output of desired size.</span>
<span class="n">gated_input</span> <span class="o">=</span> <span class="n">gate</span> <span class="o">*</span> <span class="nb">input</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">output_size</span><span class="p">)(</span><span class="n">gated_input</span><span class="p">)</span>
<span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</dd>
<dt>Interfaces:</dt><dd><p>forward</p>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This module also supports 2D convolution, in which case, the input and context must have the same shape.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/activation.html#GLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return GLU computed tensor</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) : the input tensor</p></li>
<li><p>context (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) : the context tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the computed tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ding.torch_utils.network.activation.build_activation">
<span id="build-activation"></span><h2>build_activation<a class="headerlink" href="#module-ding.torch_utils.network.activation.build_activation" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Return the activation module according to the given type.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>actvation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the type of activation module, now supports [‘relu’, ‘glu’, ‘prelu’]</p></li>
<li><p>inplace (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): can optionally do the operation in-place in relu. Default <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>act_func (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.module</span></code>): the corresponding activation module</p></li>
</ul>
</dd>
</dl>
</div>
</div>
<div class="section" id="network-nn-module">
<h1>network.nn_module<a class="headerlink" href="#network-nn-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-ding.torch_utils.network.nn_module.weight_init_">
<span id="weight-init"></span><h2>weight_init<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.weight_init_" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Init weight according to the specified type.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the weight that needed to init</p></li>
<li><p>init_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the type of init to implement, supports [“xavier”, “kaiming”, “orthogonal”]</p></li>
<li><dl class="simple">
<dt>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the activation function name, recommend that use only with </dt><dd><p>[‘relu’, ‘leaky_relu’].</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.sequential_pack">
<span id="sequential-pack"></span><h2>sequential_pack<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.sequential_pack" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Pack the layers in the input list to a <cite>nn.Sequential</cite> module.
If there is a convolutional layer in module, an extra attribute <cite>out_channels</cite> will be added
to the module and set to the out_channel of the conv layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): the input list</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>seq (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): packed sequential container</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.conv1d_block">
<span id="conv1d-block"></span><h2>conv1d_block<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.conv1d_block" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Create a 1-dim convlution layer with activation and normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Stride of the convolution</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Zero-padding added to both sides of the input</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Spacing between kernel elements</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of blocked connections from input channels to output channels</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the 1 dim convlution layer</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Conv1d (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d</a>)</p>
</div>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.conv2d_block">
<span id="conv2d-block"></span><h2>conv2d_block<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.conv2d_block" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Create a 2-dim convlution layer with activation and normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Stride of the convolution</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Zero-padding added to both sides of the input</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Spacing between kernel elements</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of blocked connections from input channels to output channels</p></li>
<li><p>pad_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the way to add padding, include [‘zero’, ‘reflect’, ‘replicate’], default: None</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization, default set to None, now support [‘BN’, ‘IN’, ‘SyncBN’]</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the 2 dim convlution layer</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Conv2d (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a>)</p>
</div>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.deconv2d_block">
<span id="deconv2d-block"></span><h2>deconv2d_block<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.deconv2d_block" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Create a 2-dim transopse convlution layer with activation and normalization</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Stride of the convolution</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Zero-padding added to both sides of the input</p></li>
<li><p>pad_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the way to add padding, include [‘zero’, ‘reflect’, ‘replicate’]</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the 2-dim </dt><dd><p>transpose convlution layer</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ConvTranspose2d (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html">https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html</a>)</p>
</div>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.fc_block">
<span id="fc-block"></span><h2>fc_block<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.fc_block" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Create a fully-connected block with activation, normalization and dropout.
Optional normalization can be done to the dim 1 (across the channels)
x -&gt; fc -&gt; norm -&gt; act -&gt; dropout -&gt; out</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) : whether to use dropout in the fully-connected block</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) : probability of an element to be zeroed in the dropout. Default: 0.5</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the fully-connected block</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you can refer to nn.linear (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">https://pytorch.org/docs/master/generated/torch.nn.Linear.html</a>)</p>
</div>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.MLP">
<span id="mlp"></span><h2>MLP<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.MLP" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>create a multi-layer perceptron using fully-connected blocks with activation, normalization and dropout,
optional normalization can be done to the dim 1 (across the channels)
x -&gt; fc -&gt; norm -&gt; act -&gt; dropout -&gt; out</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>hidden_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the hidden tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of layers</p></li>
<li><p>layer_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): layer function</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use dropout in the fully-connected block</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): probability of an element to be zeroed in the dropout. Default: 0.5</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the fully-connected block</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you can refer to nn.linear (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">https://pytorch.org/docs/master/generated/torch.nn.Linear.html</a>)</p>
</div>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.one_hot">
<span id="one-hot"></span><h2>one_hot<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.one_hot" title="Permalink to this headline">¶</a></h2>
<dl>
<dt>Overview:</dt><dd><p>Convert a <code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> to one hot encoding.
This implementation can be slightly faster than <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.one_hot</span></code></p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): each element contains the state to be encoded, the range should be [0, num-1]</p></li>
<li><p>num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): number of states of the one hot encoding</p></li>
<li><dl class="simple">
<dt>num_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): If <code class="docutils literal notranslate"><span class="pre">num_first</span></code> is False, the one hot encoding is added as the last; </dt><dd><p>Otherwise as the first dimension.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>one_hot (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>)</p></li>
</ul>
</dd>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">one_hot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 1.],</span>
<span class="go">         [0., 0., 1.]],</span>
<span class="go">        [[0., 0., 1.],</span>
<span class="go">         [0., 0., 1.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one_hot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span><span class="mi">3</span><span class="p">,</span><span class="n">num_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[[0., 0.], [1., 0.]],</span>
<span class="go">        [[0., 1.], [0., 0.]],</span>
<span class="go">        [[1., 0.], [0., 1.]]])</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.binary_encode">
<span id="binary-encode"></span><h2>binary_encode<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.binary_encode" title="Permalink to this headline">¶</a></h2>
<dl>
<dt>Overview:</dt><dd><p>Convert elements in a tensor to its binary representation</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>y (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the tensor to be transferred into its binary representation</p></li>
<li><p>max_val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the max value of the elements in tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>binary (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor in its binary representation</p></li>
</ul>
</dd>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_encode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="go">tensor([[0, 0, 1, 1],[0, 0, 1, 0]])</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-ding.torch_utils.network.nn_module.noise_block">
<span id="noise-block"></span><h2>noise_block<a class="headerlink" href="#module-ding.torch_utils.network.nn_module.noise_block" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Create a fully-connected block with activation, normalization and dropout
Optional normalization can be done to the dim 1 (across the channels)
x -&gt; fc -&gt; norm -&gt; act -&gt; dropout -&gt; out</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the optional activation function</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normalization</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>) : whether to use dropout in the fully-connected block</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) : probability of an element to be zeroed in the dropout. Default: 0.5</p></li>
<li><p>simga0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the sigma0 is the defalut noise volumn when init NoiseLinearLayer</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): a sequential list containing the torch layers of the fully-connected block</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you can refer to nn.linear (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">https://pytorch.org/docs/master/generated/torch.nn.Linear.html</a>)</p>
</div>
</div>
<div class="section" id="channelshuffle">
<h2>ChannelShuffle<a class="headerlink" href="#channelshuffle" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">ChannelShuffle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#ChannelShuffle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Apply channelShuffle to the input tensor</p>
</dd>
<dt>Interface:</dt><dd><p>forward</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can see the original paper shuffle net in <a class="reference external" href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#ChannelShuffle.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the upsampled input</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the shuffled input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="nearestupsample">
<h2>NearestUpsample<a class="headerlink" href="#nearestupsample" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">NearestUpsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NearestUpsample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Upsamples the input to the given member varible scale_factor using mode nearest</p>
</dd>
<dt>Interface:</dt><dd><p>forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NearestUpsample.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the upsampled input</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>upsample(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the upsampled input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bilinearupsample">
<h2>BilinearUpsample<a class="headerlink" href="#bilinearupsample" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">BilinearUpsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#BilinearUpsample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Upsamples the input to the given member varible scale_factor using mode biliner</p>
</dd>
<dt>Interface:</dt><dd><p>forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#BilinearUpsample.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the upsampled input</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>upsample(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the upsampled input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="noiselinearlayer">
<h2>NoiseLinearLayer<a class="headerlink" href="#noiselinearlayer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">NoiseLinearLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma0</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Linear layer with random noise.</p>
</dd>
<dt>Interface:</dt><dd><p>reset_noise, reset_parameters, forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Layer forward with noise.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the output with noise</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_noise">
<span class="sig-name descname"><span class="pre">reset_noise</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.reset_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_noise" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset noise settinngs in the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset parameters in the layer.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="network-normalization">
<h1>network.normalization<a class="headerlink" href="#network-normalization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-ding.torch_utils.network.normalization.build_normalization">
<span id="build-normalization"></span><h2>build_normalization<a class="headerlink" href="#module-ding.torch_utils.network.normalization.build_normalization" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Build the corresponding normalization module</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normaliztion, now support [‘BN’, ‘IN’, ‘SyncBN’, ‘AdaptiveIN’]</p></li>
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dimension of the normalization, when norm_type is in [BN, IN]</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>norm_func (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the corresponding batch normalization function</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For beginers, you can refer to &lt;<a class="reference external" href="https://zhuanlan.zhihu.com/p/34879333">https://zhuanlan.zhihu.com/p/34879333</a>&gt; to learn more about batch normalization.</p>
</div>
</div>
</div>
<div class="section" id="network-res-block">
<h1>network.res_block<a class="headerlink" href="#network-res-block" title="Permalink to this headline">¶</a></h1>
<div class="section" id="resblock">
<h2>ResBlock<a class="headerlink" href="#resblock" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.res_block.</span></span><span class="sig-name descname"><span class="pre">ResBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">res_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'basic'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/res_block.html#ResBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><dl class="simple">
<dt>Residual Block with 2D convolution layers, including 2 types:</dt><dd><dl class="simple">
<dt>basic block:</dt><dd><p>input channel: C
x -&gt; 3*3*C -&gt; norm -&gt; act -&gt; 3*3*C -&gt; norm -&gt; act -&gt; out
__________________________________________/+</p>
</dd>
<dt>bottleneck block:</dt><dd><p>x -&gt; 1*1*(1/4*C) -&gt; norm -&gt; act -&gt; 3*3*(1/4*C) -&gt; norm -&gt; act -&gt; 1*1*C -&gt; norm -&gt; act -&gt; out
_____________________________________________________________________________/+</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Interfaces:</dt><dd><p>forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/res_block.html#ResBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the redisual block output</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the resblock output tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="resfcblock">
<h2>ResFCBlock<a class="headerlink" href="#resfcblock" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.res_block.</span></span><span class="sig-name descname"><span class="pre">ResFCBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'BN'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/res_block.html#ResFCBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Residual Block with 2 fully connected block
x -&gt; fc1 -&gt; norm -&gt; act -&gt; fc2 -&gt; norm -&gt; act -&gt; out
_____________________________________/+</p>
</dd>
<dt>Interfaces:</dt><dd><p>forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/res_block.html#ResFCBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the redisual block output</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the resblock output tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="network-rnn">
<h1>network.rnn<a class="headerlink" href="#network-rnn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="lstmforwardwrapper">
<h2>LSTMForwardWrapper<a class="headerlink" href="#lstmforwardwrapper" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTMForwardWrapper</span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A class which provides methods to use before and after <cite>forward</cite>, in order to wrap the LSTM <cite>forward</cite> method.</p>
</dd>
<dt>Interfaces:</dt><dd><p>_before_forward, _after_forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper._after_forward">
<span class="sig-name descname"><span class="pre">_after_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">next_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper._after_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._after_forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Post-process the next_state, return list or tensor type next_states</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tuple[torch.Tensor]]</span></code>): List of tuple which contains the next (h, c)</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether return next_state with list format, default set to False</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>next_state(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): the formatted next_state</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper._before_forward">
<span class="sig-name descname"><span class="pre">_before_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper._before_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._before_forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Preprocess the inputs and previous states</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input vector of cell, tensor of size [seq_len, batch_size, input_size]</p></li>
<li><dl class="simple">
<dt>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): None or tensor of size </dt><dd><p>[num_directions*num_layers, batch_size, hidden_size]. If None then prv_state will be initialized to all zeros.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): batch previous state in lstm</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implimentation of LSTM cell with LN</p>
</dd>
<dt>Interface:</dt><dd><p>forward</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For beginners, you can refer to &lt;<a class="reference external" href="https://zhuanlan.zhihu.com/p/32085405">https://zhuanlan.zhihu.com/p/32085405</a>&gt; to learn the basics about lstm</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#LSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Take the previous state and the input and calculate the output and the nextstate</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input vector of cell, tensor of size [seq_len, batch_size, input_size]</p></li>
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): None or tensor of size                 [num_directions*num_layers, batch_size, hidden_size]</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether return next_state with list format, default set to False</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): output from lstm</p></li>
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): hidden state from lstm</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="pytorchlstm">
<h2>PytorchLSTM<a class="headerlink" href="#pytorchlstm" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">PytorchLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#PytorchLSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Wrap the PyTorch nn.LSTM, format the input and output</p>
</dd>
<dt>Interface:</dt><dd><p>forward</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you can reference the &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM">https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM</a>&gt;</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/rnn.html#PytorchLSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Wrapped nn.LSTM.forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input vector of cell, tensor of size </dt><dd><p>[seq_len, batch_size, input_size]</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): None or tensor of size </dt><dd><p>[num_directions*num_layers, batch_size, hidden_size]</p>
</dd>
</dl>
</li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether return next_state with list format, default set to False</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): output from lstm</p></li>
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): hidden state from lstm</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-ding.torch_utils.network.rnn.get_lstm">
<span id="get-lstm"></span><h2>get_lstm<a class="headerlink" href="#module-ding.torch_utils.network.rnn.get_lstm" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Overview:</dt><dd><p>Build and return the corresponding LSTM cell</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>lstm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): version of rnn cell, now support [‘normal’, ‘pytorch’, ‘hpc’, ‘gru’]</p></li>
<li><p>input_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): size of the input vector</p></li>
<li><p>hidden_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): size of the hidden state vector</p></li>
<li><p>num_layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): number of lstm layers</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): type of the normaliztion, (default: None)</p></li>
<li><p>dropout (:obj:float):  dropout rate, default set to .0</p></li>
<li><p>seq_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): seq len, default set to None</p></li>
<li><p>batch_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): batch_size len, default set to None</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>lstm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[LSTM,</span> <span class="pre">PytorchLSTM]</span></code>): the corresponding lstm cell</p></li>
</ul>
</dd>
</dl>
</div>
</div>
<div class="section" id="network-scatter-connection">
<h1>network.scatter_connection<a class="headerlink" href="#network-scatter-connection" title="Permalink to this headline">¶</a></h1>
<div class="section" id="scatterconnection">
<h2>ScatterConnection<a class="headerlink" href="#scatterconnection" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.scatter_connection.</span></span><span class="sig-name descname"><span class="pre">ScatterConnection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scatter_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Scatter feature to its corresponding location
In AlphaStar, each entity is embedded into a tensor,
and these tensors are scattered into a feature map with map size.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spatial_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">location</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>scatter x into a spatial feature map</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor</span></code>): input tensor :math: <cite>(B, M, N)</cite> where <cite>M</cite> means the number of entity, <cite>N</cite> means                 the dimension of entity attributes</p></li>
<li><p>spatial_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): Tuple[H, W], the size of spatial feature x will be scattered into</p></li>
<li><p>location (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor</span></code>): :math: <cite>(B, M, 2)</cite> torch.LongTensor, each location should be (y, x)</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor</span></code>): :math: <cite>(B, N, H, W)</cite> where <cite>H</cite> and <cite>W</cite> are spatial_size, return the                scattered feature map</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>Input: :math: <cite>(B, M, N)</cite> where <cite>M</cite> means the number of entity, <cite>N</cite> means                 the dimension of entity attributes</p></li>
<li><p>Size: Tuple type :math: <cite>[H, W]</cite></p></li>
<li><p>Location: :math: <cite>(B, M, 2)</cite> torch.LongTensor, each location should be (y, x)</p></li>
<li><p>Output: :math: <cite>(B, N, H, W)</cite> where <cite>H</cite> and <cite>W</cite> are spatial_size</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When there are some overlapping in locations, <code class="docutils literal notranslate"><span class="pre">cover</span></code> mode will result in the loss of information, we
use the addition as temporal substitute.</p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="network-soft-argmax">
<h1>network.soft_argmax<a class="headerlink" href="#network-soft-argmax" title="Permalink to this headline">¶</a></h1>
<div class="section" id="softargmax">
<h2>SoftArgmax<a class="headerlink" href="#softargmax" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.soft_argmax.</span></span><span class="sig-name descname"><span class="pre">SoftArgmax</span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/soft_argmax.html#SoftArgmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>An nn.Module that computes SoftArgmax</p>
</dd>
<dt>Interface:</dt><dd><p>__init__, forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/soft_argmax.html#SoftArgmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Soft-argmax for location regression</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): predict heat map</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>location (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): predict location</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>x: <span class="math notranslate nohighlight">\((B, C, H, W)\)</span>, while B is the batch size, C is number of channels, </dt><dd><p>H and W stands for height and width</p>
</dd>
</dl>
</li>
<li><p>location: <span class="math notranslate nohighlight">\((B, 2)\)</span>, while B is the batch size</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="network-transformer">
<h1>network.transformer<a class="headerlink" href="#network-transformer" title="Permalink to this headline">¶</a></h1>
<div class="section" id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">Attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#Attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For each entry embedding, compute individual attention across all entries, add them up to get output attention</p>
</dd>
<dt>Interfaces:</dt><dd><p>split, forward</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#Attention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute attention</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
<li><p>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): mask out invalid entries</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>attention (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): attention tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#Attention.split"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.split" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Split input to get multihead queries, keys, values</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): query or key or value</p></li>
<li><p>T (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to transpose output</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.Tensor]</span></code>): list of output tensors for each head</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="transformerlayer">
<h2>TransformerLayer<a class="headerlink" href="#transformerlayer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#TransformerLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>In transformer layer, first computes entries’s attention and applies a feedforward layer</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#TransformerLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Transformer layer forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor]</span></code>): x and mask</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor]</span></code>): predict value and mask</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="transformer">
<h2>Transformer<a class="headerlink" href="#transformer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">ReLU()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#Transformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Transformer implementation</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For details refer to Attention is all you need: <a class="reference external" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/ding/torch_utils/network/transformer.html#Transformer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Transformer forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor. Shape (B, N, C), B is batch size, </dt><dd><p>N is number of entries, C is feature dimension</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): bool tensor, can be used to mask out invalid entries in attention. </dt><dd><p>Shape (B, N), B is batch size, N is number of entries</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): transformer output</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../utils/index.html" class="btn btn-neutral float-right" title="Utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="torch_utils_loss.html" class="btn btn-neutral float-left" title="loss.cross_entropy_loss" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>