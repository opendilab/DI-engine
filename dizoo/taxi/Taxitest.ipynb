{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放在DI-ENGINE文件夹执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05-28 20:27:12] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> If you want to use numba to speed up segment tree, please install numba first                                              <a href=\"file://c:\\Users\\wrh\\Desktop\\Intern\\DI-engine-main\\ding\\utils\\default_helper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">default_helper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\wrh\\Desktop\\Intern\\DI-engine-main\\ding\\utils\\default_helper.py#450\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">450</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05-28 20:27:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m If you want to use numba to speed up segment tree, please install numba first                                              \u001b]8;id=123555;file://c:\\Users\\wrh\\Desktop\\Intern\\DI-engine-main\\ding\\utils\\default_helper.py\u001b\\\u001b[2mdefault_helper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=952509;file://c:\\Users\\wrh\\Desktop\\Intern\\DI-engine-main\\ding\\utils\\default_helper.py#450\u001b\\\u001b[2m450\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import os, sys\n",
    "from typing import List, Tuple, Union, Literal, Optional\n",
    "\n",
    "import imageio\n",
    "import gym\n",
    "from gym.spaces import Space, Discrete\n",
    "from gym.spaces.box import Box\n",
    "import numpy as np\n",
    "from easydict import EasyDict\n",
    "\n",
    "from ding.envs.env.base_env import BaseEnv, BaseEnvTimestep\n",
    "from ding.torch_utils import to_ndarray\n",
    "from ding.utils import ENV_REGISTRY\n",
    "\n",
    "\n",
    "@ENV_REGISTRY.register('taxi')\n",
    "class TaxiV3Env(BaseEnv):\n",
    "    def __init__(self, cfg: dict) -> None:\n",
    "        \n",
    "        #^ 该部分为初始化定义，需要有的\n",
    "        self._cfg = EasyDict(\n",
    "            env_id='Taxi-v3',\n",
    "            render_mode='single_rgb_array',\n",
    "            max_episode_steps=300,  # default max trajectory length to truncate possible infinite attempts\n",
    "        )\n",
    "        self._cfg.update(cfg)\n",
    "        self._env = gym.make(\n",
    "                \"Taxi-v3\", render_mode=self._cfg.render_mode, max_episode_steps=self._cfg.max_episode_steps\n",
    "            )\n",
    "        self._init_flag = False\n",
    "        \n",
    "        #^ SAR space 定义\n",
    "        self._observation_space = Box(low=0, high=1, shape=(500, ), dtype=np.float32)\n",
    "        self._action_space = Discrete(6)\n",
    "        self._reward_space =  Box(\n",
    "            low=self._env.reward_range[0], high=self._env.reward_range[1], shape=(1, ), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        #^ 可视化设定\n",
    "        self._replay_path = None\n",
    "        self._save_replay_bool = False\n",
    "        self._frames = []\n",
    "       \n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \n",
    "        if not self._init_flag:\n",
    "            self._env = gym.make(\n",
    "                \"Taxi-v3\", render_mode=self._cfg.render_mode, max_episode_steps=self._cfg.max_episode_steps\n",
    "            )\n",
    "            self._init_flag = True\n",
    "        self._observation_space = gym.spaces.Box(low=0, high=1, shape=(500, ), dtype=np.float32)\n",
    "        self._action_space = Discrete(6)\n",
    "        self._reward_space = Box(\n",
    "            low=self._env.reward_range[0], high=self._env.reward_range[1], shape=(1, ), dtype=np.float32\n",
    "        )\n",
    "        self._eval_episode_return = 0\n",
    "        if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:\n",
    "            np_seed = 100 * np.random.randint(1, 1000)\n",
    "            self._env_seed = self._seed + np_seed \n",
    "        elif hasattr(self, '_seed'):\n",
    "            self._env_seed = self._seed\n",
    "        if hasattr(self, '_seed'):\n",
    "            obs = self._env.reset(seed=self._env_seed)\n",
    "        else:\n",
    "            obs = self._env.reset()\n",
    "            \n",
    "        if self._save_replay_bool:\n",
    "            picture = self._env.render()\n",
    "            self._frames.append(picture)\n",
    "        self._eval_episode_return = 0.\n",
    "        obs = to_ndarray(obs)\n",
    "        return obs\n",
    "    \n",
    "    #* 本部分和规范保持大差不差\n",
    "    def close(self) -> None:\n",
    "        if self._init_flag:\n",
    "            self._env.close()\n",
    "        self._init_flag = False\n",
    "        \n",
    "    #* 本部分和规范保持大差不差\n",
    "    def seed(self, seed: int, dynamic_seed: bool = True) -> None:\n",
    "        self._seed = seed\n",
    "        self._dynamic_seed = dynamic_seed\n",
    "        np.random.seed(self._seed)\n",
    "        \n",
    "    #* 本部分参考规范，加入gif部分参考了Frozenlake\n",
    "    def step(self, action: np.ndarray) -> BaseEnvTimestep:\n",
    "        assert isinstance(action, np.ndarray), type(action)\n",
    "        action = action.item()\n",
    "        obs, rew, done, info = self._env.step(action)\n",
    "        self._eval_episode_return += rew\n",
    "        obs = to_ndarray(obs)\n",
    "        rew = to_ndarray([rew])  # Transformed to an array with shape (1, )\n",
    "        if self._save_replay_bool:\n",
    "            picture = self._env.render()\n",
    "            self._frames.append(picture)\n",
    "            \n",
    "        #^ 这里为可视化save过程\n",
    "        #^ 测试里随机采样木有等到done触发，故可能没有gif保存\n",
    "        #^ 但当把下面的replay取出来时发现可执行，有gif图片保存。\n",
    "        if done:\n",
    "            info['eval_episode_return'] = self._eval_episode_return\n",
    "            if self._save_replay_bool:\n",
    "                assert self._replay_path is not None, \"your should have a path\"\n",
    "                path = os.path.join(\n",
    "                    self._replay_path, '{}_episode_{}.gif'.format(self._cfg.env_id, self._save_replay_count)\n",
    "                )\n",
    "                self.frames_to_gif(self._frames, path)\n",
    "                self._frames = []\n",
    "                self._save_replay_count += 1\n",
    "        rew = rew.astype(np.float32)\n",
    "        obs = obs.astype(np.float32)\n",
    "        return BaseEnvTimestep(obs, rew, done, info)\n",
    "    \n",
    "    def enable_save_replay(self, replay_path: Optional[str] = None) -> None:\n",
    "        if replay_path is None:\n",
    "            replay_path = './video'\n",
    "            if not os.path.exists(replay_path):\n",
    "                os.makedirs(replay_path)\n",
    "        self._replay_path = replay_path\n",
    "        self._save_replay_bool = True\n",
    "        self._save_replay_count = 0\n",
    "        \n",
    "    #* 该部分为random_action 部分，一致\n",
    "    def random_action(self) -> np.ndarray:\n",
    "        random_action = self.action_space.sample()\n",
    "        if isinstance(random_action, np.ndarray):\n",
    "            pass\n",
    "        elif isinstance(random_action, int):\n",
    "            random_action = to_ndarray([random_action], dtype=np.int64)\n",
    "        elif isinstance(random_action, dict):\n",
    "            random_action = to_ndarray(random_action)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                '`random_action` should be either int/np.ndarray or dict of int/np.ndarray, but get {}: {}'.format(\n",
    "                type(random_action), random_action\n",
    "                )\n",
    "            )\n",
    "        return random_action\n",
    "        \n",
    "    #todo 有关taxi的state的编码implementation     \n",
    "    def _encode_taxi(self, obs: np.ndarray) -> np.ndarray:\n",
    "        taxi_row, taxi_col, passenger_location, destination = self._env.unwrapped.decode(obs)\n",
    "        \n",
    "    #* 三个Property部分，一致\n",
    "    @property\n",
    "    def observation_space(self) -> Space:\n",
    "        return self._observation_space\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> Space:\n",
    "        return self._action_space\n",
    "\n",
    "    @property\n",
    "    def reward_space(self) -> Space:\n",
    "        return self._reward_space\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"DI-engine Taxi-v3 Env\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def frames_to_gif(frames: List[imageio.core.util.Array], gif_path: str, duration: float = 0.1) -> None:\n",
    "        \"\"\"\n",
    "        Convert a list of frames into a GIF.\n",
    "        Args:\n",
    "        - frames (List[imageio.core.util.Array]): A list of frames, each frame is an image.\n",
    "        - gif_path (str): The path to save the GIF file.\n",
    "        - duration (float): Duration between each frame in the GIF (seconds).\n",
    "\n",
    "        Returns:\n",
    "        None, the GIF file is saved directly to the specified path.\n",
    "        \"\"\"\n",
    "        # Save all frames as temporary image files\n",
    "        temp_image_files = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            temp_image_file = f\"frame_{i}.png\"  # Temporary file name\n",
    "            imageio.imwrite(temp_image_file, frame)  # Save the frame as a PNG file\n",
    "            temp_image_files.append(temp_image_file)\n",
    "\n",
    "        # Use imageio to convert temporary image files to GIF\n",
    "        with imageio.get_writer(gif_path, mode='I', duration=duration) as writer:\n",
    "            for temp_image_file in temp_image_files:\n",
    "                image = imageio.imread(temp_image_file)\n",
    "                writer.append_data(image)\n",
    "\n",
    "        # Clean up temporary image files\n",
    "        for temp_image_file in temp_image_files:\n",
    "            os.remove(temp_image_file)\n",
    "        print(f\"GIF saved as {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 0, 1]\n",
      "============================================================\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "============================================================\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(361., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(361., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(261., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(281., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(281., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(281., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "============================================================\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "============================================================\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(361., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(361., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(361., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "============================================================\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(461., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(481., dtype=float32), reward=array([-10.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(381., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "你本次的Timestep是：BaseEnvTimestep(obs=array(281., dtype=float32), reward=array([-1.], dtype=float32), done=False, info={'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n",
      "Box(0.0, 1.0, (500,), float32) Discrete(6) Box(-inf, inf, (1,), float32)\n",
      "已通过类型检查！\n"
     ]
    }
   ],
   "source": [
    "env = TaxiV3Env({})\n",
    "env.seed(314, dynamic_seed=False)\n",
    "assert env._seed == 314\n",
    "#* seed通过检查\n",
    "env.enable_save_replay()\n",
    "#* replay可视化通过检查\n",
    "\n",
    "obs = env.reset()\n",
    "assert obs.shape == ()\n",
    "print(list(env._env.unwrapped.decode(obs)))\n",
    "#* reset通过检查\n",
    "\n",
    "for _ in range(5):\n",
    "    env.reset()\n",
    "    np.random.seed(314)\n",
    "    print('=' * 60)\n",
    "    for i in range(10):\n",
    "        # Both ``env.random_action()``, and utilizing ``np.random`` as well as action space,\n",
    "        # can generate legal random action.\n",
    "        if i < 5:\n",
    "            random_action = np.array([env.action_space.sample()])\n",
    "        else:\n",
    "            random_action = env.random_action()\n",
    "        #* random_action通过检查\n",
    "        \n",
    "        timestep = env.step(random_action)\n",
    "        print(f\"你本次的Timestep是：{timestep}\")\n",
    "        assert isinstance(timestep.obs, np.ndarray)\n",
    "        assert isinstance(timestep.done, bool)\n",
    "        assert timestep.obs.shape == ()\n",
    "        assert timestep.reward.shape == (1, )\n",
    "        assert timestep.reward >= env.reward_space.low\n",
    "        assert timestep.reward <= env.reward_space.high\n",
    "        #* step 通过检查\n",
    "print(env.observation_space, env.action_space, env.reward_space)\n",
    "env.close()\n",
    "print(f\"已通过类型检查！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as ./video\\Taxi-v3_episode_0.gif\n"
     ]
    }
   ],
   "source": [
    "env = TaxiV3Env({})\n",
    "env.seed(314, dynamic_seed=False)\n",
    "assert env._seed == 314\n",
    "#* seed通过检查\n",
    "env.enable_save_replay()\n",
    "#* replay可视化通过检查\n",
    "\n",
    "obs = env.reset()\n",
    "actions = [1, 1, 3, 3, 3, 1, 1, 4, 0, 0, 2, 2, 2, 2, 1, 1, 5]\n",
    "for action in actions:\n",
    "    timestep = env.step(np.array([action]))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
