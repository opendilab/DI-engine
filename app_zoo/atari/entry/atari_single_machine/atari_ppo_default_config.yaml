common:
    name: AtariPpoConfig
    time_wrapper_type: cuda
    save_path: '.'
    load_path: ''
    algo_type: 'ppo'  # ['dqn', 'ppo']
learner:
    use_cuda: False
    use_distributed: False
    max_iterations: 10000000
    train_step: 1000
    batch_size: 64
    learning_rate: 0.001
    weight_decay: 0.0001
    eps:
        type: 'linear'
        start: 1.
        end: 0.005
        decay: 100000
    data:
        max_reuse: 8
        buffer_length: 20000
    ppo:
        gamma: 0.99
        value_weight: 0.5
        entropy_weight: 0.01
        gae_lambda: 0.95
        clip_ratio: 0.2
    hook:
        save_ckpt_after_iter:
            name: save_ckpt_after_iter
            type: save_ckpt
            priority: 40
            position: after_iter
            ext_args:
                freq: 100000
        log_show:
            name: log_show
            type: log_show
            priority: 20
            position: after_iter
            ext_args:
                freq: 1000
model:
    embedding_dim: 256
actor:
    env_num: 8
    episode_num: 8
    print_freq: 500
evaluator:
    env_num: 4
    episode_num: 1
    eval_step: 10000
    stop_val: 20  # pong: 20
env:
    env_id: 'PongNoFrameskip-v4'
    frame_stack: 4
    is_train: True
