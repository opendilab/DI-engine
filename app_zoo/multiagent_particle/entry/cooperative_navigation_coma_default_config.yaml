common:
    name: CooperativeNavigationCOMAConfig
    save_path: '.'
    load_path: ''
env:
    env_manager_type: 'subprocess'  # [base, subprocess]
    import_names: ['app_zoo.multiagent_particle.envs.particle_env']
    env_type: 'cooperative_navigation'
    num_agents: 5
    num_landmarks: 5
    agent_num: 5
    actor_env_num: 8
    evaluator_env_num: 3
    shared_memory: False
policy:
    use_cuda: True
    policy_type: 'coma'
    import_names: ['nervex.policy.coma']
    on_policy: True
    model:
        agent_num: 5
        obs_dim:
            agent_state: [5, 22]
            global_state: 30
        act_dim: [5, ]
        embedding_dim: 64
    learn:
        train_step: 1
        batch_size: 32
        agent_num: 5
        learning_rate: 0.0005
        weight_decay: 0.00001
        algo:
            target_update_theta: 0.001
            discount_factor: 0.99
            td_lambda: 0.8
            value_weight: 1.0
            entropy_weight: 0.01
    collect:
        traj_len: 'inf'
        unroll_len: 16
        agent_num: 5
        env_num: 8  # actor_env_num
    eval:
        agent_num: 5
        env_num: 3  # evaluator_env_num
    command:
        eps:
            type: 'exp'
            start: 0.5
            end: 0.01
            decay: 100000
replay_buffer:
    buffer_name: ['agent']
    agent:
        meta_maxlen: 64
        max_reuse: 100
        min_sample_ratio: 1
actor:
    n_episode: 4
    traj_len: 1000  # smac_episode_max_length
    traj_print_freq: 100
    collect_print_freq: 100
evaluator:
    n_episode: 3
    eval_freq: 1000
    stop_val: 0
learner:
    hook:
        log_show:
            name: log_show
            type: log_show
            priority: 20
            position: after_iter
            ext_args:
                freq: 100
commander:
    placeholder: 'placeholder'
