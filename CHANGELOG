2021.4.6(v0.1.1rc1)
 - gfootball IL Policy
 - polish SQN
 - Atari Multi-Discrete Env
 - PPO/Rainbow 支持Multi-Discrete Env
 - 分离requirements，支持按需安装相关的库
 - 更新使用自建gitlab CI runner
 - 添加dist entry(local/slurm)
 - 添加注册器(registry)机制
 - 添加replay buffer的保存和加载机制
 - 添加subprocess env manager reset超时重启机制
 - 添加sync subprocess env manager并设为默认选择
 - 添加新的replay buffer数据删除机制
 - 添加共用tensorboard，四个主模块可只使用一个tb
 - 添加env step作为tb横轴
 - 添加计算grad norm的相关函数
 - 在policy中删除traj_len字段
 - 修改train_step为train_iteration字段
 - 修改stop_val为stop_value字段
 - 修复rainbow取log数值稳定性和分布计算的问题
 - 修复命令行工具无法设计stdout缓冲区大小的问题
 - 移到env manager到env文件夹下
 - 重构replay buffer配置文件字段
 - 重构actor部分log显示机制
 - 修改replay buffer中部分变量命名
 - 添加exploration算法相关文档
 - 增加若干文档和单元测试

2021.3.12(v0.1.1rc0)
 - League Demo
 - Inverse RL Pipeline
 - SQN算法实现
 - Multi Discrete Action Space Demo(SUMO环境)
 - 更新UPGO算法实现
 - 文档Best Practice
 - 废弃grad armor plugin
 - 删除learner中的use_cuda选项
 - 添加actor tensorboard和tensorboard中histogram接口
 - 抽象通用replay buffer接口
 - 添加pytorch计算cov(协方差)的工具函数
 - 修复多卡训练时关于cuda device的bug
 - 修复ATOC RNN使用相关bug
 - 修复replay buffer anneal step相关bug

2021.3.5(v0.1.1b0)
 - PPG算法实现
 - ATOC Policy实现
 - 支持procgen环境
 - 支持五子棋环境
 - 支持POMDP Atari环境
 - 适配SUMO环境相关policy
 - 提供HPL_RL库+相应文档
 - 重构replay buffer多buffer相关接口
 - 重构priority相关操作（由learner->policy中实现）
 - 修复atari环境中随机种子问题
 - 修复并行训练中learner数据聚合相关bug
 - 文档Algorithm Overview，提供基础benchmark
 - 文档Interaction Overview
 - 文档Loader Overview
 - 若干细节优化

2021.2.1(v0.1.0b2)
 - 文档补充 (env_manager, dataloader, learner, policy, adder)
 - dataloader speed test
 - 单独eval入口
 - tensorboard优化
 - SAC auto alpha
 - install check in doc
 - render demo(enable_save_replay)
 - transfer to python-config

2021.1.25(v0.1.0b1)
 - 添加HER算法
 - 修改register model机制，改变现在的神经网络创建方式
 - 修复线程和进程管理相关bug
 - 若干易用性调整
 - 优化SAC算法效率

2021.1.19(v0.1.0b0)
 - 异步训练版本
 - multi replay buffer
 - shm vec env manager

2020.12.6(v0.0.2b1)
 - DDPG/TD3算法实现
 - QMIX/COMA算法实现
 - SMAC + multiagent_particle多智能体环境
 - 连续动作空间exploration相关方法
 - 离散动作采样插件支持mask
 - Q-value Actor-Critic单优化器实现
 - 可以传递内部异常的线程实现
 - MARL相关paper文档
 - 修复若干bug

2020.11.20(v0.0.2b0)
 - A2C算法实现，在atari环境上的多机运行示例
 - mujoco环境初步适配
 - DRQN + R2D2算法实现（支持动态RNN，nstep-td, LSTM隐状态burn_in，value rescaling）
 - 重构通用的league模块
 - 分离解耦model/pipeline agent plugin
 - 补充API文档和单元测试（单元测试代码覆盖率83%）
 - 补充MCTS和alphago相关文档
 - 修复若干bug
 
2020.8.14-2020.11.9(v0.01)
 - 初始版本发布 (9ec6fa8ed947c3ddd9db2ab983f4b41369226561)
 - sumo 望京3路口环境baseline (4f63086dc6ef243aad8ab77a4ea6649acf21bd3e)
 - sumo env learner(basic DQN) (37495a734a2381e834fc8f0a12cf9baeaab5c488)
 - pong env环境（3117bebb391ef346f8e95557e50c3b78a11e733d）
 - catepole/pendulum env环境 (35e779cbba713b063cf97ed848f2dc68da327ee2)
 - RL warm up文档(RL intro，DQN系列，PG，DDPG，PPO）（dcabacc3ee4d774a90d84e4fa10835335e2f5fc5）
 - learner重构版本 （837658b10c3f88bfe72498a93d1447ce8e1be54d）
 - dueling DQN (e60c7694dd64b6961e2b6705c1120fe1f2d9ad37)
 - 分离示例代码和框架主体代码，提供单机和多机示例（3b904fdfe6a4523140aec26ec83654dc06ce8527）
 - 修复vec env manger环境报错调用栈显示问题，修复vec env manager pickle问题，从而env定义时无需定义全局变量 （b81a2594052efd2b7cea79baea8726488bb32963）
 - 异步actor + gfootball环境及训练pipeline (519781d58ed6955af7e02a1cba1478ef06a454b5)
 - PPO+GAE + 单机入口更新 (b4c841c296d854050ef9f196564277b1aaec66fe)
