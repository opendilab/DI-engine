


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ding.torch_utils &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="ding.utils" href="utils.html" />
  <link rel="prev" title="ding.rl_utils" href="rl_utils.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">API Doc</a> &gt;</li>
        
      <li>ding.torch_utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/05_api_doc/torch_utils.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="ding-torch-utils">
<h1>ding.torch_utils<a class="headerlink" href="#ding-torch-utils" title="Permalink to this heading">¶</a></h1>
<section id="loss">
<h2>loss<a class="headerlink" href="#loss" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/loss</span></code> for more details.</p>
<section id="contrastiveloss">
<h3>ContrastiveLoss<a class="headerlink" href="#contrastiveloss" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.loss.</span></span><span class="sig-name descname"><span class="pre">ContrastiveLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SequenceType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SequenceType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encode_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'infoNCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/loss/contrastive_loss.html#ContrastiveLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The class for contrastive learning losses. Only InfoNCE loss is supported currently.         Code Reference: <a class="reference external" href="https://github.com/rdevon/DIM">https://github.com/rdevon/DIM</a>. Paper Reference: <a class="reference external" href="https://arxiv.org/abs/1808.06670">https://arxiv.org/abs/1808.06670</a>.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SequenceType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SequenceType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encode_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'infoNCE'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/contrastive_loss.html#ContrastiveLoss.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ContrastiveLoss object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">SequenceType]</span></code>): input shape for x, both the obs shape and the encoding shape                 are supported.</p></li>
<li><p>y_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">SequenceType]</span></code>): Input shape for y, both the obs shape and the encoding shape                 are supported.</p></li>
<li><p>heads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">SequenceType</span></code>): A list of 2 int elems, <code class="docutils literal notranslate"><span class="pre">heads[0]</span></code> for x and <code class="docutils literal notranslate"><span class="pre">head[1]</span></code> for y.                 Used in multi-head, global-local, local-local MI maximization process.</p></li>
<li><p>encoder_shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">SequenceType]</span></code>): The dimension of encoder hidden state.</p></li>
<li><p>loss_type: Only the InfoNCE loss is available now.</p></li>
<li><p>temperature: The parameter to adjust the <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._create_encoder">
<span class="sig-name descname"><span class="pre">_create_encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SequenceType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/contrastive_loss.html#ContrastiveLoss._create_encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._create_encoder" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Create the encoder for the input obs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">SequenceType]</span></code>): input shape for x, both the obs shape and the encoding shape                 are supported. If the obs_size is an int, it means the obs is a 1D vector. If the obs_size is a list                 such as [1, 16, 16], it means the obs is a 3D image with shape [1, 16, 16].</p></li>
<li><p>heads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of heads.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>encoder (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The encoder module.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">obs_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="go">or</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">heads</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_encoder</span><span class="p">(</span><span class="n">obs_size</span><span class="p">,</span> <span class="n">heads</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/contrastive_loss.html#ContrastiveLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Computes the noise contrastive estimation-based loss, a.k.a. infoNCE.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input x, both raw obs and encoding are supported.</p></li>
<li><p>y (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input y, both raw obs and encoding are supported.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The calculated loss value.</p>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x_dim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encode_shape</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">encode_shape</span><span class="o">=</span><span class="n">encode_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x_dim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encode_shape</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">encode_shape</span><span class="o">=</span><span class="n">encode_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.ContrastiveLoss.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.loss.ContrastiveLoss.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="labelsmoothceloss">
<h3>LabelSmoothCELoss<a class="headerlink" href="#labelsmoothceloss" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.loss.</span></span><span class="sig-name descname"><span class="pre">LabelSmoothCELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#LabelSmoothCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Label smooth cross entropy loss.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#LabelSmoothCELoss.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the LabelSmoothCELoss object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The ratio of label-smoothing (the value is in 0-1). If the ratio is larger, the                 extent of label smoothing is larger.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#LabelSmoothCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate label smooth cross entropy loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Predicted logits.</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Calculated loss.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.LabelSmoothCELoss.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.loss.LabelSmoothCELoss.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="softfocalloss">
<h3>SoftFocalLoss<a class="headerlink" href="#softfocalloss" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.loss.</span></span><span class="sig-name descname"><span class="pre">SoftFocalLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#SoftFocalLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Soft focal loss.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#SoftFocalLoss.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the SoftFocalLoss object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The extent of focus on hard samples. A smaller <code class="docutils literal notranslate"><span class="pre">gamma</span></code> will lead to more focus on                 easy samples, while a larger <code class="docutils literal notranslate"><span class="pre">gamma</span></code> will lead to more focus on hard samples.</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The weight for loss of each class.</p></li>
<li><p>size_average (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): By default, the losses are averaged over each loss element in the batch.                 Note that for some losses, there are multiple elements per sample. If the field <code class="docutils literal notranslate"><span class="pre">size_average</span></code> is                 set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored when <code class="docutils literal notranslate"><span class="pre">reduce</span></code> is                 <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>reduce (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[bool]</span></code>): By default, the losses are averaged or summed over observations for                 each minibatch depending on size_average. When <code class="docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss for each batch                 element instead and ignores <code class="docutils literal notranslate"><span class="pre">size_average</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#SoftFocalLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate soft focal loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Predicted logits.</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Calculated loss.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.SoftFocalLoss.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.loss.SoftFocalLoss.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="build-ce-criterion">
<h3>build_ce_criterion<a class="headerlink" href="#build-ce-criterion" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.loss.build_ce_criterion">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.loss.</span></span><span class="sig-name descname"><span class="pre">build_ce_criterion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/cross_entropy_loss.html#build_ce_criterion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.build_ce_criterion" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get a cross entropy loss instance according to given config.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>)<span class="classifier">Config dict. It contains:</span></dt><dd><ul>
<li><p>type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Type of loss function, now supports [‘cross_entropy’, ‘label_smooth_ce’,                 ‘soft_focal_loss’].</p></li>
<li><p>kwargs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Arguments for the corresponding loss function.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): loss function instance</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="multilogitsloss">
<h3>MultiLogitsLoss<a class="headerlink" href="#multilogitsloss" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.loss.</span></span><span class="sig-name descname"><span class="pre">MultiLogitsLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Base class for supervised learning on linklink, including basic processes.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization method, use cross_entropy as default criterion.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Criterion type, supports [‘cross_entropy’, ‘label_smooth_ce’].</p></li>
<li><p>smooth_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Smoothing ratio for label smoothing.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._get_distance_matrix">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_get_distance_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ly</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">M</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss._get_distance_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._get_distance_matrix" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get distance matrix.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>lx (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): lx.</p></li>
<li><p>ly (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): ly.</p></li>
<li><p>mat (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): mat.</p></li>
<li><p>M (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): M.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._get_metric_matrix">
<span class="sig-name descname"><span class="pre">_get_metric_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss._get_metric_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._get_metric_matrix" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the metric matrix.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Predicted logits.</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>metric (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Calculated metric matrix.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._label_process">
<span class="sig-name descname"><span class="pre">_label_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss._label_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._label_process" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process the label according to the criterion.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Predicted logits.</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ret (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Processed label.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._match">
<span class="sig-name descname"><span class="pre">_match</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss._match"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._match" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Match the metric matrix.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>matrix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Metric matrix.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>index (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code>): Matched index.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._nll_loss">
<span class="sig-name descname"><span class="pre">_nll_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nlls</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss._nll_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the negative log likelihood loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>nlls (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Negative log likelihood loss.</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ret (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Calculated loss.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/loss/multi_logits_loss.html#MultiLogitsLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate multiple logits loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Predicted logits, whose shape must be 2-dim, like (B, N).</p></li>
<li><p>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Ground truth.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Calculated loss.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.loss.MultiLogitsLoss.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.loss.MultiLogitsLoss.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-activation">
<h2>network.activation<a class="headerlink" href="#network-activation" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/activation</span></code> for more details.</p>
<section id="lambda">
<h3>Lambda<a class="headerlink" href="#lambda" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">Lambda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Lambda"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A custom lambda module for constructing custom layers.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">f</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Lambda.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the lambda module with a given function.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>f (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): a python function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Lambda.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the function of the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Lambda.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Lambda.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="glu">
<h3>GLU<a class="headerlink" href="#glu" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">GLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'fc'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GLU" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Gating Linear Unit (GLU), a specific type of activation function, which is first proposed in
[Language Modeling with Gated Convolutional Networks](<a class="reference external" href="https://arxiv.org/pdf/1612.08083.pdf">https://arxiv.org/pdf/1612.08083.pdf</a>).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'fc'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GLU.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GLU.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the GLU module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input tensor.</p></li>
<li><p>output_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the output tensor.</p></li>
<li><p>context_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the context tensor.</p></li>
<li><p>input_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of input, now supports [‘fc’, ‘conv2d’]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the GLU transformation of the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
<li><p>context (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The context tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after GLU transformation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GLU.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GLU.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="swish">
<h3>Swish<a class="headerlink" href="#swish" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">Swish</span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Swish"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Swish" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Swish activation function, which is a smooth, non-monotonic activation function. For more details, please refer
to [Searching for Activation Functions](<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a>).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Swish.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Swish.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Swish module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#Swish.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.Swish.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the Swish transformation of the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after Swish transformation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.Swish.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.Swish.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gelu">
<h3>GELU<a class="headerlink" href="#gelu" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">GELU</span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GELU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GELU" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Gaussian Error Linear Units (GELU) activation function, which is widely used in NLP models like GPT, BERT.
For more details, please refer to the original paper: <a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a>.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GELU.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GELU.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the GELU module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#GELU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.GELU.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the GELU transformation of the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after GELU transformation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.GELU.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.activation.GELU.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="build-activation">
<h3>build_activation<a class="headerlink" href="#build-activation" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.activation.build_activation">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.activation.</span></span><span class="sig-name descname"><span class="pre">build_activation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/activation.html#build_activation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.activation.build_activation" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Build and return the activation module according to the given type.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of activation module, now supports             [‘relu’, ‘glu’, ‘prelu’, ‘swish’, ‘gelu’, ‘tanh’, ‘sigmoid’, ‘softplus’, ‘elu’, ‘square’, ‘identity’].</p></li>
<li><p>inplace (Optional[<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Execute the operation in-place in activation, defaults to None.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>act_func (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.module</span></code>): The corresponding activation module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="network-diffusion">
<h2>network.diffusion<a class="headerlink" href="#network-diffusion" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/diffusion</span></code> for more details.</p>
<section id="extract">
<h3>extract<a class="headerlink" href="#extract" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.extract">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#extract"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.extract" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>extract output from a through index t.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>a (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
<li><p>t (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): index tensor</p></li>
<li><p>x_shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): shape of x</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="cosine-beta-schedule">
<h3>cosine_beta_schedule<a class="headerlink" href="#cosine-beta-schedule" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.cosine_beta_schedule">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">cosine_beta_schedule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">timesteps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.008</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#cosine_beta_schedule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.cosine_beta_schedule" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>cosine schedule
as proposed in <a class="reference external" href="https://openreview.net/forum?id=-NEXDKk8gZ">https://openreview.net/forum?id=-NEXDKk8gZ</a></p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>timesteps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): timesteps of diffusion step</p></li>
<li><p>s (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): s</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code>): dtype of beta</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><p>Tensor of beta [timesteps,], computing by cosine.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="apply-conditioning">
<h3>apply_conditioning<a class="headerlink" href="#apply-conditioning" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.apply_conditioning">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">apply_conditioning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conditions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#apply_conditioning"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.apply_conditioning" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>add condition into x</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
<li><p>conditions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): condition dict, key is timestep, value is condition</p></li>
<li><p>action_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): action dim</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="diffusionconv1d">
<h3>DiffusionConv1d<a class="headerlink" href="#diffusionconv1d" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">DiffusionConv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionConv1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Conv1d with activation and normalization for diffusion models.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionConv1d.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a 1-dim convlution layer with activation and normalization. This Conv1d have GropuNorm.
And need add 1-dim when compute norm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Zero-padding added to both sides of the input</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionConv1d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute conv1d for inputs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>out (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): output tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionConv1d.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="sinusoidalposemb">
<h3>SinusoidalPosEmb<a class="headerlink" href="#sinusoidalposemb" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">SinusoidalPosEmb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#SinusoidalPosEmb"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>class for computing sin position embeding</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#SinusoidalPosEmb.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of SinusoidalPosEmb class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dimension of embeding</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#SinusoidalPosEmb.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute sin position embeding</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>emb (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): output tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.SinusoidalPosEmb.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="residual">
<h3>Residual<a class="headerlink" href="#residual" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">Residual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#Residual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Basic Residual block</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#Residual.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of Residual class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): function of residual block</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">arg</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#Residual.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute residual block</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.Residual.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.Residual.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="layernorm">
<h3>LayerNorm<a class="headerlink" href="#layernorm" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>LayerNorm, compute dim = 1, because Temporal input x [batch, dim, horizon]</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LayerNorm.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of LayerNorm class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dimension of input</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): eps of LayerNorm</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute LayerNorm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LayerNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LayerNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="prenorm">
<h3>PreNorm<a class="headerlink" href="#prenorm" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">PreNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#PreNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>PreNorm, compute dim = 1, because Temporal input x [batch, dim, horizon]</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#PreNorm.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of PreNorm class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dimension of input</p></li>
<li><p>fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): function of residual block</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#PreNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute PreNorm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.PreNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.PreNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="linearattention">
<h3>LinearAttention<a class="headerlink" href="#linearattention" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">LinearAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LinearAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Linear Attention head</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LinearAttention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of LinearAttention class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dimension of input</p></li>
<li><p>heads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): heads of attention</p></li>
<li><p>dim_head (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): dim of head</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#LinearAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute LinearAttention</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): input tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.LinearAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.LinearAttention.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="residualtemporalblock">
<h3>ResidualTemporalBlock<a class="headerlink" href="#residualtemporalblock" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">ResidualTemporalBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mish</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#ResidualTemporalBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Residual block of temporal</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mish</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#ResidualTemporalBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of ResidualTemporalBlock class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (:obj:’int’): dim of in_channels</p></li>
<li><p>out_channels (:obj:’int’): dim of out_channels</p></li>
<li><p>embed_dim (:obj:’int’): dim of embeding layer</p></li>
<li><p>kernel_size (:obj:’int’): kernel_size of conv1d</p></li>
<li><p>mish (:obj:’bool’): whether use mish as a activate function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#ResidualTemporalBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute residual block</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (:obj:’tensor’): input tensor</p></li>
<li><p>t (:obj:’tensor’): time tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.ResidualTemporalBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="diffusionunet1d">
<h3>DiffusionUNet1d<a class="headerlink" href="#diffusionunet1d" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">DiffusionUNet1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transition_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_mults</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">8]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">returns_condition</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calc_energy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionUNet1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Diffusion unet for 1d vector data</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">get_pred</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transition_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_mults</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">8]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">returns_condition</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calc_energy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionUNet1d.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of DiffusionUNet1d class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transition_dim (:obj:’int’): dim of transition, it is obs_dim + action_dim</p></li>
<li><p>dim (:obj:’int’): dim of layer</p></li>
<li><p>dim_mults (:obj:’SequenceType’): mults of dim</p></li>
<li><p>returns_condition (:obj:’bool’): whether use return as a condition</p></li>
<li><p>condition_dropout (:obj:’float’): dropout of returns condition</p></li>
<li><p>calc_energy (:obj:’bool’): whether use calc_energy</p></li>
<li><p>kernel_size (:obj:’int’): kernel_size of conv1d</p></li>
<li><p>attention (:obj:’bool’): whether use attention</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cond</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">returns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionUNet1d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute diffusion unet forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (:obj:’tensor’): noise trajectory</p></li>
<li><p>cond (:obj:’tuple’): [ (time, state), … ] state is init state of env, time = 0</p></li>
<li><p>time (:obj:’int’): timestep of diffusion step</p></li>
<li><p>returns (:obj:’tensor’): condition returns of trajectory, returns is normal return</p></li>
<li><p>use_dropout (:obj:’bool’): Whether use returns condition mask</p></li>
<li><p>force_dropout (:obj:’bool’): Whether use returns condition</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d.get_pred">
<span class="sig-name descname"><span class="pre">get_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cond</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">returns</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#DiffusionUNet1d.get_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.get_pred" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute diffusion unet forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (:obj:’tensor’): noise trajectory</p></li>
<li><p>cond (:obj:’tuple’): [ (time, state), … ] state is init state of env, time = 0</p></li>
<li><p>time (:obj:’int’): timestep of diffusion step</p></li>
<li><p>returns (:obj:’tensor’): condition returns of trajectory, returns is normal return</p></li>
<li><p>use_dropout (:obj:’bool’): Whether use returns condition mask</p></li>
<li><p>force_dropout (:obj:’bool’): Whether use returns condition</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.DiffusionUNet1d.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="temporalvalue">
<h3>TemporalValue<a class="headerlink" href="#temporalvalue" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.diffusion.</span></span><span class="sig-name descname"><span class="pre">TemporalValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">horizon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transition_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_mults</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">8]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#TemporalValue"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>temporal net for value function</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">horizon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transition_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_mults</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SequenceType</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">8]</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#TemporalValue.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of TemporalValue class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>horizon (:obj:’int’): horizon of trajectory</p></li>
<li><p>transition_dim (:obj:’int’): dim of transition, it is obs_dim + action_dim</p></li>
<li><p>dim (:obj:’int’): dim of layer</p></li>
<li><p>time_dim (:obj:’int’): dim of time</p></li>
<li><p>out_dim (:obj:’int’): dim of output</p></li>
<li><p>kernel_size (:obj:’int’): kernel_size of conv1d</p></li>
<li><p>dim_mults (:obj:’SequenceType’): mults of dim</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cond</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/diffusion.html#TemporalValue.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute temporal value forward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (:obj:’tensor’): noise trajectory</p></li>
<li><p>cond (:obj:’tuple’): [ (time, state), … ] state is init state of env, time = 0</p></li>
<li><p>time (:obj:’int’): timestep of diffusion step</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.diffusion.TemporalValue.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.diffusion.TemporalValue.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-dreamer">
<h2>network.dreamer<a class="headerlink" href="#network-dreamer" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/dreamer</span></code> for more details.</p>
<section id="conv2dsame">
<h3>Conv2dSame<a class="headerlink" href="#conv2dsame" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">Conv2dSame</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Conv2dSame"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Conv2dSame Network for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame._reversed_padding_repeated_twice">
<span class="sig-name descname"><span class="pre">_reversed_padding_repeated_twice</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame._reversed_padding_repeated_twice" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.calc_same_pad">
<span class="sig-name descname"><span class="pre">calc_same_pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Conv2dSame.calc_same_pad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.calc_same_pad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the same padding size.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>i (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Input size.</p></li>
<li><p>k (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Kernel size.</p></li>
<li><p>s (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Stride size.</p></li>
<li><p>d (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Dilation size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.dilation">
<span class="sig-name descname"><span class="pre">dilation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.dilation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Conv2dSame.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute the forward of Conv2dSame.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.groups">
<span class="sig-name descname"><span class="pre">groups</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.in_channels">
<span class="sig-name descname"><span class="pre">in_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.in_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.kernel_size">
<span class="sig-name descname"><span class="pre">kernel_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.out_channels">
<span class="sig-name descname"><span class="pre">out_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.out_channels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.output_padding">
<span class="sig-name descname"><span class="pre">output_padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.output_padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.padding_mode">
<span class="sig-name descname"><span class="pre">padding_mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.padding_mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.stride">
<span class="sig-name descname"><span class="pre">stride</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.transposed">
<span class="sig-name descname"><span class="pre">transposed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.transposed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Conv2dSame.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tensor</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.Conv2dSame.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="dreamerlayernorm">
<h3>DreamerLayerNorm<a class="headerlink" href="#dreamerlayernorm" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">DreamerLayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DreamerLayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>DreamerLayerNorm Network for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DreamerLayerNorm.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the DreamerLayerNorm class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>ch (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Input channel.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Epsilon.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DreamerLayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute the forward of DreamerLayerNorm.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DreamerLayerNorm.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="densehead">
<h3>DenseHead<a class="headerlink" href="#densehead" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">DenseHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SiLU'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DenseHead"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>DenseHead Network for value head, reward head, and discount head of dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SiLU'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DenseHead.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the DenseHead class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inp_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Input dimension.</p></li>
<li><p>shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): Output shape.</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of layers.</p></li>
<li><p>units (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of units.</p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Activation function.</p></li>
<li><p>norm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Normalization function.</p></li>
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Distribution function.</p></li>
<li><p>std (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Standard deviation.</p></li>
<li><p>outscale (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Output scale.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Device.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#DenseHead.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute the forward of DenseHead.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.DenseHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.DenseHead.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="actionhead">
<h3>ActionHead<a class="headerlink" href="#actionhead" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">ActionHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act=&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm=&lt;class</span> <span class="pre">'torch.nn.modules.normalization.LayerNorm'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist='trunc_normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_std=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_std=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_std=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outscale=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unimix_ratio=0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ActionHead"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>ActionHead Network for action head of dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act=&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm=&lt;class</span> <span class="pre">'torch.nn.modules.normalization.LayerNorm'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist='trunc_normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_std=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_std=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_std=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outscale=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unimix_ratio=0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ActionHead.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ActionHead class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inp_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Input dimension.</p></li>
<li><p>size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Output size.</p></li>
<li><p>layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of layers.</p></li>
<li><p>units (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of units.</p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Activation function.</p></li>
<li><p>norm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Normalization function.</p></li>
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Distribution function.</p></li>
<li><p>init_std (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Initial standard deviation.</p></li>
<li><p>min_std (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Minimum standard deviation.</p></li>
<li><p>max_std (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Maximum standard deviation.</p></li>
<li><p>temp (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Temperature.</p></li>
<li><p>outscale (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Output scale.</p></li>
<li><p>unimix_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Unimix ratio.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ActionHead.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute the forward of ActionHead.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ActionHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.dreamer.ActionHead.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="sampledist">
<h3>SampleDist<a class="headerlink" href="#sampledist" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SampleDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">SampleDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SampleDist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SampleDist" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of sample Dist for ActionHead of dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SampleDist.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SampleDist.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SampleDist.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the SampleDist class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Distribution.</p></li>
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of samples.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SampleDist.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SampleDist.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SampleDist.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the entropy of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SampleDist.mean">
<span class="sig-name descname"><span class="pre">mean</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SampleDist.mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SampleDist.mean" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mean of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SampleDist.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SampleDist.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SampleDist.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="onehotdist">
<h3>OneHotDist<a class="headerlink" href="#onehotdist" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.OneHotDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">OneHotDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unimix_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#OneHotDist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.OneHotDist" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of onehot Dist for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.OneHotDist.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unimix_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#OneHotDist.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.OneHotDist.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the OneHotDist class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Logits.</p></li>
<li><p>probs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Probabilities.</p></li>
<li><p>unimix_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Unimix ratio.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.OneHotDist.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#OneHotDist.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.OneHotDist.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.OneHotDist.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#OneHotDist.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.OneHotDist.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>sample_shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): Sample shape.</p></li>
<li><p>seed (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Seed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="twohotdistsymlog">
<h3>TwoHotDistSymlog<a class="headerlink" href="#twohotdistsymlog" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">TwoHotDistSymlog</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">low</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">high</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of twohotsymlog Dist for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob_target</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">low</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">high</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the TwoHotDistSymlog class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Logits.</p></li>
<li><p>low (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Low.</p></li>
<li><p>high (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): High.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Device.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the log probability of the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob_target">
<span class="sig-name descname"><span class="pre">log_prob_target</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog.log_prob_target"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob_target" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the log probability of the target.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>target (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Target tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog.mean">
<span class="sig-name descname"><span class="pre">mean</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog.mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.mean" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mean of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.TwoHotDistSymlog.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#TwoHotDistSymlog.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="symlogdist">
<h3>SymlogDist<a class="headerlink" href="#symlogdist" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SymlogDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">SymlogDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sum'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_to_reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[-1,</span> <span class="pre">-2,</span> <span class="pre">-3]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SymlogDist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SymlogDist" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of Symlog Dist for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SymlogDist.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aggregation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sum'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_to_reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[-1,</span> <span class="pre">-2,</span> <span class="pre">-3]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SymlogDist.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SymlogDist.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the SymlogDist class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>mode (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Mode.</p></li>
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Distribution function.</p></li>
<li><p>aggregation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Aggregation function.</p></li>
<li><p>tol (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Tolerance.</p></li>
<li><p>dim_to_reduce (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): Dimension to reduce.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SymlogDist.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SymlogDist.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SymlogDist.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the log probability of the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SymlogDist.mean">
<span class="sig-name descname"><span class="pre">mean</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SymlogDist.mean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SymlogDist.mean" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mean of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.SymlogDist.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#SymlogDist.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.SymlogDist.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="contdist">
<h3>ContDist<a class="headerlink" href="#contdist" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">ContDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of ordinary Dist for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ContDist class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Distribution.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the entropy of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.ContDist.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#ContDist.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.ContDist.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>sample_shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): Sample shape.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bernoulli">
<h3>Bernoulli<a class="headerlink" href="#bernoulli" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.dreamer.</span></span><span class="sig-name descname"><span class="pre">Bernoulli</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A kind of Bernoulli Dist for dreamerv3.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code>, <code class="docutils literal notranslate"><span class="pre">log_prob</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Bernoulli distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Distribution.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the entropy of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli.log_prob"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the log probability of the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the mode of the distribution.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.dreamer.Bernoulli.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/dreamer.html#Bernoulli.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.dreamer.Bernoulli.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from the distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>sample_shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): Sample shape.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="network-gtrxl">
<h2>network.gtrxl<a class="headerlink" href="#network-gtrxl" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/gtrxl</span></code> for more details.</p>
<section id="positionalembedding">
<h3>PositionalEmbedding<a class="headerlink" href="#positionalembedding" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">PositionalEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#PositionalEmbedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The PositionalEmbedding module implements the positional embedding used in the vanilla Transformer model.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This implementation is adapted from <a class="reference external" href="https://github.com/kimiyoung/transformer-xl/blob/">https://github.com/kimiyoung/transformer-xl/blob/</a>             master/pytorch/mem_transformer.py</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#PositionalEmbedding.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the PositionalEmbedding module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>embedding_dim: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimensionality of the embeddings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pos_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#PositionalEmbedding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute positional embedding given a sequence of positions.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>pos_seq (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The positional sequence,                 typically a 1D tensor of integers in the form of [seq_len-1, seq_len-2, …, 1, 0],</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>pos_embedding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The computed positional embeddings.                 The shape of the tensor is (seq_len, 1, embedding_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.PositionalEmbedding.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="grugatingunit">
<h3>GRUGatingUnit<a class="headerlink" href="#grugatingunit" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">GRUGatingUnit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GRUGatingUnit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The GRUGatingUnit module implements the GRU gating mechanism used in the GTrXL model.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GRUGatingUnit.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the GRUGatingUnit module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimensionality of the input.</p></li>
<li><p>bg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bg</span></code>): The gate bias. By setting bg &gt; 0 we can explicitly initialize the gating mechanism to                 be close to the identity map. This can greatly improve the learning speed and stability since it                 initializes the agent close to a Markovian policy (ignore attention at the beginning).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GRUGatingUnit.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the output value using the GRU gating mechanism.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The first input tensor.</p></li>
<li><p>y: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The second input tensor.                 x and y should have the same shape and their last dimension should match the input_dim.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>g: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output of the GRU gating mechanism.                 The shape of g matches the shapes of x and y.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GRUGatingUnit.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="memory">
<h3>Memory<a class="headerlink" href="#memory" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">Memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A class that stores the context used to add memory to Transformer.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">init</span></code>, <code class="docutils literal notranslate"><span class="pre">update</span></code>, <code class="docutils literal notranslate"><span class="pre">get</span></code>, <code class="docutils literal notranslate"><span class="pre">to</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For details, refer to Transformer-XL: <a class="reference external" href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Memory module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>memory_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of memory, i.e., how many past observations to use as memory.</p></li>
<li><p>batch_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of each batch.</p></li>
<li><p>embedding_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of embedding, which is the dimension of a single observation                 after embedding.</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of transformer layers.</p></li>
<li><p>memory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The initial memory. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory.get"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory.get" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the current memory.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>memory: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The current memory,                 with shape (layer_num, memory_len, bs, embedding_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory.init">
<span class="sig-name descname"><span class="pre">init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory.init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory.init" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize memory with an input list of tensors or create it automatically given its dimensions.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>memory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): Input memory tensor with shape                 (layer_num, memory_len, bs, embedding_dim). Its shape is (layer_num, memory_len, bs, embedding_dim),                 where memory_len is length of memory, bs is batch size and embedding_dim is the dimension of embedding.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory.to"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory.to" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Move the current memory to the specified device.</p>
</dd>
<dt>Arguments:</dt><dd><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The device to move the memory to. Default is ‘cpu’.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.Memory.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#Memory.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.Memory.update" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Update the memory given a sequence of hidden states.
Example for single layer:</p>
<blockquote>
<div><p>memory_len=3, hidden_size_len=2, bs=3</p>
<blockquote>
<div><p>m00 m01 m02      h00 h01 h02              m20 m21 m22</p>
</div></blockquote>
<dl class="simple">
<dt>m = m10 m11 m12  h = h10 h11 h12  =&gt; new_m =  h00 h01 h02</dt><dd><p>m20 m21 m22                               h10 h11 h12</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>hidden_state: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.Tensor]</span></code>): The hidden states to update the memory.                 Each tensor in the list has shape (cur_seq, bs, embedding_dim), where cur_seq                 is the length of the sequence.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>memory: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The updated memory, with shape                 (layer_num, memory_len, bs, embedding_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="attentionxl">
<h3>AttentionXL<a class="headerlink" href="#attentionxl" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">AttentionXL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#AttentionXL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>An implementation of the Attention mechanism used in the TransformerXL model.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#AttentionXL.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the AttentionXL module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimensionality of the input features.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimensionality of each attention head.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of attention heads.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The dropout layer to use</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._rel_shift">
<span class="sig-name descname"><span class="pre">_rel_shift</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_upper</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#AttentionXL._rel_shift"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._rel_shift" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Perform a relative shift operation on the attention score matrix.
Example:</p>
<blockquote>
<div><p>a00 a01 a02      0 a00 a01 a02       0  a00 a01      a02  0  a10     a02  0   0
a10 a11 a12  =&gt;  0 a10 a11 a12  =&gt;  a02  0  a10  =&gt;  a11 a12  0  =&gt;  a11 a12  0
a20 a21 a22      0 a20 a21 a22      a11 a12  0       a20 a21 a22     a20 a21 a22</p>
<blockquote>
<div><p>a20 a21 a22</p>
</div></blockquote>
<ol class="arabic simple">
<li><p>Append one “column” of zeros to the left</p></li>
<li><p>Reshape the matrix from [3 x 4] into [4 x 3]</p></li>
<li><p>Remove the first “row”</p></li>
<li><p>Mask out the upper triangle (optional)</p></li>
</ol>
</div></blockquote>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>See the following material for better understanding:</dt><dd><p><a class="reference external" href="https://github.com/kimiyoung/transformer-xl/issues/8">https://github.com/kimiyoung/transformer-xl/issues/8</a>
<a class="reference external" href="https://arxiv.org/pdf/1901.02860.pdf">https://arxiv.org/pdf/1901.02860.pdf</a> (Appendix B)</p>
</dd>
</dl>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor with shape (cur_seq, full_seq, bs, head_num).</p></li>
<li><p>zero_upper (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): If True, the upper-right triangle of the matrix is set to zero.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor after the relative shift operation,                 with shape (cur_seq, full_seq, bs, head_num).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">full_input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">u</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#AttentionXL.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the forward pass for the AttentionXL module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The attention input with shape (cur_seq, bs, input_dim).</p></li>
<li><p>pos_embedding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The positional embedding with shape (full_seq, 1, full_seq).</p></li>
<li><p>full_input (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The concatenated memory and input tensor with shape                 (full_seq, bs, input_dim).</p></li>
<li><p>u (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>): The content parameter with shape (head_num, head_dim).</p></li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>): The position parameter with shape (head_num, head_dim).</p></li>
<li><p>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The attention mask with shape (cur_seq, full_seq, 1).                 If None, no masking is applied.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output of the attention mechanism with shape (cur_seq, bs, input_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.AttentionXL.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.AttentionXL.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gatedtransformerxllayer">
<h3>GatedTransformerXLLayer<a class="headerlink" href="#gatedtransformerxllayer" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">GatedTransformerXLLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_gating</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GatedTransformerXLLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This class implements the attention layer of GTrXL (Gated Transformer-XL).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_gating</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GatedTransformerXLLayer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize GatedTransformerXLLayer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input tensor.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of each head in the multi-head attention.</p></li>
<li><p>hidden_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the hidden layer in the MLP.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of heads for the multi-head attention.</p></li>
<li><p>mlp_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of MLP layers in the attention layer.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The dropout module used in the MLP and attention layers.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The activation function to be used in the MLP layers.</p></li>
<li><p>gru_gating (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use GRU gates. If False, replace GRU gates with                 residual connections. Default is True.</p></li>
<li><p>gru_bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The bias of the GRU gate. Default is 2.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_embedding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">u</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GatedTransformerXLLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute forward pass of GTrXL layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The attention input tensor of shape (cur_seq, bs, input_dim).</p></li>
<li><p>pos_embedding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The positional embedding tensor of shape (full_seq, 1, full_seq).</p></li>
<li><p>u (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>): The content parameter tensor of shape (head_num, head_dim).</p></li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code>): The position parameter tensor of shape (head_num, head_dim).</p></li>
<li><p>memory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The memory tensor of shape (prev_seq, bs, input_dim).</p></li>
<li><dl class="simple">
<dt>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The attention mask tensor of shape (cur_seq, full_seq, 1).</dt><dd><p>Default is None.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): layer output of shape (cur_seq, bs, input_dim)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gtrxl">
<h3>GTrXL<a class="headerlink" href="#gtrxl" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gtrxl.</span></span><span class="sig-name descname"><span class="pre">GTrXL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_gating</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_embedding_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GTrXL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>GTrXL Transformer implementation as described in “Stabilizing Transformer for Reinforcement Learning”
(<a class="reference external" href="https://arxiv.org/abs/1910.06764">https://arxiv.org/abs/1910.06764</a>).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">reset_memory</span></code>, <code class="docutils literal notranslate"><span class="pre">get_memory</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_gating</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gru_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_embedding_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GTrXL.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init GTrXL Model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input observation.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The dimension of each head. Default is 128.</p></li>
<li><p>embedding_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The dimension of the embedding. Default is 256.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The number of heads for multi-head attention. Default is 2.</p></li>
<li><p>mlp_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The number of MLP layers in the attention layer. Default is 2.</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The number of transformer layers. Default is 3.</p></li>
<li><p>memory_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The length of memory. Default is 64.</p></li>
<li><p>dropout_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The dropout ratio. Default is 0.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): The activation function. Default is nn.ReLU().</p></li>
<li><p>gru_gating (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If False, replace GRU gates with residual connections.                 Default is True.</p></li>
<li><p>gru_bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The GRU gate bias. Default is 2.0.</p></li>
<li><p>use_embedding_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If False, don’t use input embedding layer. Default is True.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><ul class="simple">
<li><p>AssertionError: If <cite>embedding_dim</cite> is not an even number.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_mem</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GTrXL.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Performs a forward pass on the GTrXL.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor with shape (seq_len, bs, input_size).</p></li>
<li><p>batch_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If the input data has shape (bs, seq_len, input_size),                 set this parameter to True to transpose along the first and second dimension and obtain shape                 (seq_len, bs, input_size). This does not affect the output memory. Default is False.             - return_mem (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If False, return only the output tensor without dict. Default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): A dictionary containing the transformer output of shape              (seq_len, bs, embedding_size) and memory of shape (layer_num, seq_len, bs, embedding_size).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL.get_memory">
<span class="sig-name descname"><span class="pre">get_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GTrXL.get_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL.get_memory" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Returns the memory of GTrXL.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>memory (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The output memory or None if memory has not been initialized.                 The shape is (layer_num, memory_len, bs, embedding_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL.reset_memory">
<span class="sig-name descname"><span class="pre">reset_memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/gtrxl.html#GTrXL.reset_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL.reset_memory" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Clear or set the memory of GTrXL.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>batch_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): The batch size. Default is None.</p></li>
<li><p>state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The input memory with shape                 (layer_num, memory_len, bs, embedding_dim). Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gtrxl.GTrXL.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gtrxl.GTrXL.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-gumbel-softmax">
<h2>network.gumbel_softmax<a class="headerlink" href="#network-gumbel-softmax" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/gumbel_softmax</span></code> for more details.</p>
<section id="gumbelsoftmax">
<h3>GumbelSoftmax<a class="headerlink" href="#gumbelsoftmax" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.gumbel_softmax.</span></span><span class="sig-name descname"><span class="pre">GumbelSoftmax</span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gumbel_softmax.html#GumbelSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>An <cite>nn.Module</cite> that computes GumbelSoftmax.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">gumbel_softmax_sample</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on GumbelSoftmax, refer to the paper [Categorical Reparameterization         with Gumbel-Softmax](<a class="reference external" href="https://arxiv.org/abs/1611.01144">https://arxiv.org/abs/1611.01144</a>).</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gumbel_softmax.html#GumbelSoftmax.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the <cite>GumbelSoftmax</cite> module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hard</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gumbel_softmax.html#GumbelSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass for the <cite>GumbelSoftmax</cite> module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Unnormalized log-probabilities.</p></li>
<li><p>temperature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Non-negative scalar controlling the sharpness of the distribution.</p></li>
<li><p>hard (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): If <cite>True</cite>, returns one-hot encoded labels. Default is <cite>False</cite>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Sample from Gumbel-Softmax distribution.</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>x: its shape is <span class="math notranslate nohighlight">\((B, N)\)</span>, where <cite>B</cite> is the batch size and <cite>N</cite> is the number of classes.</p></li>
<li><p>y: its shape is <span class="math notranslate nohighlight">\((B, N)\)</span>, where <cite>B</cite> is the batch size and <cite>N</cite> is the number of classes.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.gumbel_softmax_sample">
<span class="sig-name descname"><span class="pre">gumbel_softmax_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/gumbel_softmax.html#GumbelSoftmax.gumbel_softmax_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.gumbel_softmax_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Draw a sample from the Gumbel-Softmax distribution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
<li><p>temperature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Non-negative scalar controlling the sharpness of the distribution.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Small number to prevent division by zero, default is <cite>1e-8</cite>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Sample from Gumbel-Softmax distribution.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-merge">
<h2>network.merge<a class="headerlink" href="#network-merge" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/merge</span></code> for more details.</p>
<section id="bilineargeneral">
<h3>BilinearGeneral<a class="headerlink" href="#bilineargeneral" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">BilinearGeneral</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in1_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in2_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#BilinearGeneral"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Bilinear implementation as in: Multiplicative Interactions and Where to Find Them,
ICLR 2020, <a class="reference external" href="https://openreview.net/forum?id=rylnK6VtDH">https://openreview.net/forum?id=rylnK6VtDH</a>.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in1_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in2_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#BilinearGeneral.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Bilinear layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in1_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each first input sample.</p></li>
<li><p>in2_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each second input sample.</p></li>
<li><p>out_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each output sample.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#BilinearGeneral.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>compute the bilinear function.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The first input tensor.</p></li>
<li><p>z (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The second input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#BilinearGeneral.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the parameters of the Bilinear layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.BilinearGeneral.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.BilinearGeneral.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="torchbilinearcustomized">
<h3>TorchBilinearCustomized<a class="headerlink" href="#torchbilinearcustomized" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">TorchBilinearCustomized</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in1_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in2_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#TorchBilinearCustomized"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Customized Torch Bilinear implementation.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in1_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in2_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#TorchBilinearCustomized.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Bilinear layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in1_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each first input sample.</p></li>
<li><p>in2_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each second input sample.</p></li>
<li><p>out_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of each output sample.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#TorchBilinearCustomized.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the bilinear function.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The first input tensor.</p></li>
<li><p>z (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The second input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#TorchBilinearCustomized.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the parameters of the Bilinear layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.TorchBilinearCustomized.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="film">
<h3>FiLM<a class="headerlink" href="#film" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">FiLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#FiLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Feature-wise Linear Modulation (FiLM) Layer.
This layer applies feature-wise affine transformation based on context.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#FiLM.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the FiLM layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>feature_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>). The dimension of the input feature vector.</p></li>
<li><p>context_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>). The dimension of the input context vector.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#FiLM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward propagation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>feature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>). The input feature, shape (batch_size, feature_dim).</p></li>
<li><p>context (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>). The input context, shape (batch_size, context_dim).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>conditioned_feature : torch.Tensor. The output feature after FiLM, shape (batch_size, feature_dim).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.FiLM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.FiLM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gatingtype">
<h3>GatingType<a class="headerlink" href="#gatingtype" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.GatingType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">GatingType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#GatingType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.GatingType" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Enum class defining different types of tensor gating and aggregation in modules.</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.GatingType.GLOBAL">
<span class="sig-name descname"><span class="pre">GLOBAL</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'global'</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.GatingType.GLOBAL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.GatingType.NONE">
<span class="sig-name descname"><span class="pre">NONE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'none'</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.GatingType.NONE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.GatingType.POINTWISE">
<span class="sig-name descname"><span class="pre">POINTWISE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'pointwise'</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.GatingType.POINTWISE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="summerge">
<h3>SumMerge<a class="headerlink" href="#summerge" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">SumMerge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#SumMerge"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A PyTorch module that merges a list of tensors by computing their sum. All input tensors must have the same
size. This module can work with any type of tensor (vector, units or visual).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#SumMerge.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass of the SumMerge module, which sums the input tensors.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>tensors (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tensor]</span></code>): List of input tensors to be summed. All tensors must have the same size.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>summed (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): Tensor resulting from the sum of all input tensors.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.SumMerge.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.SumMerge.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="vectormerge">
<h3>VectorMerge<a class="headerlink" href="#vectormerge" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.merge.</span></span><span class="sig-name descname"><span class="pre">VectorMerge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gating_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType" title="ding.torch_utils.network.merge.GatingType"><span class="pre">GatingType</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">GatingType.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_layer_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#VectorMerge"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Merges multiple vector streams. Streams are first transformed through layer normalization, relu, and linear
layers, then summed. They don’t need to have the same size. Gating can also be used before the sum.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">encode</span></code>, <code class="docutils literal notranslate"><span class="pre">_compute_gate</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more details about the gating types, please refer to the GatingType enum class.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gating_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType" title="ding.torch_utils.network.merge.GatingType"><span class="pre">GatingType</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">GatingType.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_layer_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#VectorMerge.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the <cite>VectorMerge</cite> module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_sizes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">int]</span></code>): A dictionary mapping input names to their sizes.                 The size is a single integer for 1D inputs, or <cite>None</cite> for 0D inputs.                 If an input size is <cite>None</cite>, we assume it’s <cite>()</cite>.</p></li>
<li><p>output_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the output vector.</p></li>
<li><p>gating_type (<a class="reference internal" href="#ding.torch_utils.network.merge.GatingType" title="ding.torch_utils.network.merge.GatingType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GatingType</span></code></a>): The type of gating mechanism to use. Default is <cite>GatingType.NONE</cite>.</p></li>
<li><p>use_layer_norm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use layer normalization. Default is <cite>True</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._compute_gate">
<span class="sig-name descname"><span class="pre">_compute_gate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#VectorMerge._compute_gate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._compute_gate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the gate values based on the initial gate values.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>init_gate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tensor]</span></code>): The initial gate values.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>gate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tensor]</span></code>): The computed gate values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#VectorMerge.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge.encode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Encode the input tensors using layer normalization, relu, and linear transformations.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Tensor]</span></code>): The input tensors.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>gates (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tensor]</span></code>): The gate tensors after transformations.</p></li>
<li><p>outputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Tensor]</span></code>): The output tensors after transformations.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/merge.html#VectorMerge.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass through the VectorMerge module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Tensor]</span></code>): The input tensors.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The output tensor after passing through the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.merge.VectorMerge.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.merge.VectorMerge.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-nn-module">
<h2>network.nn_module<a class="headerlink" href="#network-nn-module" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/nn_module</span></code> for more details.</p>
<section id="weight-init">
<h3><a class="reference internal" href="#weight-init">weight_init</a><a class="headerlink" href="#weight-init" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.weight_init_">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">weight_init_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'xavier'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#weight_init_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.weight_init_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize weight according to the specified type.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The weight that needs to be initialized.</p></li>
<li><p>init_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The type of initialization to implement,             supports [“xavier”, “kaiming”, “orthogonal”].</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The activation function name. Recommended to use only with             [‘relu’, ‘leaky_relu’].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="sequential-pack">
<h3>sequential_pack<a class="headerlink" href="#sequential-pack" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.sequential_pack">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">sequential_pack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#sequential_pack"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.sequential_pack" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Pack the layers in the input list to a <cite>nn.Sequential</cite> module.
If there is a convolutional layer in module, an extra attribute <cite>out_channels</cite> will be added
to the module and set to the out_channel of the conv layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[nn.Module]</span></code>): The input list of layers.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>seq (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): Packed sequential container.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="conv1d-block">
<h3>conv1d_block<a class="headerlink" href="#conv1d-block" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.conv1d_block">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">conv1d_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#conv1d_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.conv1d_block" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a 1-dimensional convolution layer with activation and normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Stride of the convolution. Default is 1.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Zero-padding added to both sides of the input. Default is 0.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Spacing between kernel elements. Default is 1.</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of blocked connections from input channels to output channels.             Default is 1.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Type of the normalization.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the 1-dimensional             convolution layer.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Conv1d (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d</a>)</p>
</div>
</dd></dl>

</section>
<section id="conv2d-block">
<h3>conv2d_block<a class="headerlink" href="#conv2d-block" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.conv2d_block">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">conv2d_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'zero'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_groups_for_gn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#conv2d_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.conv2d_block" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a 2-dimensional convolution layer with activation and normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Stride of the convolution. Default is 1.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Zero-padding added to both sides of the input. Default is 0.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Spacing between kernel elements.</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of blocked connections from input channels to output channels.             Default is 1.</p></li>
<li><p>pad_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The way to add padding, include [‘zero’, ‘reflect’, ‘replicate’].             Default is ‘zero’.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): the optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of the normalization, now support [‘BN’, ‘LN’, ‘IN’, ‘GN’, ‘SyncBN’],             default set to None, which means no normalization.</p></li>
<li><p>num_groups_for_gn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of groups for GroupNorm.</p></li>
<li><p>bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to add a learnable bias to the nn.Conv2d. Default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the 2-dimensional             convolution layer.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Conv2d (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a>)</p>
</div>
</dd></dl>

</section>
<section id="deconv2d-block">
<h3>deconv2d_block<a class="headerlink" href="#deconv2d-block" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.deconv2d_block">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">deconv2d_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#deconv2d_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.deconv2d_block" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a 2-dimensional transpose convolution layer with activation and normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the convolving kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Stride of the convolution. Default is 1.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Zero-padding added to both sides of the input. Default is 0.</p></li>
<li><p>output_padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Additional size added to one side of the output shape. Default is 0.</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of blocked connections from input channels to output channels.             Default is 1.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Type of the normalization.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the 2-dimensional             transpose convolution layer.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ConvTranspose2d (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html">https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html</a>)</p>
</div>
</dd></dl>

</section>
<section id="fc-block">
<h3>fc_block<a class="headerlink" href="#fc-block" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.fc_block">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">fc_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#fc_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.fc_block" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a fully-connected block with activation, normalization, and dropout.
Optional normalization can be done to the dim 1 (across the channels).
x -&gt; fc -&gt; norm -&gt; act -&gt; dropout -&gt; out</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Type of the normalization.</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use dropout in the fully-connected block. Default is False.</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Probability of an element to be zeroed in the dropout.             Default is 0.5.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the fully-connected block.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can refer to nn.linear (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">https://pytorch.org/docs/master/generated/torch.nn.Linear.html</a>).</p>
</div>
</dd></dl>

</section>
<section id="normed-linear">
<h3>normed_linear<a class="headerlink" href="#normed-linear" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.normed_linear">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">normed_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Linear</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#normed_linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.normed_linear" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a nn.Linear module but with normalized fan-in init.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of features in the input tensor.</p></li>
<li><p>out_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of features in the output tensor.</p></li>
<li><p>bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to add a learnable bias to the nn.Linear. Default is True.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): The device to put the created module on. Default is None.</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): The desired data type of created module. Default is None.</p></li>
<li><p>scale (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The scale factor for initialization. Default is 1.0.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>out (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Linear</span></code>): A nn.Linear module with normalized fan-in init.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="normed-conv2d">
<h3>normed_conv2d<a class="headerlink" href="#normed-conv2d" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.normed_conv2d">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">normed_conv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Conv2d</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#normed_conv2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.normed_conv2d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a nn.Conv2d module but with normalized fan-in init.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">Tuple[int,</span> <span class="pre">int]]</span></code>): Size of the convolving kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">Tuple[int,</span> <span class="pre">int]]</span></code>, optional): Stride of the convolution. Default is 1.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">Tuple[int,</span> <span class="pre">int]]</span></code>, optional): Zero-padding added to both sides of the input.             Default is 0.</p></li>
<li><p>dilation (:<cite>Union[int, Tuple[int, int]]</cite>, optional): Spacing between kernel elements. Default is 1.</p></li>
<li><p>groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of blocked connections from input channels to output channels.             Default is 1.</p></li>
<li><p>bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to add a learnable bias to the nn.Conv2d. Default is True.</p></li>
<li><p>padding_mode (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The type of padding algorithm to use. Default is ‘zeros’.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): The device to put the created module on. Default is None.</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): The desired data type of created module. Default is None.</p></li>
<li><p>scale (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The scale factor for initialization. Default is 1.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>out (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Conv2d</span></code>): A nn.Conv2d module with normalized fan-in init.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="mlp">
<h3>MLP<a class="headerlink" href="#mlp" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.MLP">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">MLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_linear_layer_init_zero</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#MLP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.MLP" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a multi-layer perceptron using fully-connected blocks with activation, normalization, and dropout,
optional normalization can be done to the dim 1 (across the channels).
x -&gt; fc -&gt; norm -&gt; act -&gt; dropout -&gt; out</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>hidden_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the hidden tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of layers.</p></li>
<li><p>layer_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>, optional): Layer function.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The type of the normalization.</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use dropout in the fully-connected block. Default is False.</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Probability of an element to be zeroed in the dropout.             Default is 0.5.</p></li>
<li><p>output_activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use activation in the output layer. If True,             we use the same activation as front layers. Default is True.</p></li>
<li><p>output_norm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use normalization in the output layer. If True,             we use the same normalization as front layers. Default is True.</p></li>
<li><p>last_linear_layer_init_zero (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use zero initializations for the last             linear layer (including w and b), which can provide stable zero outputs in the beginning,             usually used in the policy network in RL settings.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the multi-layer perceptron.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you can refer to nn.linear (<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.Linear.html">https://pytorch.org/docs/master/generated/torch.nn.Linear.html</a>).</p>
</div>
</dd></dl>

</section>
<section id="channelshuffle">
<h3>ChannelShuffle<a class="headerlink" href="#channelshuffle" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">ChannelShuffle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#ChannelShuffle"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Apply channel shuffle to the input tensor. For more details about the channel shuffle,
please refer to the ‘ShuffleNet’ paper: <a class="reference external" href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a></p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#ChannelShuffle.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ChannelShuffle class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>group_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of groups to exchange.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#ChannelShuffle.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass through the ChannelShuffle module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The shuffled input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.ChannelShuffle.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.ChannelShuffle.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="one-hot">
<h3>one_hot<a class="headerlink" href="#one-hot" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.one_hot">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">one_hot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">FloatTensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#one_hot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert a torch.LongTensor to one-hot encoding. This implementation can be slightly faster than
<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.one_hot</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Each element contains the state to be encoded, the range should be [0, num-1]</p></li>
<li><p>num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of states of the one-hot encoding</p></li>
<li><p>num_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If False, the one-hot encoding is added as the last dimension; otherwise,             it is added as the first dimension. Default is False.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>one_hot (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): The one-hot encoded tensor.</p></li>
</ul>
</dd>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">one_hot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 1.],</span>
<span class="go">         [0., 0., 1.]],</span>
<span class="go">        [[0., 0., 1.],</span>
<span class="go">         [0., 0., 1.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one_hot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span><span class="mi">3</span><span class="p">,</span><span class="n">num_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[[0., 0.], [1., 0.]],</span>
<span class="go">        [[0., 1.], [0., 0.]],</span>
<span class="go">        [[1., 0.], [0., 1.]]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="nearestupsample">
<h3>NearestUpsample<a class="headerlink" href="#nearestupsample" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">NearestUpsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NearestUpsample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This module upsamples the input to the given scale_factor using the nearest mode.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NearestUpsample.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the NearestUpsample class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>scale_factor (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[float,</span> <span class="pre">List[float]]</span></code>): The multiplier for the spatial size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NearestUpsample.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the upsampled input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>upsample(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The upsampled input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NearestUpsample.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NearestUpsample.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="bilinearupsample">
<h3>BilinearUpsample<a class="headerlink" href="#bilinearupsample" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">BilinearUpsample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#BilinearUpsample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This module upsamples the input to the given scale_factor using the bilinear mode.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#BilinearUpsample.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the BilinearUpsample class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>scale_factor (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[float,</span> <span class="pre">List[float]]</span></code>): The multiplier for the spatial size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#BilinearUpsample.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the upsampled input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>upsample(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The upsampled input tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.BilinearUpsample.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.BilinearUpsample.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="binary-encode">
<h3>binary_encode<a class="headerlink" href="#binary-encode" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.binary_encode">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">binary_encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#binary_encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.binary_encode" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert elements in a tensor to its binary representation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>y (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The tensor to be converted into its binary representation.</p></li>
<li><p>max_val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The maximum value of the elements in the tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>binary (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor in its binary representation.</p></li>
</ul>
</dd>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_encode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="go">tensor([[0, 0, 1, 1],[0, 0, 1, 0]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="noiselinearlayer">
<h3>NoiseLinearLayer<a class="headerlink" href="#noiselinearlayer" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">NoiseLinearLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This is a linear layer with random noise.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">reset_noise</span></code>, <code class="docutils literal notranslate"><span class="pre">reset_parameters</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the NoiseLinearLayer class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>sigma0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Default noise volume when initializing NoiseLinearLayer.                 Default is 0.4.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._scale_noise">
<span class="sig-name descname"><span class="pre">_scale_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer._scale_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._scale_noise" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Scale the noise.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">Tuple]</span></code>): The size of the noise.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Perform the forward pass with noise.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor with noise.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_noise">
<span class="sig-name descname"><span class="pre">reset_noise</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.reset_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_noise" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset the noise settings in the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NoiseLinearLayer.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset the parameters in the layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NoiseLinearLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="noise-block">
<h3>noise_block<a class="headerlink" href="#noise-block" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.noise_block">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">noise_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#noise_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.noise_block" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a fully-connected noise layer with activation, normalization, and dropout.
Optional normalization can be done to the dim 1 (across the channels).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The optional activation function. Default is None.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Type of normalization. Default is None.</p></li>
<li><p>use_dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use dropout in the fully-connected block.</p></li>
<li><p>dropout_probability (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Probability of an element to be zeroed in the dropout.             Default is 0.5.</p></li>
<li><p>sigma0 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The sigma0 is the default noise volume when initializing NoiseLinearLayer.             Default is 0.4.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Sequential</span></code>): A sequential list containing the torch layers of the fully-connected block.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="naiveflatten">
<h3>NaiveFlatten<a class="headerlink" href="#naiveflatten" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.nn_module.</span></span><span class="sig-name descname"><span class="pre">NaiveFlatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NaiveFlatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This module is a naive implementation of the flatten operation.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NaiveFlatten.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the NaiveFlatten class.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>start_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first dimension to flatten. Default is 1.</p></li>
<li><p>end_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The last dimension to flatten. Default is -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/nn_module.html#NaiveFlatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Perform the flatten operation on the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The flattened output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.nn_module.NaiveFlatten.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.nn_module.NaiveFlatten.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-normalization">
<h2>network.normalization<a class="headerlink" href="#network-normalization" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/normalization</span></code> for more details.</p>
<section id="build-normalization">
<h3>build_normalization<a class="headerlink" href="#build-normalization" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.normalization.build_normalization">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.normalization.</span></span><span class="sig-name descname"><span class="pre">build_normalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/normalization.html#build_normalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.normalization.build_normalization" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Construct the corresponding normalization module. For beginners,
refer to [this article](<a class="reference external" href="https://zhuanlan.zhihu.com/p/34879333">https://zhuanlan.zhihu.com/p/34879333</a>) to learn more about batch normalization.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Type of the normalization. Currently supports [‘BN’, ‘LN’, ‘IN’, ‘SyncBN’].</p></li>
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): Dimension of the normalization, applicable when norm_type is in [‘BN’, ‘IN’].</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>norm_func (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The corresponding batch normalization function.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="network-popart">
<h2>network.popart<a class="headerlink" href="#network-popart" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/popart</span></code> for more details.</p>
<section id="popart">
<h3>PopArt<a class="headerlink" href="#popart" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.popart.</span></span><span class="sig-name descname"><span class="pre">PopArt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/popart.html#PopArt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A linear layer with popart normalization. This class implements a linear transformation followed by
PopArt normalization, which is a method to automatically adapt the contribution of each task to the agent’s
updates in multi-task learning, as described in the paper &lt;<a class="reference external" href="https://arxiv.org/abs/1809.04474">https://arxiv.org/abs/1809.04474</a>&gt;.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">reset_parameters</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">update_parameters</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/popart.html#PopArt.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the class with input features, output features, and the beta parameter.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">None]</span></code>): The size of each input sample.</p></li>
<li><p>output_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[int,</span> <span class="pre">None]</span></code>): The size of each output sample.</p></li>
<li><p>beta (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The parameter for moving average.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/popart.html#PopArt.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implement the forward computation of the linear layer and return both the output and the
normalized output of the layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor which is to be normalized.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): A dictionary contains ‘pred’ and ‘unnormalized_pred’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/popart.html#PopArt.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset the parameters including weights and bias using <code class="docutils literal notranslate"><span class="pre">kaiming_uniform_</span></code> and <code class="docutils literal notranslate"><span class="pre">uniform_</span></code> initialization.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.popart.PopArt.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/popart.html#PopArt.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.popart.PopArt.update_parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Update the normalization parameters based on the given value and return the new mean and
standard deviation after the update.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The tensor to be used for updating parameters.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>update_results (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): A dictionary contains ‘new_mean’ and ‘new_std’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="network-res-block">
<h2>network.res_block<a class="headerlink" href="#network-res-block" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/res_block</span></code> for more details.</p>
<section id="resblock">
<h3>ResBlock<a class="headerlink" href="#resblock" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.res_block.</span></span><span class="sig-name descname"><span class="pre">ResBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">res_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><dl class="simple">
<dt>Residual Block with 2D convolution layers, including 3 types:</dt><dd><dl class="simple">
<dt>basic block:</dt><dd><p>input channel: C
x -&gt; 3*3*C -&gt; norm -&gt; act -&gt; 3*3*C -&gt; norm -&gt; act -&gt; out
__________________________________________/+</p>
</dd>
<dt>bottleneck block:</dt><dd><p>x -&gt; 1*1*(1/4*C) -&gt; norm -&gt; act -&gt; 3*3*(1/4*C) -&gt; norm -&gt; act -&gt; 1*1*C -&gt; norm -&gt; act -&gt; out
_____________________________________________________________________________/+</p>
</dd>
<dt>downsample block: used in EfficientZero</dt><dd><p>input channel: C
x -&gt; 3*3*C -&gt; norm -&gt; act -&gt; 3*3*C -&gt; norm -&gt; act -&gt; out
__________________ 3*3*C ____________________/+</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can refer to <a class="reference external" href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> for more         details.</p>
</div>
<dl class="simple">
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">res_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the 2D convolution residual block.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the input tensor.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Type of the normalization, default set to ‘BN’(Batch Normalization),                 supports [‘BN’, ‘LN’, ‘IN’, ‘GN’, ‘SyncBN’, None].</p></li>
<li><p>res_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Type of residual block, supports [‘basic’, ‘bottleneck’, ‘downsample’]</p></li>
<li><p>bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to add a learnable bias to the conv2d_block. default set to True.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of channels in the output tensor, default set to None,                 which means out_channels = in_channels.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the redisual block output.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The resblock output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="resfcblock">
<h3>ResFCBlock<a class="headerlink" href="#resfcblock" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.res_block.</span></span><span class="sig-name descname"><span class="pre">ResFCBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResFCBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Residual Block with 2 fully connected layers.
x -&gt; fc1 -&gt; norm -&gt; act -&gt; fc2 -&gt; norm -&gt; act -&gt; out
_____________________________________/+</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'BN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResFCBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the fully connected layer residual block.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of channels in the input tensor.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The optional activation function.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of the normalization, default set to ‘BN’.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The dropout rate, default set to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/res_block.html#ResFCBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the output of the redisual block.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The resblock output tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.res_block.ResFCBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.res_block.ResFCBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-resnet">
<h2>network.resnet<a class="headerlink" href="#network-resnet" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/resnet</span></code> for more details.</p>
<section id="to-2tuple">
<h3>to_2tuple<a class="headerlink" href="#to-2tuple" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.to_2tuple">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">to_2tuple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#to_2tuple"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.to_2tuple" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Convert a scalar to a 2-tuple or return the item if it’s not a scalar.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): An item to be converted to a 2-tuple.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): A 2-tuple of the item.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-same-padding">
<h3>get_same_padding<a class="headerlink" href="#get-same-padding" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.get_same_padding">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">get_same_padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#get_same_padding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.get_same_padding" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate asymmetric TensorFlow-like ‘SAME’ padding for a convolution.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the input.</p></li>
<li><p>k (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the kernel.</p></li>
<li><p>s (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The stride of the convolution.</p></li>
<li><p>d (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dilation of the convolution.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the padding.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="pad-same">
<h3>pad_same<a class="headerlink" href="#pad-same" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.pad_same">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">pad_same</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#pad_same"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.pad_same" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Dynamically pad input x with ‘SAME’ padding for conv with specified args.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The input tensor.</p></li>
<li><p>k (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The size of the kernel.</p></li>
<li><p>s (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The stride of the convolution.</p></li>
<li><p>d (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The dilation of the convolution.</p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Value to fill the padding.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The padded tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="avg-pool2d-same">
<h3>avg_pool2d_same<a class="headerlink" href="#avg-pool2d-same" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.avg_pool2d_same">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">avg_pool2d_same</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(0,</span> <span class="pre">0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#avg_pool2d_same"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.avg_pool2d_same" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Apply average pooling with ‘SAME’ padding on the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The input tensor.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The size of the kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The stride of the convolution.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The size of the padding.</p></li>
<li><p>ceil_mode (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): When True, will use ceil instead of floor to compute the output shape.</p></li>
<li><p>count_include_pad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): When True, will include the zero-padding in the averaging calculation.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The tensor after average pooling.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="avgpool2dsame">
<h3>AvgPool2dSame<a class="headerlink" href="#avgpool2dsame" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">AvgPool2dSame</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#AvgPool2dSame"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Tensorflow-like ‘SAME’ wrapper for 2D average pooling.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#AvgPool2dSame.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the AvgPool2dSame with given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the window to take an average over.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[Tuple[int,</span> <span class="pre">int]]</span></code>): The stride of the window. If None, default to kernel_size.</p></li>
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Implicit zero padding to be added on both sides.</p></li>
<li><p>ceil_mode (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): When True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape.</p></li>
<li><p>count_include_pad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): When True, will include the zero-padding in the averaging calculation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.ceil_mode">
<span class="sig-name descname"><span class="pre">ceil_mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.ceil_mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.count_include_pad">
<span class="sig-name descname"><span class="pre">count_include_pad</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.count_include_pad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#AvgPool2dSame.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass of the AvgPool2dSame.</p>
</dd>
<dt>Argument:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Output tensor after average pooling.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.kernel_size">
<span class="sig-name descname"><span class="pre">kernel_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.kernel_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.AvgPool2dSame.stride">
<span class="sig-name descname"><span class="pre">stride</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.AvgPool2dSame.stride" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="create-classifier">
<h3>create_classifier<a class="headerlink" href="#create-classifier" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.create_classifier">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">create_classifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'avg'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_conv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#create_classifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.create_classifier" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a classifier with global pooling layer and fully connected layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>num_features (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of features.</p></li>
<li><p>num_classes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of classes for the final classification.</p></li>
<li><p>pool_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of pooling to use; ‘avg’ for Average Pooling.</p></li>
<li><p>use_conv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use convolution or not.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>global_pool (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The created global pooling layer.</p></li>
<li><p>fc (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The created fully connected layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="classifierhead">
<h3>ClassifierHead<a class="headerlink" href="#classifierhead" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">ClassifierHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_chs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'avg'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_conv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ClassifierHead"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Classifier head with configurable global pooling and dropout.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_chs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'avg'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_conv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ClassifierHead.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ClassifierHead with given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_chs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of input channels.</p></li>
<li><p>num_classes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of classes for the final classification.</p></li>
<li><p>pool_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The type of pooling to use; ‘avg’ for Average Pooling.</p></li>
<li><p>drop_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The dropout rate.</p></li>
<li><p>use_conv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use convolution or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ClassifierHead.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass of the ClassifierHead.</p>
</dd>
<dt>Argument:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Output tensor after classification.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ClassifierHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ClassifierHead.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="create-attn">
<h3>create_attn<a class="headerlink" href="#create-attn" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.create_attn">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">create_attn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plane</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#create_attn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.create_attn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create an attention mechanism.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The layer where the attention is to be applied.</p></li>
<li><p>plane (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The plane on which the attention is to be applied.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-padding">
<h3>get_padding<a class="headerlink" href="#get-padding" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.get_padding">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">get_padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#get_padding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.get_padding" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the padding based on the kernel size, stride and dilation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The stride of the convolution.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dilation factor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>padding (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The computed padding.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="basicblock">
<h3>BasicBlock<a class="headerlink" href="#basicblock" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">BasicBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplanes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aa_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_block:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#BasicBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The basic building block for models like ResNet. This class extends pytorch’s Module class.
It represents a standard block of layers including two convolutions, batch normalization,
an optional attention mechanism, and activation functions.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">zero_init_last_bn</span></code></p>
</dd>
<dt>Properties:</dt><dd><ul class="simple">
<li><p>expansion (:obj:int): Specifies the expansion factor for the planes of the conv layers.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplanes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aa_layer:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_block:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#BasicBlock.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the BasicBlock with given parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inplanes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of input channels.</p></li>
<li><p>planes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of output channels.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The stride of the convolutional layer.</p></li>
<li><p>downsample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Function for downsampling the inputs.</p></li>
<li><p>cardinality (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Group size for grouped convolution.</p></li>
<li><p>base_width (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Base width of the convolutions.</p></li>
<li><p>reduce_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Reduction factor for first convolution of each block.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Spacing between kernel points.</p></li>
<li><p>first_dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): First dilation value.</p></li>
<li><p>act_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Function for activation layer.</p></li>
<li><p>norm_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Function for normalization layer.</p></li>
<li><p>attn_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Function for attention layer.</p></li>
<li><p>aa_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Function for anti-aliasing layer.</p></li>
<li><p>drop_block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Method for dropping block.</p></li>
<li><p>drop_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Method for dropping path.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock.expansion">
<span class="sig-name descname"><span class="pre">expansion</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#BasicBlock.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Defines the computation performed at every call.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after passing through the BasicBlock.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.BasicBlock.zero_init_last_bn">
<span class="sig-name descname"><span class="pre">zero_init_last_bn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#BasicBlock.zero_init_last_bn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.BasicBlock.zero_init_last_bn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the batch normalization layer with zeros.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bottleneck">
<h3>Bottleneck<a class="headerlink" href="#bottleneck" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">Bottleneck</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplanes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aa_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_block:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#Bottleneck"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The Bottleneck class is a basic block used to build ResNet networks. It is a part of the PyTorch’s
implementation of ResNet. This block is designed with several layers including a convolutional layer,
normalization layer, activation layer, attention layer, anti-aliasing layer, and a dropout layer.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">zero_init_last_bn</span></code></p>
</dd>
<dt>Properties:</dt><dd><p>expansion, inplanes, planes, stride, downsample, cardinality, base_width, reduce_first, dilation,         first_dilation, act_layer, norm_layer, attn_layer, aa_layer, drop_block, drop_path</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inplanes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">planes:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aa_layer:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_block:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#Bottleneck.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Bottleneck class with various parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inplanes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of input planes.</p></li>
<li><p>planes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of output planes.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The stride size, defaults to 1.</p></li>
<li><p>downsample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): The downsample method, defaults to None.</p></li>
<li><p>cardinality (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The size of the group convolutions, defaults to 1.</p></li>
<li><p>base_width (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The base width, defaults to 64.</p></li>
<li><p>reduce_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first reduction factor, defaults to 1.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The dilation factor, defaults to 1.</p></li>
<li><p>first_dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first dilation factor, defaults to None.</p></li>
<li><p>act_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The activation layer type, defaults to nn.ReLU.</p></li>
<li><p>norm_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The normalization layer type, defaults to nn.BatchNorm2d.</p></li>
<li><p>attn_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The attention layer type, defaults to None.</p></li>
<li><p>aa_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The anti-aliasing layer type, defaults to None.</p></li>
<li><p>drop_block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The dropout block, defaults to None.</p></li>
<li><p>drop_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The drop path, defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck.expansion">
<span class="sig-name descname"><span class="pre">expansion</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">4</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#Bottleneck.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Defines the computation performed at every call.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor</span></code>): The output tensor resulting from the computation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.Bottleneck.zero_init_last_bn">
<span class="sig-name descname"><span class="pre">zero_init_last_bn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#Bottleneck.zero_init_last_bn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.Bottleneck.zero_init_last_bn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the last batch normalization layer with zero.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="downsample-conv">
<h3>downsample_conv<a class="headerlink" href="#downsample-conv" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.downsample_conv">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">downsample_conv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#downsample_conv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.downsample_conv" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a sequential module for downsampling that includes a convolution layer and a normalization layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of input channels.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of output channels.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The stride size, defaults to 1.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The dilation factor, defaults to 1.</p></li>
<li><p>first_dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first dilation factor, defaults to None.</p></li>
<li><p>norm_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The normalization layer type, defaults to nn.BatchNorm2d.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>nn.Sequential: A sequence of layers performing downsampling through convolution.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="downsample-avg">
<h3>downsample_avg<a class="headerlink" href="#downsample-avg" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.downsample_avg">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">downsample_avg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_dilation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Sequential</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#downsample_avg"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.downsample_avg" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a sequential module for downsampling that includes an average pooling layer, a convolution layer,
and a normalization layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>in_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of input channels.</p></li>
<li><p>out_channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of output channels.</p></li>
<li><p>kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the kernel.</p></li>
<li><p>stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The stride size, defaults to 1.</p></li>
<li><p>dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The dilation factor, defaults to 1.</p></li>
<li><p>first_dilation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first dilation factor, defaults to None.</p></li>
<li><p>norm_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>, optional): The normalization layer type, defaults to nn.BatchNorm2d.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>nn.Sequential: A sequence of layers performing downsampling through average pooling.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="drop-blocks">
<h3>drop_blocks<a class="headerlink" href="#drop-blocks" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.drop_blocks">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">drop_blocks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">drop_block_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#drop_blocks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.drop_blocks" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generate a list of None values based on the drop block rate.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>drop_block_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The drop block rate, defaults to 0.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>List[None]: A list of None values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="make-blocks">
<h3>make_blocks<a class="headerlink" href="#make-blocks" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.make_blocks">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">make_blocks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">block_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_repeats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplanes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">down_kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_down</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_block_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#make_blocks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.make_blocks" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a list of blocks for the network, with each block having a given number of repeats. Also, create a
feature info list that contains information about the output of each block.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>block_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Type[nn.Module]</span></code>): The type of block to use.</p></li>
<li><p>channels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The list of output channels for each block.</p></li>
<li><p>block_repeats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): The list of number of repeats for each block.</p></li>
<li><p>inplanes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of input planes.</p></li>
<li><p>reduce_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The first reduction factor, defaults to 1.</p></li>
<li><p>output_stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The total stride of the network, defaults to 32.</p></li>
<li><p>down_kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The size of the downsample kernel, defaults to 1.</p></li>
<li><p>avg_down (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use average pooling for downsampling, defaults to False.</p></li>
<li><p>drop_block_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The drop block rate, defaults to 0.</p></li>
<li><p>drop_path_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The drop path rate, defaults to 0.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Tuple[List[Tuple[str, nn.Module]], List[Dict[str, Union[int, str]]]]:             A tuple that includes a list of blocks for the network and a feature info list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="resnet">
<h3>ResNet<a class="headerlink" href="#resnet" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">ResNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">block:</span> <span class="pre">~torch.nn.modules.module.Module,</span> <span class="pre">layers:</span> <span class="pre">~typing.List[int],</span> <span class="pre">num_classes:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1000,</span> <span class="pre">in_chans:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3,</span> <span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64,</span> <span class="pre">stem_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64,</span> <span class="pre">stem_type:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'',</span> <span class="pre">replace_stem_pool:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">output_stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32,</span> <span class="pre">block_reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">down_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">avg_down:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">act_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;,</span> <span class="pre">norm_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,</span> <span class="pre">aa_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">drop_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">drop_path_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">drop_block_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">global_pool:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'avg',</span> <span class="pre">zero_init_last_bn:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">block_args:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implements ResNet, ResNeXt, SE-ResNeXt, and SENet models. This implementation supports various modifications
based on the v1c, v1d, v1e, and v1s variants included in the MXNet Gluon ResNetV1b model. For more details
about the variants and options, please refer to the ‘Bag of Tricks’ paper: <a class="reference external" href="https://arxiv.org/pdf/1812.01187">https://arxiv.org/pdf/1812.01187</a>.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">zero_init_last_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">get_classifier</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">block:</span> <span class="pre">~torch.nn.modules.module.Module,</span> <span class="pre">layers:</span> <span class="pre">~typing.List[int],</span> <span class="pre">num_classes:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1000,</span> <span class="pre">in_chans:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3,</span> <span class="pre">cardinality:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">base_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64,</span> <span class="pre">stem_width:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64,</span> <span class="pre">stem_type:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'',</span> <span class="pre">replace_stem_pool:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">output_stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32,</span> <span class="pre">block_reduce_first:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">down_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">avg_down:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">act_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;,</span> <span class="pre">norm_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,</span> <span class="pre">aa_layer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">drop_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">drop_path_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">drop_block_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0,</span> <span class="pre">global_pool:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'avg',</span> <span class="pre">zero_init_last_bn:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">block_args:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ResNet model with given block, layers and other configuration options.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>block (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): Class for the residual block.</p></li>
<li><p>layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): Numbers of layers in each block.</p></li>
<li><p>num_classes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of classification classes. Default is 1000.</p></li>
<li><p>in_chans (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of input (color) channels. Default is 3.</p></li>
<li><p>cardinality (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of convolution groups for 3x3 conv in Bottleneck. Default is 1.</p></li>
<li><p>base_width (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Factor determining bottleneck channels. Default is 64.</p></li>
<li><p>stem_width (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Number of channels in stem convolutions. Default is 64.</p></li>
<li><p>stem_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): The type of stem. Default is ‘’.</p></li>
<li><p>replace_stem_pool (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to replace stem pooling. Default is False.</p></li>
<li><p>output_stride (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Output stride of the network. Default is 32.</p></li>
<li><p>block_reduce_first (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Reduction factor for first convolution output width of                 residual blocks. Default is 1.</p></li>
<li><p>down_kernel_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): Kernel size of residual block downsampling path. Default is 1.</p></li>
<li><dl class="simple">
<dt>avg_down (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to use average pooling for projection skip connection between</dt><dd><p>stages/downsample. Default is False.</p>
</dd>
</dl>
</li>
<li><p>act_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): Activation layer. Default is nn.ReLU.</p></li>
<li><p>norm_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>, optional): Normalization layer. Default is nn.BatchNorm2d.</p></li>
<li><p>aa_layer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[nn.Module]</span></code>, optional): Anti-aliasing layer. Default is None.</p></li>
<li><p>drop_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Dropout probability before classifier, for training. Default is 0.0.</p></li>
<li><p>drop_path_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Drop path rate. Default is 0.0.</p></li>
<li><p>drop_block_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): Drop block rate. Default is 0.0.</p></li>
<li><p>global_pool (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): Global pooling type. Default is ‘avg’.</p></li>
<li><p>zero_init_last_bn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to initialize last batch normalization with zero.                 Default is True.</p></li>
<li><p>block_args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[dict]</span></code>, optional): Additional arguments for block. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Full forward pass through the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after passing through the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.forward_features">
<span class="sig-name descname"><span class="pre">forward_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.forward_features"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.forward_features" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward pass through the feature layers of the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after passing through feature layers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.get_classifier">
<span class="sig-name descname"><span class="pre">get_classifier</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.get_classifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.get_classifier" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the classifier module from the model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>classifier (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The classifier module in the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.init_weights">
<span class="sig-name descname"><span class="pre">init_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">zero_init_last_bn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.init_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.init_weights" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the weights in the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>zero_init_last_bn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Whether to initialize last batch normalization with zero.</dt><dd><p>Default is True.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.reset_classifier">
<span class="sig-name descname"><span class="pre">reset_classifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">global_pool</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'avg'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#ResNet.reset_classifier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.reset_classifier" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset the classifier with a new number of classes and pooling type.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>num_classes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): New number of classification classes.</p></li>
<li><p>global_pool (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, optional): New global pooling type. Default is ‘avg’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.ResNet.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.resnet.ResNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="resnet18">
<h3>resnet18<a class="headerlink" href="#resnet18" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.resnet.resnet18">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.resnet.</span></span><span class="sig-name descname"><span class="pre">resnet18</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/resnet.html#resnet18"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.resnet.resnet18" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Creates a ResNet18 model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): ResNet18 model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="network-rnn">
<h2>network.rnn<a class="headerlink" href="#network-rnn" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/rnn</span></code> for more details.</p>
<section id="is-sequence">
<h3>is_sequence<a class="headerlink" href="#is-sequence" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.is_sequence">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">is_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#is_sequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.is_sequence" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Determines if the input data is of type list or tuple.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data: The input data to be checked.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>boolean: True if the input is a list or a tuple, False otherwise.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="sequence-mask">
<h3>sequence_mask<a class="headerlink" href="#sequence-mask" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.sequence_mask">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">sequence_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BoolTensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#sequence_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.sequence_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generates a boolean mask for a batch of sequences with differing lengths.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>lengths (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): A tensor with the lengths of each sequence. Shape could be (n, 1) or (n).</p></li>
<li><p>max_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional): The padding size. If max_len is None, the padding size is the max length of             sequences.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>masks (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>): A boolean mask tensor. The mask has the same device as lengths.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="lstmforwardwrapper">
<h3>LSTMForwardWrapper<a class="headerlink" href="#lstmforwardwrapper" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTMForwardWrapper</span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Class providing methods to use before and after the LSTM <cite>forward</cite> method.
Wraps the LSTM <cite>forward</cite> method.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">_before_forward</span></code>, <code class="docutils literal notranslate"><span class="pre">_after_forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper._after_forward">
<span class="sig-name descname"><span class="pre">_after_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">next_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper._after_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._after_forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Post-processes the next_state after the LSTM <cite>forward</cite> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor]</span></code>): Tuple containing the next state (h, c).</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): Determines the format of the returned next_state.                 If True, returns next_state in list format. Default is False.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>next_state(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[List[Dict],</span> <span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]]</span></code>): The post-processed next_state.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTMForwardWrapper._before_forward">
<span class="sig-name descname"><span class="pre">_before_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTMForwardWrapper._before_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._before_forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Preprocesses the inputs and previous states before the LSTM <cite>forward</cite> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input vector of the LSTM cell. Shape: [seq_len, batch_size, input_size]</p></li>
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[None,</span> <span class="pre">List[Dict]]</span></code>): Previous state tensor. Shape: [num_directions*num_layers,                 batch_size, hidden_size]. If None, prv_state will be initialized to all zeros.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Preprocessed previous state for the LSTM batch.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="lstm">
<h3>LSTM<a class="headerlink" href="#lstm" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implementation of an LSTM cell with Layer Normalization (LN).</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For a primer on LSTM, refer to <a class="reference external" href="https://zhuanlan.zhihu.com/p/32085405">https://zhuanlan.zhihu.com/p/32085405</a>.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTM.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize LSTM cell parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the input vector.</p></li>
<li><p>hidden_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the hidden state vector.</p></li>
<li><p>num_layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of LSTM layers.</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[str]</span></code>): Normalization type, default is None.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Dropout rate, default is 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._init">
<span class="sig-name descname"><span class="pre">_init</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTM._init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._init" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the parameters of the LSTM cell.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#LSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute output and next state given previous state and input.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input vector of cell, size [seq_len, batch_size, input_size].</p></li>
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Previous state,                 size [num_directions*num_layers, batch_size, hidden_size].</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return next_state in list format, default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Output from LSTM.</p></li>
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): Hidden state from LSTM.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.LSTM.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.LSTM.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="pytorchlstm">
<h3>PytorchLSTM<a class="headerlink" href="#pytorchlstm" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">PytorchLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proj_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#PytorchLSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM" title="Permalink to this definition">¶</a></dt>
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">PytorchLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Wrapper class for PyTorch’s nn.LSTM, formats the input and output. For more details on nn.LSTM,
refer to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM">https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM</a></p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.batch_first">
<span class="sig-name descname"><span class="pre">batch_first</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.batch_first" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.bidirectional">
<span class="sig-name descname"><span class="pre">bidirectional</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.bidirectional" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#PytorchLSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Executes nn.LSTM.forward with preprocessed input.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input vector of cell, size [seq_len, batch_size, input_size].</p></li>
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Previous state, size [num_directions*num_layers, batch_size,                 hidden_size].</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return next_state in list format, default is True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Output from LSTM.</p></li>
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">list]</span></code>): Hidden state from LSTM.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.hidden_size">
<span class="sig-name descname"><span class="pre">hidden_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.hidden_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.input_size">
<span class="sig-name descname"><span class="pre">input_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.input_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.mode" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.num_layers">
<span class="sig-name descname"><span class="pre">num_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.num_layers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.PytorchLSTM.proj_size">
<span class="sig-name descname"><span class="pre">proj_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.PytorchLSTM.proj_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="gru">
<h3>GRU<a class="headerlink" href="#gru" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">GRU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#GRU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This class extends the <cite>torch.nn.GRUCell</cite> and <cite>LSTMForwardWrapper</cite> classes, and formats inputs and outputs
accordingly.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
<dt>Properties:</dt><dd><p>hidden_size, num_layers</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For further details, refer to the official PyTorch documentation:
&lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU">https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU</a>&gt;</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#GRU.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the GRU class with input size, hidden size, and number of layers.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the input vector.</p></li>
<li><p>hidden_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The size of the hidden state vector.</p></li>
<li><p>num_layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of GRU layers.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.bias" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_next_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#GRU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Wrap the <cite>nn.GRU.forward</cite> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Input vector of cell, tensor of size [seq_len, batch_size, input_size].</p></li>
<li><p>prev_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): None or tensor of                 size [num_directions*num_layers, batch_size, hidden_size].</p></li>
<li><p>list_next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return next_state in list format (default is True).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Output from GRU.</p></li>
<li><p>next_state (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): Hidden state from GRU.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.hidden_size">
<span class="sig-name descname"><span class="pre">hidden_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.hidden_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.input_size">
<span class="sig-name descname"><span class="pre">input_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.input_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.weight_hh">
<span class="sig-name descname"><span class="pre">weight_hh</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tensor</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.weight_hh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.GRU.weight_ih">
<span class="sig-name descname"><span class="pre">weight_ih</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tensor</span></em><a class="headerlink" href="#ding.torch_utils.network.rnn.GRU.weight_ih" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="get-lstm">
<h3>get_lstm<a class="headerlink" href="#get-lstm" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.rnn.get_lstm">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.rnn.</span></span><span class="sig-name descname"><span class="pre">get_lstm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lstm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM" title="ding.torch_utils.network.rnn.LSTM"><span class="pre">LSTM</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM" title="ding.torch_utils.network.rnn.PytorchLSTM"><span class="pre">PytorchLSTM</span></a></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/rnn.html#get_lstm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.rnn.get_lstm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Build and return the corresponding LSTM cell based on the provided parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>lstm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Version of RNN cell. Supported options are [‘normal’, ‘pytorch’, ‘hpc’, ‘gru’].</p></li>
<li><p>input_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the input vector.</p></li>
<li><p>hidden_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Size of the hidden state vector.</p></li>
<li><p>num_layers (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of LSTM layers (default is 1).</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Type of normalization (default is ‘LN’).</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Dropout rate (default is 0.0).</p></li>
<li><p>seq_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): Sequence length (default is None).</p></li>
<li><p>batch_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): Batch size (default is None).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>lstm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[LSTM,</span> <span class="pre">PytorchLSTM]</span></code>): The corresponding LSTM cell.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="network-scatter-connection">
<h2>network.scatter_connection<a class="headerlink" href="#network-scatter-connection" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/scatter_connection</span></code> for more details.</p>
<section id="shape-fn-scatter-connection">
<h3>shape_fn_scatter_connection<a class="headerlink" href="#shape-fn-scatter-connection" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.shape_fn_scatter_connection">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.scatter_connection.</span></span><span class="sig-name descname"><span class="pre">shape_fn_scatter_connection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/scatter_connection.html#shape_fn_scatter_connection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.shape_fn_scatter_connection" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the shape of scatter_connection for HPC.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple</span></code>): The arguments passed to the scatter_connection function.</p></li>
<li><p>kwargs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict</span></code>): The keyword arguments passed to the scatter_connection function.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[int]</span></code>): A list representing the shape of scatter_connection,             in the form of [B, M, N, H, W, scatter_type].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="scatterconnection">
<h3>ScatterConnection<a class="headerlink" href="#scatterconnection" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.scatter_connection.</span></span><span class="sig-name descname"><span class="pre">ScatterConnection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scatter_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Scatter feature to its corresponding location. In AlphaStar, each entity is embedded into a tensor,
and these tensors are scattered into a feature map with map size.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">xy_forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scatter_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ScatterConnection object.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>scatter_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The scatter type, which decides the behavior when two entities have the                 same location. It can be either ‘add’ or ‘cover’. If ‘add’, the first one will be added to the                 second one. If ‘cover’, the first one will be covered by the second one.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spatial_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">location</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Scatter input tensor ‘x’ into a spatial feature map.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor of shape <cite>(B, M, N)</cite>, where <cite>B</cite> is the batch size, <cite>M</cite>                 is the number of entities, and <cite>N</cite> is the dimension of entity attributes.</p></li>
<li><p>spatial_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[int,</span> <span class="pre">int]</span></code>): The size <cite>(H, W)</cite> of the spatial feature map into which ‘x’                 will be scattered, where <cite>H</cite> is the height and <cite>W</cite> is the width.</p></li>
<li><p>location (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The tensor of locations of shape <cite>(B, M, 2)</cite>.                 Each location should be (y, x).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The scattered feature map of shape <cite>(B, N, H, W)</cite>.</p></li>
</ul>
</dd>
<dt>Note:</dt><dd><p>When there are some overlapping in locations, ‘cover’ mode will result in the loss of information.
‘add’ mode is used as a temporary substitute.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.scatter_connection.ScatterConnection.xy_forward">
<span class="sig-name descname"><span class="pre">xy_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spatial_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coord_x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coord_y</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/scatter_connection.html#ScatterConnection.xy_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.xy_forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Scatter input tensor ‘x’ into a spatial feature map using separate x and y coordinates.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor of shape <cite>(B, M, N)</cite>, where <cite>B</cite> is the batch size, <cite>M</cite>                 is the number of entities, and <cite>N</cite> is the dimension of entity attributes.</p></li>
<li><p>spatial_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[int,</span> <span class="pre">int]</span></code>): The size <cite>(H, W)</cite> of the spatial feature map into which ‘x’                 will be scattered, where <cite>H</cite> is the height and <cite>W</cite> is the width.</p></li>
<li><p>coord_x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The x-coordinates tensor of shape <cite>(B, M)</cite>.</p></li>
<li><p>coord_y (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The y-coordinates tensor of shape <cite>(B, M)</cite>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The scattered feature map of shape <cite>(B, N, H, W)</cite>.</p></li>
</ul>
</dd>
<dt>Note:</dt><dd><p>When there are some overlapping in locations, ‘cover’ mode will result in the loss of information.
‘add’ mode is used as a temporary substitute.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="network-soft-argmax">
<h2>network.soft_argmax<a class="headerlink" href="#network-soft-argmax" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/soft_argmax</span></code> for more details.</p>
<section id="softargmax">
<h3>SoftArgmax<a class="headerlink" href="#softargmax" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.soft_argmax.</span></span><span class="sig-name descname"><span class="pre">SoftArgmax</span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/soft_argmax.html#SoftArgmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A neural network module that computes the SoftArgmax operation (essentially a 2-dimensional spatial softmax),
which is often used for location regression tasks. It converts a feature map (such as a heatmap) into precise
coordinate locations.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on SoftArgmax, you can refer to &lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">https://en.wikipedia.org/wiki/Softmax_function</a>&gt;
and the paper &lt;<a class="reference external" href="https://arxiv.org/pdf/1504.00702.pdf">https://arxiv.org/pdf/1504.00702.pdf</a>&gt;.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/soft_argmax.html#SoftArgmax.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the SoftArgmax module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/soft_argmax.html#SoftArgmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Perform the forward pass of the SoftArgmax operation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor, typically a heatmap representing predicted locations.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>location (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The predicted coordinates as a result of the SoftArgmax operation.</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>x: <span class="math notranslate nohighlight">\((B, C, H, W)\)</span>, where <cite>B</cite> is the batch size, <cite>C</cite> is the number of channels,                 and <cite>H</cite> and <cite>W</cite> represent height and width respectively.</p></li>
<li><p>location: <span class="math notranslate nohighlight">\((B, 2)\)</span>, where <cite>B</cite> is the batch size and 2 represents the coordinates (height, width).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.soft_argmax.SoftArgmax.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="network-transformer">
<h2>network.transformer<a class="headerlink" href="#network-transformer" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/network/transformer</span></code> for more details.</p>
<section id="attention">
<h3>Attention<a class="headerlink" href="#attention" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">Attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For each entry embedding, compute individual attention across all entries, add them up to get output attention.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">split</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Attention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Attention module with the provided dimensions and dropout layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of each head in the multi-head attention mechanism.</p></li>
<li><p>output_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the output.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of heads in the multi-head attention mechanism.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The dropout layer used in the attention mechanism.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Attention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the attention from the input tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor for the forward computation.</p></li>
<li><dl class="simple">
<dt>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>, optional): Optional mask to exclude invalid entries.</dt><dd><p>Defaults to None.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>attention (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The computed attention tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">T</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Attention.split"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.split" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Split the input to get multi-head queries, keys, and values.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The tensor to be split, which could be a query, key, or value.</p></li>
<li><p>T (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, optional): If True, transpose the output tensors. Defaults to False.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.Tensor]</span></code>): A list of output tensors for each head.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Attention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Attention.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="transformerlayer">
<h3>TransformerLayer<a class="headerlink" href="#transformerlayer" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#TransformerLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>In transformer layer, first computes entries’s attention and applies a feedforward layer.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#TransformerLayer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the TransformerLayer with the provided dimensions, dropout layer, and activation function.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of each head in the multi-head attention mechanism.</p></li>
<li><p>hidden_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the hidden layer in the MLP (Multi-Layer Perceptron).</p></li>
<li><p>output_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the output.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of heads in the multi-head attention mechanism.</p></li>
<li><p>mlp_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of layers in the MLP.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The dropout layer used in the attention mechanism.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The activation function used in the MLP.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#TransformerLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Compute the forward pass through the Transformer layer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor]</span></code>): A tuple containing the input tensor <cite>x</cite> and</dt><dd><p>the mask tensor.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor]</span></code>): A tuple containing the predicted value tensor and</dt><dd><p>the mask tensor.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.TransformerLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.TransformerLayer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="transformer">
<h3>Transformer<a class="headerlink" href="#transformer" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Transformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implementation of the Transformer model.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more details, refer to “Attention is All You Need”: <a class="reference external" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</p>
</div>
<dl class="simple">
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_ratio</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">ReLU()</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Transformer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the Transformer with the provided dimensions, dropout layer, activation function,
and layer numbers.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>input_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the input.</p></li>
<li><p>head_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of each head in the multi-head attention mechanism.</p></li>
<li><p>hidden_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the hidden layer in the MLP (Multi-Layer Perceptron).</p></li>
<li><p>output_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the output.</p></li>
<li><p>head_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of heads in the multi-head attention mechanism.</p></li>
<li><p>mlp_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of layers in the MLP.</p></li>
<li><p>layer_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of Transformer layers.</p></li>
<li><p>dropout_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The dropout ratio for the dropout layer.</p></li>
<li><p>activation (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The activation function used in the MLP.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#Transformer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Perform the forward pass through the Transformer.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The input tensor, with shape <cite>(B, N, C)</cite>, where <cite>B</cite> is batch size,                 <cite>N</cite> is the number of entries, and <cite>C</cite> is the feature dimension.</p></li>
<li><p>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>, optional): The mask tensor (bool), used to mask out invalid                 entries in attention. It has shape <cite>(B, N)</cite>, where <cite>B</cite> is batch size and <cite>N</cite> is number of                 entries. Defaults to None.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor from the Transformer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.Transformer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.Transformer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="scaleddotproductattention">
<h3>ScaledDotProductAttention<a class="headerlink" href="#scaleddotproductattention" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.network.transformer.</span></span><span class="sig-name descname"><span class="pre">ScaledDotProductAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#ScaledDotProductAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Implementation of Scaled Dot Product Attention, a key component of Transformer models.
This class performs the dot product of the query, key and value tensors, scales it with the square root of the
dimension of the key vector (d_k) and applies dropout for regularization.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#ScaledDotProductAttention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the ScaledDotProductAttention module with the dimension of the key vector and the dropout rate.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>d_k (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension of the key vector. This will be used to scale the dot product of the                 query and key.</p></li>
<li><p>dropout (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional): The dropout rate to be applied after the softmax operation.                 Defaults to 0.0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/network/transformer.html#ScaledDotProductAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Perform the Scaled Dot Product Attention operation on the query, key and value tensors.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The query tensor.</p></li>
<li><p>k (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The key tensor.</p></li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The value tensor.</p></li>
<li><dl class="simple">
<dt>mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): An optional mask tensor to be applied on the attention scores.</dt><dd><p>Defaults to None.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The output tensor after the attention operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.network.transformer.ScaledDotProductAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="backend-helper">
<h2>backend_helper<a class="headerlink" href="#backend-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/backend_helper</span></code> for more details.</p>
<section id="enable-tf32">
<h3>enable_tf32<a class="headerlink" href="#enable-tf32" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.backend_helper.enable_tf32">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.backend_helper.</span></span><span class="sig-name descname"><span class="pre">enable_tf32</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/backend_helper.html#enable_tf32"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.backend_helper.enable_tf32" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Enable tf32 on matmul and cudnn for faster computation. This only works on Ampere GPU devices.         For detailed information, please refer to:         <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices">https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices</a>.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="checkpoint-helper">
<h2>checkpoint_helper<a class="headerlink" href="#checkpoint-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/checkpoint_helper</span></code> for more details.</p>
<section id="build-checkpoint-helper">
<h3>build_checkpoint_helper<a class="headerlink" href="#build-checkpoint-helper" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.build_checkpoint_helper">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.checkpoint_helper.</span></span><span class="sig-name descname"><span class="pre">build_checkpoint_helper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#build_checkpoint_helper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.build_checkpoint_helper" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Use config to build checkpoint helper.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): ckpt_helper config</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper" title="ding.torch_utils.checkpoint_helper.CheckpointHelper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CheckpointHelper</span></code></a>): checkpoint_helper created by this function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="checkpointhelper">
<h3>CheckpointHelper<a class="headerlink" href="#checkpointhelper" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.checkpoint_helper.</span></span><span class="sig-name descname"><span class="pre">CheckpointHelper</span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Help to save or load checkpoint by give args.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">save</span></code>, <code class="docutils literal notranslate"><span class="pre">load</span></code>, <code class="docutils literal notranslate"><span class="pre">_remove_prefix</span></code>, <code class="docutils literal notranslate"><span class="pre">_add_prefix</span></code>, <code class="docutils literal notranslate"><span class="pre">_load_matched_model_state_dict</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper._add_prefix">
<span class="sig-name descname"><span class="pre">_add_prefix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'module.'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper._add_prefix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._add_prefix" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Add prefix in state_dict</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): model’s state_dict</p></li>
<li><p>prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): this prefix will be added in keys</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): new state_dict after adding prefix</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper._load_matched_model_state_dict">
<span class="sig-name descname"><span class="pre">_load_matched_model_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper._load_matched_model_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._load_matched_model_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load matched model state_dict, and show mismatch keys between model’s state_dict and checkpoint’s state_dict</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): model</p></li>
<li><p>ckpt_state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): checkpoint’s state_dict</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper._remove_prefix">
<span class="sig-name descname"><span class="pre">_remove_prefix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'module.'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper._remove_prefix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._remove_prefix" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Remove prefix in state_dict</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): model’s state_dict</p></li>
<li><p>prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): this prefix will be removed in keys</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>new_state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): new state_dict after removing prefix</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">load_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_iter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_frame</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_schduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="utils.html#ding.utils.scheduler_helper.Scheduler" title="ding.utils.scheduler_helper.Scheduler"><span class="pre">Scheduler</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collector_info</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.load" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load checkpoint by given path</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>load_path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): checkpoint’s path</p></li>
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): model definition</p></li>
<li><p>optimizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>): optimizer obj</p></li>
<li><p>last_iter (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): iter num, default None</p></li>
<li><p>last_epoch (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): epoch num, default None</p></li>
<li><p>last_frame (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): frame num, default None</p></li>
<li><p>lr_schduler (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Schduler</span></code>): lr_schduler obj</p></li>
<li><p>dataset (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>): dataset, should be replaydataset</p></li>
<li><p>collector_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): attr of checkpoint, save collector info</p></li>
<li><p>prefix_op (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): should be [‘remove’, ‘add’], process on state_dict</p></li>
<li><p>prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): prefix to be processed on state_dict</p></li>
<li><p>strict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): args of model.load_state_dict</p></li>
<li><p>logger_prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): prefix of logger</p></li>
<li><p>state_dict_mask (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): A list containing state_dict keys,                 which shouldn’t be loaded into model(after prefix op)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The checkpoint loaded from load_path is a dict, whose format is like ‘{‘state_dict’: OrderedDict(), …}’</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CheckpointHelper.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_iter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_frame</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><span class="pre">CountVar</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">collector_info</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CheckpointHelper.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.save" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Save checkpoint by given args</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>path (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the path of saving checkpoint</p></li>
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): model to be saved</p></li>
<li><p>optimizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>): optimizer obj</p></li>
<li><p>last_iter (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): iter num, default None</p></li>
<li><p>last_epoch (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): epoch num, default None</p></li>
<li><p>last_frame (<a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar" title="ding.torch_utils.checkpoint_helper.CountVar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CountVar</span></code></a>): frame num, default None</p></li>
<li><p>dataset (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>): dataset, should be replaydataset</p></li>
<li><p>collector_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): attr of checkpoint, save collector info</p></li>
<li><p>prefix_op (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): should be [‘remove’, ‘add’], process on state_dict</p></li>
<li><p>prefix (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): prefix to be processed on state_dict</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="countvar">
<h3>CountVar<a class="headerlink" href="#countvar" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CountVar">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.checkpoint_helper.</span></span><span class="sig-name descname"><span class="pre">CountVar</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CountVar"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CountVar" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Number counter</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">update</span></code>, <code class="docutils literal notranslate"><span class="pre">add</span></code></p>
</dd>
<dt>Properties:</dt><dd><ul class="simple">
<li><p>val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the value of the counter</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CountVar.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CountVar.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CountVar.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the var counter</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>init_val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the init value of the counter</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CountVar.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">add_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CountVar.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CountVar.add" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Add the number to counter</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>add_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the number added to the counter</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CountVar.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#CountVar.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CountVar.update" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Update the var counter</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>val (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the update value of the counter</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.CountVar.val">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">val</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.CountVar.val" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the var counter</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="auto-checkpoint">
<h3>auto_checkpoint<a class="headerlink" href="#auto-checkpoint" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.checkpoint_helper.auto_checkpoint">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.checkpoint_helper.</span></span><span class="sig-name descname"><span class="pre">auto_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/checkpoint_helper.html#auto_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.checkpoint_helper.auto_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a wrapper to wrap function, and the wrapper will call the save_checkpoint method
whenever an exception happens.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>func(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): the function to be wrapped</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>wrapper (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): the wrapped function</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="data-helper">
<h2>data_helper<a class="headerlink" href="#data-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/data_helper</span></code> for more details.</p>
<section id="to-device">
<h3>to_device<a class="headerlink" href="#to-device" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_device">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_device" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Transfer data to certain device.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The item to be transferred.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The device wanted.</p></li>
<li><p>ignore_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): The keys to be ignored in transfer, default set to empty.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The transferred item.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">setup_data_dict</span><span class="p">[</span><span class="s1">&#39;module&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda_d</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">setup_data_dict</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">ignore_keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;module&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">cuda_d</span><span class="p">[</span><span class="s1">&#39;module&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">setup_data_dict</span><span class="p">[</span><span class="s1">&#39;module&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda_d</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">setup_data_dict</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">cuda_d</span><span class="p">[</span><span class="s1">&#39;module&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-dtype">
<h3>to_dtype<a class="headerlink" href="#to-dtype" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_dtype">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_dtype</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">type</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Change data to certain dtype.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The item for changing the dtype.</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code>): The type wanted.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">object</span></code>): The item with changed dtype.</p></li>
</ul>
</dd>
<dt>Examples (tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tfloat</span> <span class="o">=</span> <span class="n">to_dtype</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tfloat</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
</pre></div>
</div>
</dd>
<dt>Examples (list):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tlist</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tlfloat</span> <span class="o">=</span> <span class="n">to_dtype</span><span class="p">(</span><span class="n">tlist</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tlfloat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdictf</span> <span class="o">=</span> <span class="n">to_dtype</span><span class="p">(</span><span class="n">tdict</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tdictf</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-tensor">
<h3>to_tensor<a class="headerlink" href="#to-tensor" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_tensor">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform_scalar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> object to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> objects to be converted. It can be exactly a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>             object or a container (list, tuple or dict) that contains several <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> objects.</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.dtype</span></code>): The type of wanted tensor. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, its dtype will be unchanged.</p></li>
<li><p>ignore_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): If the <code class="docutils literal notranslate"><span class="pre">item</span></code> is a dict, values whose keys are in <code class="docutils literal notranslate"><span class="pre">ignore_keys</span></code> will not             be converted.</p></li>
<li><p>transform_scalar (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, a scalar will be also converted to a tensor object.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The converted tensors.</p></li>
</ul>
</dd>
<dt>Examples (scalar):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">i</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;i&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dt</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">dt</span><span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">i</span>
</pre></div>
</div>
</dd>
<dt>Examples (named tuple):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data_type</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;data_type&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">data_type</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">data_type</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">outputs</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">outputs</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-ndarray">
<h3>to_ndarray<a class="headerlink" href="#to-ndarray" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_ndarray">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_ndarray</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_ndarray"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_ndarray" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects to be converted. It can be exactly a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>             object or a container (list, tuple or dict) that contains several <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects.</p></li>
<li><p>dtype (<code class="xref py py-obj docutils literal notranslate"><span class="pre">np.dtype</span></code>): The type of wanted array. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, its dtype will be unchanged.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">object</span></code>): The changed arrays.</p></li>
</ul>
</dd>
<dt>Examples (ndarray):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tarray1</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tarray1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tarray1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Examples (list):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tarray1</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tarray1</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tarray1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tarray1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="to-list">
<h3>to_list<a class="headerlink" href="#to-list" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_list">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_list" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> objects to <code class="docutils literal notranslate"><span class="pre">list</span></code> objects, and keep their dtypes unchanged.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The item to be converted.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The list after conversion.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>                 <span class="s1">&#39;tensor&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>                 <span class="s1">&#39;list&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>                 <span class="s1">&#39;tuple&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>                 <span class="s1">&#39;bool&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>                 <span class="s1">&#39;int&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>                 <span class="s1">&#39;float&#39;</span><span class="p">:</span> <span class="mf">10.</span><span class="p">,</span>                 <span class="s1">&#39;array&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>                 <span class="s1">&#39;str&#39;</span><span class="p">:</span> <span class="s2">&quot;asdf&quot;</span><span class="p">,</span>                 <span class="s1">&#39;none&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>             <span class="p">}</span>         <span class="o">&gt;&gt;&gt;</span> <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">to_list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Now supports item type: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>,         <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>.</p>
</div>
</dd></dl>

</section>
<section id="tensor-to-list">
<h3>tensor_to_list<a class="headerlink" href="#tensor-to-list" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.tensor_to_list">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">tensor_to_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#tensor_to_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.tensor_to_list" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Convert <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> objects to <code class="docutils literal notranslate"><span class="pre">list</span></code>, and keep their dtypes unchanged.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The item to be converted.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>item (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The lists after conversion.</p></li>
</ul>
</dd>
<dt>Examples (2d-tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tlist1</span> <span class="o">=</span> <span class="n">tensor_to_list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tlist1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tlist1</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">5</span>
</pre></div>
</div>
</dd>
<dt>Examples (1d-tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tlist1</span> <span class="o">=</span> <span class="n">tensor_to_list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tlist1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
</pre></div>
</div>
</dd>
<dt>Examples (list)</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tlist1</span> <span class="o">=</span> <span class="n">tensor_to_list</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tlist1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tlist1</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">5</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">td</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tdlist1</span> <span class="o">=</span> <span class="n">tensor_to_list</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tdlist1</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tdlist1</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">5</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Now supports item type: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>.</p>
</div>
</dd></dl>

</section>
<section id="to-item">
<h3>to_item<a class="headerlink" href="#to-item" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.to_item">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">to_item</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#to_item"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.to_item" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Convert data to python native scalar (i.e. data item), and keep their dtypes unchanged.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The data that needs to be converted.</p></li>
<li><p>ignore_error (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to ignore the error when the data type is not supported. That is to             say, only the data can be transformed into a python native scalar will be returned.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): Converted data.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><p>&gt;&gt;&gt;&gt; data = {                 ‘tensor’: torch.randn(1),                 ‘list’: [True, False, torch.randn(1)],                 ‘tuple’: (4, 5, 6),                 ‘bool’: True,                 ‘int’: 10,                 ‘float’: 10.,                 ‘array’: np.random.randn(1),                 ‘str’: “asdf”,                 ‘none’: None,              }
&gt;&gt;&gt;&gt; new_data = to_item(data)
&gt;&gt;&gt;&gt; assert np.isscalar(new_data[‘tensor’])
&gt;&gt;&gt;&gt; assert np.isscalar(new_data[‘array’])
&gt;&gt;&gt;&gt; assert np.isscalar(new_data[‘list’][-1])</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Now supports item type: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">ttorch.Tensor</span></code>,         <code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>.</p>
</div>
</dd></dl>

</section>
<section id="same-shape">
<h3>same_shape<a class="headerlink" href="#same-shape" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.same_shape">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">same_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#same_shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.same_shape" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Judge whether all data elements in a list have the same shapes.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): The list of data.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>same (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether the list of data all have the same shape.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tlist</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">same_shape</span><span class="p">(</span><span class="n">tlist</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tlist</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="n">same_shape</span><span class="p">(</span><span class="n">tlist</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="logdict">
<h3>LogDict<a class="headerlink" href="#logdict" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.LogDict">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">LogDict</span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#LogDict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.LogDict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Derived from <code class="docutils literal notranslate"><span class="pre">dict</span></code>. Would convert <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> to <code class="docutils literal notranslate"><span class="pre">list</span></code> for convenient logging.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">_transform</span></code>, <code class="docutils literal notranslate"><span class="pre">__setitem__</span></code>, <code class="docutils literal notranslate"><span class="pre">update</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.LogDict._transform">
<span class="sig-name descname"><span class="pre">_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#LogDict._transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.LogDict._transform" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Convert tensor objects to lists for better logging.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The input data to be converted.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.LogDict.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#LogDict.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.LogDict.update" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Override the <code class="docutils literal notranslate"><span class="pre">update</span></code> function of built-in dict.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): The dict for updating current object.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="build-log-buffer">
<h3>build_log_buffer<a class="headerlink" href="#build-log-buffer" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.build_log_buffer">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">build_log_buffer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#ding.torch_utils.data_helper.LogDict" title="ding.torch_utils.data_helper.LogDict"><span class="pre">LogDict</span></a></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#build_log_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.build_log_buffer" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Build log buffer, a subclass of dict, which can convert the input data into log format.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>log_buffer (<a class="reference internal" href="#ding.torch_utils.data_helper.LogDict" title="ding.torch_utils.data_helper.LogDict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogDict</span></code></a>): Log buffer dict.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">log_buffer</span> <span class="o">=</span> <span class="n">build_log_buffer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_buffer</span><span class="p">[</span><span class="s1">&#39;not_tensor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">log_buffer</span><span class="p">[</span><span class="s1">&#39;not_tensor&#39;</span><span class="p">],</span> <span class="nb">list</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">log_buffer</span><span class="p">[</span><span class="s1">&#39;not_tensor&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;not_tensor&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">log_buffer</span><span class="p">[</span><span class="s1">&#39;not_tensor&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">4</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="cudafetcher">
<h3>CudaFetcher<a class="headerlink" href="#cudafetcher" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.CudaFetcher">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">CudaFetcher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_source</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sleep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#CudaFetcher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.CudaFetcher" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Fetch data from source, and transfer it to a specified device.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">__next__</span></code>, <code class="docutils literal notranslate"><span class="pre">run</span></code>, <code class="docutils literal notranslate"><span class="pre">close</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.CudaFetcher.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_source</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sleep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#CudaFetcher.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.CudaFetcher.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the CudaFetcher object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_source (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable</span></code>): The iterable data source.</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The device to put data to, such as “cuda:0”.</p></li>
<li><p>queue_size (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The internal size of queue, such as 4.</p></li>
<li><p>sleep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Sleeping time when the internal queue is full.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.CudaFetcher._producer">
<span class="sig-name descname"><span class="pre">_producer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#CudaFetcher._producer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.CudaFetcher._producer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Keep fetching data from source, change the device, and put into <code class="docutils literal notranslate"><span class="pre">queue</span></code> for request.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.CudaFetcher.close">
<span class="sig-name descname"><span class="pre">close</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#CudaFetcher.close"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.CudaFetcher.close" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Stop <code class="docutils literal notranslate"><span class="pre">producer</span></code> thread by setting <code class="docutils literal notranslate"><span class="pre">end_flag</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> .</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.CudaFetcher.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#CudaFetcher.run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.CudaFetcher.run" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Start <code class="docutils literal notranslate"><span class="pre">producer</span></code> thread: Keep fetching data from source, change the device, and put into             <code class="docutils literal notranslate"><span class="pre">queue</span></code> for request.</p>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">timer</span> <span class="o">=</span> <span class="n">EasyTimer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">CudaFetcher</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">sleep</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="get-tensor-data">
<h3>get_tensor_data<a class="headerlink" href="#get-tensor-data" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.get_tensor_data">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">get_tensor_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#get_tensor_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.get_tensor_data" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get pure tensor data from the given data (without disturbing grad computation graph).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The original data. It can be exactly a tensor or a container (Sequence or dict).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The output data.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">{</span>                 <span class="s1">&#39;tensor&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>                 <span class="s1">&#39;list&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)],</span>                 <span class="s1">&#39;none&#39;</span><span class="p">:</span> <span class="kc">None</span>             <span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">get_tensor_data</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="n">tensor_a</span><span class="p">[</span><span class="s1">&#39;tensor&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_a</span><span class="p">[</span><span class="s1">&#39;list&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">assert</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="unsqueeze">
<h3>unsqueeze<a class="headerlink" href="#unsqueeze" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.unsqueeze">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">unsqueeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#unsqueeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Unsqueeze the tensor data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The original data. It can be exactly a tensor or a container (Sequence or dict).</p></li>
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension to be unsqueezed.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The output data.</p></li>
</ul>
</dd>
<dt>Examples (tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">unsqueeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
<dt>Examples (list):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">unsqueeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;t&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">unsqueeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="squeeze">
<h3>squeeze<a class="headerlink" href="#squeeze" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.squeeze">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">squeeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#squeeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Squeeze the tensor data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The original data. It can be exactly a tensor or a container (Sequence or dict).</p></li>
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension to be Squeezed.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The output data.</p></li>
</ul>
</dd>
<dt>Examples (tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
<dt>Examples (list):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;t&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-null-data">
<h3>get_null_data<a class="headerlink" href="#get-null-data" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.get_null_data">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">get_null_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#get_null_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.get_null_data" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get null data given an input template.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>template (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The template data.</p></li>
<li><p>num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The number of null data items to generate.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Any]</span></code>): The generated null data.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">temp</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;action&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;done&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;reward&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">null_data</span> <span class="o">=</span> <span class="n">get_null_data</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">null_data</span><span class="p">)</span> <span class="o">==</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">null_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;null&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">null_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;done&#39;</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="zeros-like">
<h3>zeros_like<a class="headerlink" href="#zeros-like" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.data_helper.zeros_like">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.data_helper.</span></span><span class="sig-name descname"><span class="pre">zeros_like</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/data_helper.html#zeros_like"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.data_helper.zeros_like" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Generate zero-tensors like the input data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>h (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The original data. It can be exactly a tensor or a container (Sequence or dict).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The output zero-tensors.</p></li>
</ul>
</dd>
<dt>Examples (tensor):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tt</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">1e-8</span>
</pre></div>
</div>
</dd>
<dt>Examples (list):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tt</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">&lt;</span> <span class="mf">1e-8</span>
</pre></div>
</div>
</dd>
<dt>Examples (dict):</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;t&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tt</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Shape</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tt</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">]))</span> <span class="o">&lt;</span> <span class="mf">1e-8</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="dataparallel">
<h2>dataparallel<a class="headerlink" href="#dataparallel" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/dataparallel</span></code> for more details.</p>
<section id="id1">
<h3>DataParallel<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.dataparallel.</span></span><span class="sig-name descname"><span class="pre">DataParallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/dataparallel.html#DataParallel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A wrapper class for nn.DataParallel.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">parameters</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/dataparallel.html#DataParallel.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the DataParallel object.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>module (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The module to be parallelized.</p></li>
<li><p>device_ids (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): The list of GPU ids.</p></li>
<li><p>output_device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The output GPU id.</p></li>
<li><p>dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The dimension to be parallelized.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/dataparallel.html#DataParallel.parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel.parameters" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the parameters of the module.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>recurse (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return the parameters of the submodules.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>params (<code class="xref py py-obj docutils literal notranslate"><span class="pre">generator</span></code>): The generator of the parameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.dataparallel.DataParallel.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.dataparallel.DataParallel.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="distribution">
<h2>distribution<a class="headerlink" href="#distribution" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/distribution</span></code> for more details.</p>
<section id="pd">
<h3>Pd<a class="headerlink" href="#pd" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.distribution.</span></span><span class="sig-name descname"><span class="pre">Pd</span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Abstract class for parameterizable probability distributions and sampling functions.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">neglogp</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">noise_mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code></p>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In dereived classes, <cite>logits</cite> should be an attribute member stored in class.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the softmax entropy of logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘mean’], default set to ‘mean’</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the calculated entropy</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return logits argmax result. This method is designed for deterministic.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd.neglogp">
<span class="sig-name descname"><span class="pre">neglogp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd.neglogp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd.neglogp" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate cross_entropy between input x and logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>cross_entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the returned cross_entropy loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd.noise_mode">
<span class="sig-name descname"><span class="pre">noise_mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd.noise_mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd.noise_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Add noise to logits. This method is designed for randomness</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.Pd.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#Pd.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.Pd.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from logits’s distribution by using softmax. This method is designed for multinomial.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="categoricalpd">
<h3>CategoricalPd<a class="headerlink" href="#categoricalpd" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.distribution.</span></span><span class="sig-name descname"><span class="pre">CategoricalPd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Catagorical probility distribution sampler</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">neglogp</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">noise_mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Init the Pd with logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (:obj:torch.Tensor): logits to sample from</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the softmax entropy of logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘mean’], default set to mean</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the calculated entropy</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">viz</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>return logits argmax result</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>viz (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return numpy from of logits, noise and noise_logits;</dt><dd><p>Short for <code class="docutils literal notranslate"><span class="pre">visualize</span></code> . (Because tensor type cannot visualize in tb or text log)</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>result (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the logits argmax result</p></li>
<li><p>viz_feature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">np.ndarray]</span></code>): ndarray type data for visualization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.neglogp">
<span class="sig-name descname"><span class="pre">neglogp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.neglogp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.neglogp" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate cross_entropy between input x and logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input tensor</p></li>
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘mean’], default set to mean</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>cross_entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the returned cross_entropy loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.noise_mode">
<span class="sig-name descname"><span class="pre">noise_mode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">viz</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.noise_mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.noise_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>add noise to logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>viz (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return numpy from of logits, noise and noise_logits;                 Short for <code class="docutils literal notranslate"><span class="pre">visualize</span></code> . (Because tensor type cannot visualize in tb or text log)</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>result (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): noised logits</p></li>
<li><p>viz_feature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">np.ndarray]</span></code>): ndarray type data for visualization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">viz</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from logits’s distribution by using softmax</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>viz (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to return numpy from of logits, noise and noise_logits;                 Short for <code class="docutils literal notranslate"><span class="pre">visualize</span></code> . (Because tensor type cannot visualize in tb or text log)</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>result (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the logits sampled result</p></li>
<li><p>viz_feature (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">np.ndarray]</span></code>): ndarray type data for visualization.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPd.update_logits">
<span class="sig-name descname"><span class="pre">update_logits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPd.update_logits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPd.update_logits" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Updata logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): logits to update</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="categoricalpdpytorch">
<h3>CategoricalPdPytorch<a class="headerlink" href="#categoricalpdpytorch" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.distribution.</span></span><span class="sig-name descname"><span class="pre">CategoricalPdPytorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Wrapped <code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code></p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">update_logits</span></code>, <code class="docutils literal notranslate"><span class="pre">update_probs</span></code>, <code class="docutils literal notranslate"><span class="pre">sample</span></code>, <code class="docutils literal notranslate"><span class="pre">neglogp</span></code>, <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">entropy</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the CategoricalPdPytorch object.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>probs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The tensor of probabilities.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.entropy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the softmax entropy of logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘mean’], default set to mean</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the calculated entropy</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.mode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return logits argmax result</p>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>result(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the logits argmax result</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.neglogp">
<span class="sig-name descname"><span class="pre">neglogp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.neglogp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.neglogp" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate cross_entropy between input x and logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the input action tensor</p></li>
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘mean’], default set to mean</p></li>
</ul>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>cross_entropy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the returned cross_entropy loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Sample from logits’s distribution by using softmax</p>
</dd>
<dt>Return:</dt><dd><ul class="simple">
<li><p>result (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the logits sampled result</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.update_logits">
<span class="sig-name descname"><span class="pre">update_logits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.update_logits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.update_logits" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Updata logits</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): logits to update</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.distribution.CategoricalPdPytorch.update_probs">
<span class="sig-name descname"><span class="pre">update_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/distribution.html#CategoricalPdPytorch.update_probs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.distribution.CategoricalPdPytorch.update_probs" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Updata probs</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>probs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): probs to update</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="lr-scheduler">
<h2>lr_scheduler<a class="headerlink" href="#lr-scheduler" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/lr_scheduler</span></code> for more details.</p>
<section id="get-lr-ratio">
<h3>get_lr_ratio<a class="headerlink" href="#get-lr-ratio" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.lr_scheduler.get_lr_ratio">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.lr_scheduler.</span></span><span class="sig-name descname"><span class="pre">get_lr_ratio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_decay_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/lr_scheduler.html#get_lr_ratio"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.lr_scheduler.get_lr_ratio" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get learning rate ratio for each epoch.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>epoch (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Current epoch.</p></li>
<li><p>warmup_epochs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Warmup epochs.</p></li>
<li><p>learning_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Learning rate.</p></li>
<li><p>lr_decay_epochs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Learning rate decay epochs.</p></li>
<li><p>min_lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Minimum learning rate.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="cos-lr-scheduler">
<h3>cos_lr_scheduler<a class="headerlink" href="#cos-lr-scheduler" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.lr_scheduler.cos_lr_scheduler">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.lr_scheduler.</span></span><span class="sig-name descname"><span class="pre">cos_lr_scheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_decay_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">6e-05</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LambdaLR</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/lr_scheduler.html#cos_lr_scheduler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.lr_scheduler.cos_lr_scheduler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Cosine learning rate scheduler.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>optimizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code>): Optimizer.</p></li>
<li><p>learning_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Learning rate.</p></li>
<li><p>warmup_epochs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Warmup epochs.</p></li>
<li><p>lr_decay_epochs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Learning rate decay epochs.</p></li>
<li><p>min_lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Minimum learning rate.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="math-helper">
<h2>math_helper<a class="headerlink" href="#math-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/math_helper</span></code> for more details.</p>
<section id="cov">
<h3>cov<a class="headerlink" href="#cov" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.math_helper.cov">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.math_helper.</span></span><span class="sig-name descname"><span class="pre">cov</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rowvar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ddof</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aweights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/math_helper.html#cov"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.math_helper.cov" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Estimates covariance matrix like <code class="docutils literal notranslate"><span class="pre">numpy.cov</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): A 1-D or 2-D tensor containing multiple variables and observations. Each row of             <code class="docutils literal notranslate"><span class="pre">x</span></code> represents a variable, and each column a single observation of all those variables.</p></li>
<li><p>rowvar (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): If <code class="docutils literal notranslate"><span class="pre">rowvar</span></code> is True by default, and each column is a single observation of all those             variables. Otherwise, each column represents a variable, while the rows contain observations.</p></li>
<li><p>bias (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Default normalization (False) is by dividing <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">-</span> <span class="pre">1</span></code>, where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of             observations given (unbiased estimate). If <code class="docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then normalization is by <code class="docutils literal notranslate"><span class="pre">N</span></code>.</p></li>
<li><p>ddof (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[int]</span></code>): If <code class="docutils literal notranslate"><span class="pre">ddof</span></code> is not <code class="docutils literal notranslate"><span class="pre">None</span></code>, it implies that the argument <code class="docutils literal notranslate"><span class="pre">bias</span></code> is             overridden. Note that <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> will return the unbiased estimate (equals to <code class="docutils literal notranslate"><span class="pre">bias=False</span></code>), and             <code class="docutils literal notranslate"><span class="pre">ddof=0</span></code> will return the biased estimation (equals to <code class="docutils literal notranslate"><span class="pre">bias=True</span></code>).</p></li>
<li><p>aweights (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): 1-D tensor of observation vector weights. These relative weights             are typically large for observations considered “important” and smaller for observations considered less             “important”. If <code class="docutils literal notranslate"><span class="pre">ddof=0</span></code>, the tensor of weights can be used to assign weights to observation vectors.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>cov_mat (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Covariance matrix calculated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="metric">
<h2>metric<a class="headerlink" href="#metric" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/metric</span></code> for more details.</p>
<section id="levenshtein-distance">
<h3>levenshtein_distance<a class="headerlink" href="#levenshtein-distance" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.metric.levenshtein_distance">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.metric.</span></span><span class="sig-name descname"><span class="pre">levenshtein_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_extra</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_extra</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">FloatTensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/metric.html#levenshtein_distance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.metric.levenshtein_distance" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Levenshtein Distance, i.e. Edit Distance.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>pred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): The first tensor to calculate the distance, shape: (N1, )  (N1 &gt;= 0).</p></li>
<li><p>target (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): The second tensor to calculate the distance, shape: (N2, )  (N2 &gt;= 0).</p></li>
<li><p>pred_extra (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): Extra tensor to calculate the distance, only works when             <code class="docutils literal notranslate"><span class="pre">extra_fn</span></code> is not <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>target_extra (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): Extra tensor to calculate the distance, only works when             <code class="docutils literal notranslate"><span class="pre">extra_fn</span></code> is not <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>extra_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[Callable]</span></code>): The distance function for <code class="docutils literal notranslate"><span class="pre">pred_extra</span></code> and             <code class="docutils literal notranslate"><span class="pre">target_extra</span></code>. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, this distance will not be considered.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>distance (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): distance(scalar), shape: (1, ).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="hamming-distance">
<h3>hamming_distance<a class="headerlink" href="#hamming-distance" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.metric.hamming_distance">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.metric.</span></span><span class="sig-name descname"><span class="pre">hamming_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LongTensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/metric.html#hamming_distance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.metric.hamming_distance" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Hamming Distance.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>pred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Pred input, boolean vector(0 or 1).</p></li>
<li><p>target (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Target input, boolean vector(0 or 1).</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Weight to multiply.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>distance(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): Distance (scalar), shape (1, ).</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>pred &amp; target (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): shape <span class="math notranslate nohighlight">\((B, N)\)</span>,             while B is the batch size, N is the dimension</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="model-helper">
<h2>model_helper<a class="headerlink" href="#model-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/model_helper</span></code> for more details.</p>
<section id="get-num-params">
<h3>get_num_params<a class="headerlink" href="#get-num-params" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.model_helper.get_num_params">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.model_helper.</span></span><span class="sig-name descname"><span class="pre">get_num_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/model_helper.html#get_num_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.model_helper.get_num_params" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Return the number of parameters in the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The model object to calculate the parameter number.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>n_params (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The calculated number of parameters.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num</span> <span class="o">=</span> <span class="n">get_num_params</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">num</span> <span class="o">==</span> <span class="mi">15</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="nn-test-helper">
<h2>nn_test_helper<a class="headerlink" href="#nn-test-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/nn_test_helper</span></code> for more details.</p>
<section id="is-differentiable">
<h3>is_differentiable<a class="headerlink" href="#is-differentiable" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.nn_test_helper.is_differentiable">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.nn_test_helper.</span></span><span class="sig-name descname"><span class="pre">is_differentiable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Module</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_instead</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/nn_test_helper.html#is_differentiable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.nn_test_helper.is_differentiable" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Judge whether the model/models are differentiable. First check whether module’s grad is None,
then do loss’s back propagation, finally check whether module’s grad are torch.Tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): loss tensor of the model</p></li>
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.nn.Module,</span> <span class="pre">List[torch.nn.Module]]</span></code>): model or models to be checked</p></li>
<li><p>print_instead (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to print module’s final grad result,             instead of asserting. Default set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="optimizer-helper">
<h2>optimizer_helper<a class="headerlink" href="#optimizer-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/optimizer_helper</span></code> for more details.</p>
<section id="calculate-grad-norm">
<h3>calculate_grad_norm<a class="headerlink" href="#calculate-grad-norm" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.calculate_grad_norm">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">calculate_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#calculate_grad_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.calculate_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>calculate grad norm of the parameters whose grad norms are not None in the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model: torch.nn.Module</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code> or <cite>inf</cite>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="calculate-grad-norm-without-bias-two-norm">
<h3>calculate_grad_norm_without_bias_two_norm<a class="headerlink" href="#calculate-grad-norm-without-bias-two-norm" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.calculate_grad_norm_without_bias_two_norm">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">calculate_grad_norm_without_bias_two_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#calculate_grad_norm_without_bias_two_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.calculate_grad_norm_without_bias_two_norm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>calculate grad norm of the parameters whose grad norms are not None in the model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model: torch.nn.Module</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="grad-ignore-norm">
<h3>grad_ignore_norm<a class="headerlink" href="#grad-ignore-norm" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.grad_ignore_norm">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">grad_ignore_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#grad_ignore_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.grad_ignore_norm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Clip the gradient norm of an iterable of parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>parameters (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable</span></code>): an iterable of torch.Tensor</p></li>
<li><p>max_norm (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the max norm of the gradients</p></li>
<li><p>norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): 2.0 means use norm2 to clip</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="grad-ignore-value">
<h3>grad_ignore_value<a class="headerlink" href="#grad-ignore-value" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.grad_ignore_value">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">grad_ignore_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#grad_ignore_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.grad_ignore_value" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Clip the gradient value of an iterable of parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>parameters (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Iterable</span></code>): an iterable of torch.Tensor</p></li>
<li><p>clip_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the value to start clipping</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="adam">
<h3>Adam<a class="headerlink" href="#adam" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">betas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(0.9,</span> <span class="pre">0.999)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amsgrad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_ignore_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#Adam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Rewrited Adam optimizer to support more features.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">step</span></code>, <code class="docutils literal notranslate"><span class="pre">_state_init</span></code>, <code class="docutils literal notranslate"><span class="pre">get_grad</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">betas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(0.9,</span> <span class="pre">0.999)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amsgrad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_ignore_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#Adam.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>init method of refactored Adam class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>params (<code class="xref py py-obj docutils literal notranslate"><span class="pre">iterable</span></code>):  – an iterable of torch.Tensor s or dict s.                 Specifies what Tensors should be optimized</p></li>
<li><p>lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): learning rate, default set to 1e-3</p></li>
<li><p>betas (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[float,</span> <span class="pre">float]</span></code>): coefficients used for computing running averages of gradient and its                square, default set to (0.9, 0.999))</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): term added to the denominator to improve numerical stability, default set to 1e-8</p></li>
<li><p>weight_decay (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): weight decay coefficient, deault set to 0</p></li>
<li><p>amsgrad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use the AMSGrad variant of this algorithm from the paper                On the Convergence of Adam and Beyond &lt;<a class="reference external" href="https://arxiv.org/abs/1904.09237">https://arxiv.org/abs/1904.09237</a>&gt;</p></li>
<li><p>optim_type (:obj:str): support [“adam”, “adamw”]</p></li>
<li><p>grad_clip_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘clip_momentum’, ‘clip_value’, ‘clip_norm’,                 ‘clip_momentum_norm’]</p></li>
<li><p>clip_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the value to start clipping</p></li>
<li><p>clip_coef (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the cliping coefficient</p></li>
<li><p>clip_norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): 2.0 means use norm2 to clip</p></li>
<li><p>clip_momentum_timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): after how many step should we start the momentum clipping</p></li>
<li><p>grad_ignore_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘ignore_momentum’, ‘ignore_value’, ‘ignore_norm’,                 ‘ignore_momentum_norm’]</p></li>
<li><p>ignore_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the value to start ignoring</p></li>
<li><p>ignore_coef (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the ignoreing coefficient</p></li>
<li><p>ignore_norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): 2.0 means use norm2 to ignore</p></li>
<li><p>ignore_momentum_timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): after how many step should we start the momentum ignoring</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_step_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_step_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Self</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_step_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._optimizer_step_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_step_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Self</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_step_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam._state_init">
<span class="sig-name descname"><span class="pre">_state_init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amsgrad</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#Adam._state_init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam._state_init" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the state of the optimizer</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>p (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the parameter to be optimized</p></li>
<li><p>amsgrad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use the AMSGrad variant of this algorithm from the paper                On the Convergence of Adam and Beyond &lt;<a class="reference external" href="https://arxiv.org/abs/1904.09237">https://arxiv.org/abs/1904.09237</a>&gt;</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam.get_grad">
<span class="sig-name descname"><span class="pre">get_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#Adam.get_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam.get_grad" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.Adam.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#Adam.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.Adam.step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Performs a single optimization step</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>closure (<code class="xref py py-obj docutils literal notranslate"><span class="pre">callable</span></code>): A closure that reevaluates the model and returns the loss, default set to None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="rmsprop">
<h3>RMSprop<a class="headerlink" href="#rmsprop" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">RMSprop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">centered</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_ignore_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#RMSprop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Rewrited RMSprop optimizer to support more features.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">step</span></code>, <code class="docutils literal notranslate"><span class="pre">_state_init</span></code>, <code class="docutils literal notranslate"><span class="pre">get_grad</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">centered</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_ignore_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_norm_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_momentum_timestep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#RMSprop.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>init method of refactored Adam class</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>params (<code class="xref py py-obj docutils literal notranslate"><span class="pre">iterable</span></code>):  – an iterable of torch.Tensor s or dict s.                 Specifies what Tensors should be optimized</p></li>
<li><p>lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): learning rate, default set to 1e-3</p></li>
<li><p>alpha (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): smoothing constant, default set to 0.99</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): term added to the denominator to improve numerical stability, default set to 1e-8</p></li>
<li><p>weight_decay (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): weight decay coefficient, deault set to 0</p></li>
<li><p>centred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): if True, compute the centered RMSprop,                 the gradient is normalized by an estimation of its variance</p></li>
<li><p>grad_clip_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘clip_momentum’, ‘clip_value’, ‘clip_norm’,                 ‘clip_momentum_norm’]</p></li>
<li><p>clip_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the value to start clipping</p></li>
<li><p>clip_coef (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the cliping coefficient</p></li>
<li><p>clip_norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): 2.0 means use norm2 to clip</p></li>
<li><p>clip_momentum_timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): after how many step should we start the momentum clipping</p></li>
<li><p>grad_ignore_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): support [None, ‘ignore_momentum’, ‘ignore_value’, ‘ignore_norm’,                 ‘ignore_momentum_norm’]</p></li>
<li><p>ignore_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the value to start ignoring</p></li>
<li><p>ignore_coef (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the ignoreing coefficient</p></li>
<li><p>ignore_norm_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): 2.0 means use norm2 to ignore</p></li>
<li><p>ignore_momentum_timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): after how many step should we start the momentum ignoring</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">StateDict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">OrderedDict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'Optimizer'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_post_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_step_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Self</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_pre_hooks">
<span class="sig-name descname"><span class="pre">_optimizer_step_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Self</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop._state_init">
<span class="sig-name descname"><span class="pre">_state_init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">centered</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#RMSprop._state_init"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop._state_init" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the state of the optimizer</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>p (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the parameter to be optimized</p></li>
<li><p>momentum (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the momentum coefficient</p></li>
<li><p>centered (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): if True, compute the centered RMSprop,                 the gradient is normalized by an estimation of its variance</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop.get_grad">
<span class="sig-name descname"><span class="pre">get_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#RMSprop.get_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop.get_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>calculate grad norm of the parameters whose grad norms are not None in the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.RMSprop.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#RMSprop.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.RMSprop.step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Performs a single optimization step</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>closure (<code class="xref py py-obj docutils literal notranslate"><span class="pre">callable</span></code>): A closure that reevaluates the model and returns the loss, default set to None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="pcgrad">
<h3>PCGrad<a class="headerlink" href="#pcgrad" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">PCGrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>PCGrad optimizer to support multi-task.
you can view the paper in the following link <a class="reference external" href="https://arxiv.org/pdf/2001.06782.pdf">https://arxiv.org/pdf/2001.06782.pdf</a></p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">zero_grad</span></code>, <code class="docutils literal notranslate"><span class="pre">step</span></code>, <code class="docutils literal notranslate"><span class="pre">pc_backward</span></code></p>
</dd>
<dt>Properties:</dt><dd><ul class="simple">
<li><p>optimizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim</span></code>): the optimizer to be used</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization of PCGrad optimizer</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>optimizer (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim</span></code>): the optimizer to be used</p></li>
<li><p>reduction (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the reduction method, support [‘mean’, ‘sum’]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._flatten_grad">
<span class="sig-name descname"><span class="pre">_flatten_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shapes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._flatten_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._flatten_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>flatten the gradient of the parameters of the network</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>grads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the gradient of the parameters</p></li>
<li><p>shapes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the shape of the parameters</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._pack_grad">
<span class="sig-name descname"><span class="pre">_pack_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objectives</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._pack_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._pack_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>pack the gradient of the parameters of the network for each objective</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>objectives: a list of objectives</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>grad: a list of the gradient of the parameters</p></li>
<li><p>shape: a list of the shape of the parameters</p></li>
<li><p>has_grad: a list of mask represent whether the parameter has gradient</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._project_conflicting">
<span class="sig-name descname"><span class="pre">_project_conflicting</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shapes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._project_conflicting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._project_conflicting" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>project the conflicting gradient to the orthogonal space</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>grads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the gradient of the parameters</p></li>
<li><p>has_grads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of mask represent whether the parameter has gradient</p></li>
<li><p>shapes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the shape of the parameters</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._retrieve_grad">
<span class="sig-name descname"><span class="pre">_retrieve_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._retrieve_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._retrieve_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>get the gradient of the parameters of the network with specific objective</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>grad: a list of the gradient of the parameters</p></li>
<li><p>shape: a list of the shape of the parameters</p></li>
<li><p>has_grad: a list of mask represent whether the parameter has gradient</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._set_grad">
<span class="sig-name descname"><span class="pre">_set_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._set_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._set_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>set the modified gradients to the network</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>grads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the gradient of the parameters</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad._unflatten_grad">
<span class="sig-name descname"><span class="pre">_unflatten_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shapes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad._unflatten_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad._unflatten_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>unflatten the gradient of the parameters of the network</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>grads (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the gradient of the parameters</p></li>
<li><p>shapes (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): a list of the shape of the parameters</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad.optimizer">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">optimizer</span></span><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>get the optimizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad.pc_backward">
<span class="sig-name descname"><span class="pre">pc_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objectives</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad.pc_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad.pc_backward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>calculate the gradient of the parameters</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>objectives: a list of objectives</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad.step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>update the parameters with the gradient</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.PCGrad.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#PCGrad.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.PCGrad.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>clear the gradient of the parameters</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="configure-weight-decay">
<h3>configure_weight_decay<a class="headerlink" href="#configure-weight-decay" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.optimizer_helper.configure_weight_decay">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.optimizer_helper.</span></span><span class="sig-name descname"><span class="pre">configure_weight_decay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/optimizer_helper.html#configure_weight_decay"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.optimizer_helper.configure_weight_decay" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Separating out all parameters of the model into two buckets: those that will experience
weight decay for regularization and those that won’t (biases, and layer-norm or embedding weights).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>): The given PyTorch model.</p></li>
<li><p>weight_decay (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Weight decay value for optimizer.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>optim groups (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List</span></code>): The parameter groups to be set in the latter optimizer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="parameter">
<h2>parameter<a class="headerlink" href="#parameter" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/parameter</span></code> for more details.</p>
<section id="nonegativeparameter">
<h3>NonegativeParameter<a class="headerlink" href="#nonegativeparameter" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.parameter.</span></span><span class="sig-name descname"><span class="pre">NonegativeParameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#NonegativeParameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This module will output a non-negative parameter during the forward process.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">set_data</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#NonegativeParameter.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the NonegativeParameter object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The initial value of generated parameter. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the                 default value is 0.</p></li>
<li><p>requires_grad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether this parameter requires grad.</p></li>
<li><p>delta (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The delta of log function.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#NonegativeParameter.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Output the non-negative parameter during the forward process.</p>
</dd>
<dt>Returns:</dt><dd><p>parameter (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The generated parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter.set_data">
<span class="sig-name descname"><span class="pre">set_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#NonegativeParameter.set_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter.set_data" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview：</dt><dd><p>Set the value of the non-negative parameter.</p>
</dd>
<dt>Arguments:</dt><dd><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The new value of the non-negative parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.NonegativeParameter.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.parameter.NonegativeParameter.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="tanhparameter">
<h3>TanhParameter<a class="headerlink" href="#tanhparameter" title="Permalink to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ding.torch_utils.parameter.</span></span><span class="sig-name descname"><span class="pre">TanhParameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#TanhParameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>This module will output a tanh parameter during the forward process.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">set_data</span></code>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#TanhParameter.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the TanhParameter object using the given arguments.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[torch.Tensor]</span></code>): The initial value of generated parameter. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the                 default value is 1.</p></li>
<li><p>requires_grad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether this parameter requires grad.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._backward_hooks">
<span class="sig-name descname"><span class="pre">_backward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._backward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._backward_pre_hooks">
<span class="sig-name descname"><span class="pre">_backward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._backward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._buffers">
<span class="sig-name descname"><span class="pre">_buffers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._buffers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._forward_hooks">
<span class="sig-name descname"><span class="pre">_forward_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._forward_hooks_always_called">
<span class="sig-name descname"><span class="pre">_forward_hooks_always_called</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks_always_called" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._forward_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._forward_pre_hooks">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._forward_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._forward_pre_hooks_with_kwargs">
<span class="sig-name descname"><span class="pre">_forward_pre_hooks_with_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._forward_pre_hooks_with_kwargs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._is_full_backward_hook">
<span class="sig-name descname"><span class="pre">_is_full_backward_hook</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._is_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._load_state_dict_post_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_post_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._load_state_dict_post_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._load_state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_load_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._load_state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._modules">
<span class="sig-name descname"><span class="pre">_modules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._modules" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._non_persistent_buffers_set">
<span class="sig-name descname"><span class="pre">_non_persistent_buffers_set</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._non_persistent_buffers_set" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._parameters">
<span class="sig-name descname"><span class="pre">_parameters</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Parameter</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._state_dict_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._state_dict_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter._state_dict_pre_hooks">
<span class="sig-name descname"><span class="pre">_state_dict_pre_hooks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter._state_dict_pre_hooks" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#TanhParameter.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Output the tanh parameter during the forward process.</p>
</dd>
<dt>Returns:</dt><dd><p>parameter (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The generated parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter.set_data">
<span class="sig-name descname"><span class="pre">set_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/parameter.html#TanhParameter.set_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter.set_data" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Set the value of the tanh parameter.</p>
</dd>
<dt>Arguments:</dt><dd><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The new value of the tanh parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="ding.torch_utils.parameter.TanhParameter.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#ding.torch_utils.parameter.TanhParameter.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="reshape-helper">
<h2>reshape_helper<a class="headerlink" href="#reshape-helper" title="Permalink to this heading">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/torch_utils/reshape_helper</span></code> for more details.</p>
<section id="fold-batch">
<h3>fold_batch<a class="headerlink" href="#fold-batch" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.reshape_helper.fold_batch">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.reshape_helper.</span></span><span class="sig-name descname"><span class="pre">fold_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nonbatch_ndims</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Size</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/reshape_helper.html#fold_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.reshape_helper.fold_batch" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p><span class="math notranslate nohighlight">\((T, B, X) \leftarrow (T*B, X)\)</span>        Fold the first (ndim - nonbatch_ndims) dimensions of a tensor as batch dimension.        This operation is similar to <cite>torch.flatten</cite> but provides an inverse function
<cite>unfold_batch</cite> to restore the folded dimensions.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the tensor to fold</p></li>
<li><dl class="simple">
<dt>nonbatch_ndims (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the number of dimensions that is not folded as</dt><dd><p>batch dimension.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the folded tensor</p></li>
<li><dl class="simple">
<dt>batch_dims: the folded dimensions of the original tensor, which can be used to</dt><dd><p>reverse the operation</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">batch_dim</span> <span class="o">=</span> <span class="n">fold_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_dim</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="unfold-batch">
<h3>unfold_batch<a class="headerlink" href="#unfold-batch" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.reshape_helper.unfold_batch">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.reshape_helper.</span></span><span class="sig-name descname"><span class="pre">unfold_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_dims</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Size</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/reshape_helper.html#unfold_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.reshape_helper.unfold_batch" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Unfold the batch dimension of a tensor.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the tensor to unfold</p></li>
<li><p>batch_dims (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Size</span></code>): the dimensions that are folded</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the original unfolded tensor</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">batch_dim</span> <span class="o">=</span> <span class="n">fold_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_dim</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">unfold_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="unsqueeze-repeat">
<h3>unsqueeze_repeat<a class="headerlink" href="#unsqueeze-repeat" title="Permalink to this heading">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.torch_utils.reshape_helper.unsqueeze_repeat">
<span class="sig-prename descclassname"><span class="pre">ding.torch_utils.reshape_helper.</span></span><span class="sig-name descname"><span class="pre">unsqueeze_repeat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat_times</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsqueeze_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/ding/torch_utils/reshape_helper.html#unsqueeze_repeat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.torch_utils.reshape_helper.unsqueeze_repeat" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Squeeze the tensor on <cite>unsqueeze_dim</cite> and then repeat in this dimension for <cite>repeat_times</cite> times.        This is useful for preproprocessing the input to an model ensemble.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the tensor to squeeze and repeat</p></li>
<li><p>repeat_times (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the times that the tensor is repeatd</p></li>
<li><p>unsqueeze_dim (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): the unsqueezed dimension</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>x (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the unsqueezed and repeated tensor</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">unsqueeze_repeat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">unsqueeze_repeat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="utils.html" class="btn btn-neutral float-right" title="ding.utils" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="rl_utils.html" class="btn btn-neutral" title="ding.rl_utils" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">ding.torch_utils</a><ul>
<li><a class="reference internal" href="#loss">loss</a><ul>
<li><a class="reference internal" href="#contrastiveloss">ContrastiveLoss</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss.__init__"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._buffers"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._create_encoder"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._create_encoder()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._modules"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._parameters"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss.forward"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.ContrastiveLoss.training"><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#labelsmoothceloss">LabelSmoothCELoss</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss.__init__"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._backward_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._buffers"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._modules"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._parameters"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss.forward"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.LabelSmoothCELoss.training"><code class="docutils literal notranslate"><span class="pre">LabelSmoothCELoss.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#softfocalloss">SoftFocalLoss</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss.__init__"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._backward_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._buffers"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._modules"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._parameters"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss.forward"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.SoftFocalLoss.training"><code class="docutils literal notranslate"><span class="pre">SoftFocalLoss.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#build-ce-criterion">build_ce_criterion</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.build_ce_criterion"><code class="docutils literal notranslate"><span class="pre">build_ce_criterion()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#multilogitsloss">MultiLogitsLoss</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss.__init__"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._backward_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._buffers"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._get_distance_matrix"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._get_distance_matrix()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._get_metric_matrix"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._get_metric_matrix()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._label_process"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._label_process()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._match"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._match()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._modules"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._nll_loss"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._nll_loss()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._parameters"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss.forward"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.loss.MultiLogitsLoss.training"><code class="docutils literal notranslate"><span class="pre">MultiLogitsLoss.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-activation">network.activation</a><ul>
<li><a class="reference internal" href="#lambda">Lambda</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda"><code class="docutils literal notranslate"><span class="pre">Lambda</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda.__init__"><code class="docutils literal notranslate"><span class="pre">Lambda.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._buffers"><code class="docutils literal notranslate"><span class="pre">Lambda._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Lambda._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Lambda._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Lambda._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Lambda._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._modules"><code class="docutils literal notranslate"><span class="pre">Lambda._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Lambda._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._parameters"><code class="docutils literal notranslate"><span class="pre">Lambda._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Lambda._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda.forward"><code class="docutils literal notranslate"><span class="pre">Lambda.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Lambda.training"><code class="docutils literal notranslate"><span class="pre">Lambda.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#glu">GLU</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU"><code class="docutils literal notranslate"><span class="pre">GLU</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU.__init__"><code class="docutils literal notranslate"><span class="pre">GLU.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._buffers"><code class="docutils literal notranslate"><span class="pre">GLU._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GLU._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GLU._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GLU._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GLU._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._modules"><code class="docutils literal notranslate"><span class="pre">GLU._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GLU._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._parameters"><code class="docutils literal notranslate"><span class="pre">GLU._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GLU._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU.forward"><code class="docutils literal notranslate"><span class="pre">GLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GLU.training"><code class="docutils literal notranslate"><span class="pre">GLU.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#swish">Swish</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish"><code class="docutils literal notranslate"><span class="pre">Swish</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish.__init__"><code class="docutils literal notranslate"><span class="pre">Swish.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._buffers"><code class="docutils literal notranslate"><span class="pre">Swish._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Swish._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Swish._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Swish._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Swish._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._modules"><code class="docutils literal notranslate"><span class="pre">Swish._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Swish._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._parameters"><code class="docutils literal notranslate"><span class="pre">Swish._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Swish._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish.forward"><code class="docutils literal notranslate"><span class="pre">Swish.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.Swish.training"><code class="docutils literal notranslate"><span class="pre">Swish.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#gelu">GELU</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU"><code class="docutils literal notranslate"><span class="pre">GELU</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU.__init__"><code class="docutils literal notranslate"><span class="pre">GELU.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._buffers"><code class="docutils literal notranslate"><span class="pre">GELU._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GELU._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GELU._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GELU._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GELU._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._modules"><code class="docutils literal notranslate"><span class="pre">GELU._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GELU._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._parameters"><code class="docutils literal notranslate"><span class="pre">GELU._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GELU._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU.forward"><code class="docutils literal notranslate"><span class="pre">GELU.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.GELU.training"><code class="docutils literal notranslate"><span class="pre">GELU.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#build-activation">build_activation</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.activation.build_activation"><code class="docutils literal notranslate"><span class="pre">build_activation()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-diffusion">network.diffusion</a><ul>
<li><a class="reference internal" href="#extract">extract</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.extract"><code class="docutils literal notranslate"><span class="pre">extract()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#cosine-beta-schedule">cosine_beta_schedule</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.cosine_beta_schedule"><code class="docutils literal notranslate"><span class="pre">cosine_beta_schedule()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#apply-conditioning">apply_conditioning</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.apply_conditioning"><code class="docutils literal notranslate"><span class="pre">apply_conditioning()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#diffusionconv1d">DiffusionConv1d</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.__init__"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._backward_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._buffers"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._modules"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._parameters"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.forward"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionConv1d.training"><code class="docutils literal notranslate"><span class="pre">DiffusionConv1d.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sinusoidalposemb">SinusoidalPosEmb</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.__init__"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._buffers"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._modules"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._parameters"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.forward"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.SinusoidalPosEmb.training"><code class="docutils literal notranslate"><span class="pre">SinusoidalPosEmb.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#residual">Residual</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual"><code class="docutils literal notranslate"><span class="pre">Residual</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual.__init__"><code class="docutils literal notranslate"><span class="pre">Residual.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._buffers"><code class="docutils literal notranslate"><span class="pre">Residual._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Residual._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Residual._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Residual._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Residual._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._modules"><code class="docutils literal notranslate"><span class="pre">Residual._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Residual._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._parameters"><code class="docutils literal notranslate"><span class="pre">Residual._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Residual._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual.forward"><code class="docutils literal notranslate"><span class="pre">Residual.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.Residual.training"><code class="docutils literal notranslate"><span class="pre">Residual.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#layernorm">LayerNorm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm.__init__"><code class="docutils literal notranslate"><span class="pre">LayerNorm.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._backward_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._buffers"><code class="docutils literal notranslate"><span class="pre">LayerNorm._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">LayerNorm._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LayerNorm._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LayerNorm._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">LayerNorm._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._modules"><code class="docutils literal notranslate"><span class="pre">LayerNorm._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">LayerNorm._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._parameters"><code class="docutils literal notranslate"><span class="pre">LayerNorm._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LayerNorm._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm.forward"><code class="docutils literal notranslate"><span class="pre">LayerNorm.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LayerNorm.training"><code class="docutils literal notranslate"><span class="pre">LayerNorm.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#prenorm">PreNorm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm"><code class="docutils literal notranslate"><span class="pre">PreNorm</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm.__init__"><code class="docutils literal notranslate"><span class="pre">PreNorm.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._backward_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._buffers"><code class="docutils literal notranslate"><span class="pre">PreNorm._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">PreNorm._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PreNorm._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PreNorm._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">PreNorm._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._modules"><code class="docutils literal notranslate"><span class="pre">PreNorm._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">PreNorm._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._parameters"><code class="docutils literal notranslate"><span class="pre">PreNorm._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PreNorm._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm.forward"><code class="docutils literal notranslate"><span class="pre">PreNorm.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.PreNorm.training"><code class="docutils literal notranslate"><span class="pre">PreNorm.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#linearattention">LinearAttention</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention"><code class="docutils literal notranslate"><span class="pre">LinearAttention</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention.__init__"><code class="docutils literal notranslate"><span class="pre">LinearAttention.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._backward_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._buffers"><code class="docutils literal notranslate"><span class="pre">LinearAttention._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">LinearAttention._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LinearAttention._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LinearAttention._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">LinearAttention._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._modules"><code class="docutils literal notranslate"><span class="pre">LinearAttention._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">LinearAttention._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._parameters"><code class="docutils literal notranslate"><span class="pre">LinearAttention._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LinearAttention._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention.forward"><code class="docutils literal notranslate"><span class="pre">LinearAttention.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.LinearAttention.training"><code class="docutils literal notranslate"><span class="pre">LinearAttention.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#residualtemporalblock">ResidualTemporalBlock</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.__init__"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._buffers"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._modules"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._parameters"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.forward"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.ResidualTemporalBlock.training"><code class="docutils literal notranslate"><span class="pre">ResidualTemporalBlock.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#diffusionunet1d">DiffusionUNet1d</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.__init__"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._buffers"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._modules"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._parameters"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.forward"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.get_pred"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d.get_pred()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.DiffusionUNet1d.training"><code class="docutils literal notranslate"><span class="pre">DiffusionUNet1d.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#temporalvalue">TemporalValue</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue"><code class="docutils literal notranslate"><span class="pre">TemporalValue</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue.__init__"><code class="docutils literal notranslate"><span class="pre">TemporalValue.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._backward_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._buffers"><code class="docutils literal notranslate"><span class="pre">TemporalValue._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">TemporalValue._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TemporalValue._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TemporalValue._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">TemporalValue._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._modules"><code class="docutils literal notranslate"><span class="pre">TemporalValue._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">TemporalValue._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._parameters"><code class="docutils literal notranslate"><span class="pre">TemporalValue._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TemporalValue._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue.forward"><code class="docutils literal notranslate"><span class="pre">TemporalValue.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.diffusion.TemporalValue.training"><code class="docutils literal notranslate"><span class="pre">TemporalValue.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-dreamer">network.dreamer</a><ul>
<li><a class="reference internal" href="#conv2dsame">Conv2dSame</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame"><code class="docutils literal notranslate"><span class="pre">Conv2dSame</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame._reversed_padding_repeated_twice"><code class="docutils literal notranslate"><span class="pre">Conv2dSame._reversed_padding_repeated_twice</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.bias"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.bias</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.calc_same_pad"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.calc_same_pad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.dilation"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.dilation</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.forward"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.groups"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.groups</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.in_channels"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.in_channels</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.kernel_size"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.kernel_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.out_channels"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.out_channels</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.output_padding"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.output_padding</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.padding"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.padding</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.padding_mode"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.padding_mode</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.stride"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.stride</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.transposed"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.transposed</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Conv2dSame.weight"><code class="docutils literal notranslate"><span class="pre">Conv2dSame.weight</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#dreamerlayernorm">DreamerLayerNorm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.__init__"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._buffers"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._modules"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._parameters"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.forward"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DreamerLayerNorm.training"><code class="docutils literal notranslate"><span class="pre">DreamerLayerNorm.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#densehead">DenseHead</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead"><code class="docutils literal notranslate"><span class="pre">DenseHead</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead.__init__"><code class="docutils literal notranslate"><span class="pre">DenseHead.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._backward_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._buffers"><code class="docutils literal notranslate"><span class="pre">DenseHead._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">DenseHead._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DenseHead._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DenseHead._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">DenseHead._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._modules"><code class="docutils literal notranslate"><span class="pre">DenseHead._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">DenseHead._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._parameters"><code class="docutils literal notranslate"><span class="pre">DenseHead._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DenseHead._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead.forward"><code class="docutils literal notranslate"><span class="pre">DenseHead.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.DenseHead.training"><code class="docutils literal notranslate"><span class="pre">DenseHead.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#actionhead">ActionHead</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead"><code class="docutils literal notranslate"><span class="pre">ActionHead</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead.__init__"><code class="docutils literal notranslate"><span class="pre">ActionHead.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._buffers"><code class="docutils literal notranslate"><span class="pre">ActionHead._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ActionHead._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ActionHead._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ActionHead._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ActionHead._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._modules"><code class="docutils literal notranslate"><span class="pre">ActionHead._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ActionHead._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._parameters"><code class="docutils literal notranslate"><span class="pre">ActionHead._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ActionHead._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead.forward"><code class="docutils literal notranslate"><span class="pre">ActionHead.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ActionHead.training"><code class="docutils literal notranslate"><span class="pre">ActionHead.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sampledist">SampleDist</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SampleDist"><code class="docutils literal notranslate"><span class="pre">SampleDist</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SampleDist.__init__"><code class="docutils literal notranslate"><span class="pre">SampleDist.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SampleDist.entropy"><code class="docutils literal notranslate"><span class="pre">SampleDist.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SampleDist.mean"><code class="docutils literal notranslate"><span class="pre">SampleDist.mean()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SampleDist.mode"><code class="docutils literal notranslate"><span class="pre">SampleDist.mode()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#onehotdist">OneHotDist</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.OneHotDist"><code class="docutils literal notranslate"><span class="pre">OneHotDist</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.OneHotDist.__init__"><code class="docutils literal notranslate"><span class="pre">OneHotDist.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.OneHotDist.mode"><code class="docutils literal notranslate"><span class="pre">OneHotDist.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.OneHotDist.sample"><code class="docutils literal notranslate"><span class="pre">OneHotDist.sample()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#twohotdistsymlog">TwoHotDistSymlog</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.__init__"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog.log_prob()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.log_prob_target"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog.log_prob_target()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.mean"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog.mean()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.TwoHotDistSymlog.mode"><code class="docutils literal notranslate"><span class="pre">TwoHotDistSymlog.mode()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#symlogdist">SymlogDist</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SymlogDist"><code class="docutils literal notranslate"><span class="pre">SymlogDist</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SymlogDist.__init__"><code class="docutils literal notranslate"><span class="pre">SymlogDist.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SymlogDist.log_prob"><code class="docutils literal notranslate"><span class="pre">SymlogDist.log_prob()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SymlogDist.mean"><code class="docutils literal notranslate"><span class="pre">SymlogDist.mean()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.SymlogDist.mode"><code class="docutils literal notranslate"><span class="pre">SymlogDist.mode()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#contdist">ContDist</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist"><code class="docutils literal notranslate"><span class="pre">ContDist</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist.__init__"><code class="docutils literal notranslate"><span class="pre">ContDist.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist.entropy"><code class="docutils literal notranslate"><span class="pre">ContDist.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist.log_prob"><code class="docutils literal notranslate"><span class="pre">ContDist.log_prob()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist.mode"><code class="docutils literal notranslate"><span class="pre">ContDist.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.ContDist.sample"><code class="docutils literal notranslate"><span class="pre">ContDist.sample()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#bernoulli">Bernoulli</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli"><code class="docutils literal notranslate"><span class="pre">Bernoulli</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli.__init__"><code class="docutils literal notranslate"><span class="pre">Bernoulli.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli.entropy"><code class="docutils literal notranslate"><span class="pre">Bernoulli.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli.log_prob"><code class="docutils literal notranslate"><span class="pre">Bernoulli.log_prob()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli.mode"><code class="docutils literal notranslate"><span class="pre">Bernoulli.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.dreamer.Bernoulli.sample"><code class="docutils literal notranslate"><span class="pre">Bernoulli.sample()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-gtrxl">network.gtrxl</a><ul>
<li><a class="reference internal" href="#positionalembedding">PositionalEmbedding</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.__init__"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._buffers"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._modules"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._parameters"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.forward"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.PositionalEmbedding.training"><code class="docutils literal notranslate"><span class="pre">PositionalEmbedding.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#grugatingunit">GRUGatingUnit</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.__init__"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._buffers"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._modules"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._parameters"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.forward"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GRUGatingUnit.training"><code class="docutils literal notranslate"><span class="pre">GRUGatingUnit.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#memory">Memory</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory"><code class="docutils literal notranslate"><span class="pre">Memory</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory.__init__"><code class="docutils literal notranslate"><span class="pre">Memory.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory.get"><code class="docutils literal notranslate"><span class="pre">Memory.get()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory.init"><code class="docutils literal notranslate"><span class="pre">Memory.init()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory.to"><code class="docutils literal notranslate"><span class="pre">Memory.to()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.Memory.update"><code class="docutils literal notranslate"><span class="pre">Memory.update()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#attentionxl">AttentionXL</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL"><code class="docutils literal notranslate"><span class="pre">AttentionXL</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL.__init__"><code class="docutils literal notranslate"><span class="pre">AttentionXL.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._backward_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._buffers"><code class="docutils literal notranslate"><span class="pre">AttentionXL._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">AttentionXL._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">AttentionXL._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">AttentionXL._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">AttentionXL._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._modules"><code class="docutils literal notranslate"><span class="pre">AttentionXL._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">AttentionXL._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._parameters"><code class="docutils literal notranslate"><span class="pre">AttentionXL._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._rel_shift"><code class="docutils literal notranslate"><span class="pre">AttentionXL._rel_shift()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">AttentionXL._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL.forward"><code class="docutils literal notranslate"><span class="pre">AttentionXL.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.AttentionXL.training"><code class="docutils literal notranslate"><span class="pre">AttentionXL.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#gatedtransformerxllayer">GatedTransformerXLLayer</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.__init__"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._buffers"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._modules"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._parameters"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.forward"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GatedTransformerXLLayer.training"><code class="docutils literal notranslate"><span class="pre">GatedTransformerXLLayer.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#gtrxl">GTrXL</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL"><code class="docutils literal notranslate"><span class="pre">GTrXL</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL.__init__"><code class="docutils literal notranslate"><span class="pre">GTrXL.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._buffers"><code class="docutils literal notranslate"><span class="pre">GTrXL._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GTrXL._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GTrXL._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GTrXL._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GTrXL._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._modules"><code class="docutils literal notranslate"><span class="pre">GTrXL._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GTrXL._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._parameters"><code class="docutils literal notranslate"><span class="pre">GTrXL._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GTrXL._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL.forward"><code class="docutils literal notranslate"><span class="pre">GTrXL.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL.get_memory"><code class="docutils literal notranslate"><span class="pre">GTrXL.get_memory()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL.reset_memory"><code class="docutils literal notranslate"><span class="pre">GTrXL.reset_memory()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gtrxl.GTrXL.training"><code class="docutils literal notranslate"><span class="pre">GTrXL.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-gumbel-softmax">network.gumbel_softmax</a><ul>
<li><a class="reference internal" href="#gumbelsoftmax">GumbelSoftmax</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.__init__"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._buffers"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._modules"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._parameters"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.gumbel_softmax_sample"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax.gumbel_softmax_sample()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.gumbel_softmax.GumbelSoftmax.training"><code class="docutils literal notranslate"><span class="pre">GumbelSoftmax.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-merge">network.merge</a><ul>
<li><a class="reference internal" href="#bilineargeneral">BilinearGeneral</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral.__init__"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._backward_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._buffers"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._modules"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._parameters"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral.forward"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral.reset_parameters"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral.reset_parameters()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.BilinearGeneral.training"><code class="docutils literal notranslate"><span class="pre">BilinearGeneral.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#torchbilinearcustomized">TorchBilinearCustomized</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.__init__"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._backward_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._buffers"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._modules"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._parameters"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.forward"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.reset_parameters"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized.reset_parameters()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.TorchBilinearCustomized.training"><code class="docutils literal notranslate"><span class="pre">TorchBilinearCustomized.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#film">FiLM</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM"><code class="docutils literal notranslate"><span class="pre">FiLM</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM.__init__"><code class="docutils literal notranslate"><span class="pre">FiLM.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._backward_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._buffers"><code class="docutils literal notranslate"><span class="pre">FiLM._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._forward_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">FiLM._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">FiLM._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">FiLM._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">FiLM._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._modules"><code class="docutils literal notranslate"><span class="pre">FiLM._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">FiLM._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._parameters"><code class="docutils literal notranslate"><span class="pre">FiLM._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">FiLM._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM.forward"><code class="docutils literal notranslate"><span class="pre">FiLM.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.FiLM.training"><code class="docutils literal notranslate"><span class="pre">FiLM.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#gatingtype">GatingType</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType"><code class="docutils literal notranslate"><span class="pre">GatingType</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType.GLOBAL"><code class="docutils literal notranslate"><span class="pre">GatingType.GLOBAL</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType.NONE"><code class="docutils literal notranslate"><span class="pre">GatingType.NONE</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.GatingType.POINTWISE"><code class="docutils literal notranslate"><span class="pre">GatingType.POINTWISE</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#summerge">SumMerge</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge"><code class="docutils literal notranslate"><span class="pre">SumMerge</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._backward_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._buffers"><code class="docutils literal notranslate"><span class="pre">SumMerge._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">SumMerge._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SumMerge._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SumMerge._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">SumMerge._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._modules"><code class="docutils literal notranslate"><span class="pre">SumMerge._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">SumMerge._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._parameters"><code class="docutils literal notranslate"><span class="pre">SumMerge._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SumMerge._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge.forward"><code class="docutils literal notranslate"><span class="pre">SumMerge.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.SumMerge.training"><code class="docutils literal notranslate"><span class="pre">SumMerge.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#vectormerge">VectorMerge</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge"><code class="docutils literal notranslate"><span class="pre">VectorMerge</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge.__init__"><code class="docutils literal notranslate"><span class="pre">VectorMerge.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._backward_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._buffers"><code class="docutils literal notranslate"><span class="pre">VectorMerge._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._compute_gate"><code class="docutils literal notranslate"><span class="pre">VectorMerge._compute_gate()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">VectorMerge._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">VectorMerge._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">VectorMerge._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">VectorMerge._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._modules"><code class="docutils literal notranslate"><span class="pre">VectorMerge._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">VectorMerge._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._parameters"><code class="docutils literal notranslate"><span class="pre">VectorMerge._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">VectorMerge._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge.encode"><code class="docutils literal notranslate"><span class="pre">VectorMerge.encode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge.forward"><code class="docutils literal notranslate"><span class="pre">VectorMerge.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.merge.VectorMerge.training"><code class="docutils literal notranslate"><span class="pre">VectorMerge.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-nn-module">network.nn_module</a><ul>
<li><a class="reference internal" href="#weight-init">weight_init</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.weight_init_"><code class="docutils literal notranslate"><span class="pre">weight_init_()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#sequential-pack">sequential_pack</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.sequential_pack"><code class="docutils literal notranslate"><span class="pre">sequential_pack()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#conv1d-block">conv1d_block</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.conv1d_block"><code class="docutils literal notranslate"><span class="pre">conv1d_block()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#conv2d-block">conv2d_block</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.conv2d_block"><code class="docutils literal notranslate"><span class="pre">conv2d_block()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#deconv2d-block">deconv2d_block</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.deconv2d_block"><code class="docutils literal notranslate"><span class="pre">deconv2d_block()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#fc-block">fc_block</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.fc_block"><code class="docutils literal notranslate"><span class="pre">fc_block()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normed-linear">normed_linear</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.normed_linear"><code class="docutils literal notranslate"><span class="pre">normed_linear()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normed-conv2d">normed_conv2d</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.normed_conv2d"><code class="docutils literal notranslate"><span class="pre">normed_conv2d()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlp">MLP</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.MLP"><code class="docutils literal notranslate"><span class="pre">MLP()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#channelshuffle">ChannelShuffle</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle.__init__"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._buffers"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._modules"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._parameters"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle.forward"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.ChannelShuffle.training"><code class="docutils literal notranslate"><span class="pre">ChannelShuffle.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#one-hot">one_hot</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.one_hot"><code class="docutils literal notranslate"><span class="pre">one_hot()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearestupsample">NearestUpsample</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample"><code class="docutils literal notranslate"><span class="pre">NearestUpsample</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample.__init__"><code class="docutils literal notranslate"><span class="pre">NearestUpsample.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._backward_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._buffers"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._modules"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._parameters"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NearestUpsample._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample.forward"><code class="docutils literal notranslate"><span class="pre">NearestUpsample.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NearestUpsample.training"><code class="docutils literal notranslate"><span class="pre">NearestUpsample.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#bilinearupsample">BilinearUpsample</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample.__init__"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._backward_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._buffers"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._modules"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._parameters"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample.forward"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.BilinearUpsample.training"><code class="docutils literal notranslate"><span class="pre">BilinearUpsample.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#binary-encode">binary_encode</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.binary_encode"><code class="docutils literal notranslate"><span class="pre">binary_encode()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#noiselinearlayer">NoiseLinearLayer</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.__init__"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._buffers"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._modules"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._parameters"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._scale_noise"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._scale_noise()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.forward"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_noise"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer.reset_noise()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.reset_parameters"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer.reset_parameters()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NoiseLinearLayer.training"><code class="docutils literal notranslate"><span class="pre">NoiseLinearLayer.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#noise-block">noise_block</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.noise_block"><code class="docutils literal notranslate"><span class="pre">noise_block()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#naiveflatten">NaiveFlatten</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten.__init__"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._backward_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._buffers"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._modules"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._parameters"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten.forward"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.nn_module.NaiveFlatten.training"><code class="docutils literal notranslate"><span class="pre">NaiveFlatten.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-normalization">network.normalization</a><ul>
<li><a class="reference internal" href="#build-normalization">build_normalization</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.normalization.build_normalization"><code class="docutils literal notranslate"><span class="pre">build_normalization()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-popart">network.popart</a><ul>
<li><a class="reference internal" href="#popart">PopArt</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt"><code class="docutils literal notranslate"><span class="pre">PopArt</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt.__init__"><code class="docutils literal notranslate"><span class="pre">PopArt.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._backward_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._buffers"><code class="docutils literal notranslate"><span class="pre">PopArt._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._forward_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">PopArt._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PopArt._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">PopArt._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">PopArt._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._modules"><code class="docutils literal notranslate"><span class="pre">PopArt._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">PopArt._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._parameters"><code class="docutils literal notranslate"><span class="pre">PopArt._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">PopArt._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt.forward"><code class="docutils literal notranslate"><span class="pre">PopArt.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt.reset_parameters"><code class="docutils literal notranslate"><span class="pre">PopArt.reset_parameters()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt.training"><code class="docutils literal notranslate"><span class="pre">PopArt.training</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.popart.PopArt.update_parameters"><code class="docutils literal notranslate"><span class="pre">PopArt.update_parameters()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-res-block">network.res_block</a><ul>
<li><a class="reference internal" href="#resblock">ResBlock</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock"><code class="docutils literal notranslate"><span class="pre">ResBlock</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock.__init__"><code class="docutils literal notranslate"><span class="pre">ResBlock.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._buffers"><code class="docutils literal notranslate"><span class="pre">ResBlock._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ResBlock._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResBlock._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResBlock._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ResBlock._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._modules"><code class="docutils literal notranslate"><span class="pre">ResBlock._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ResBlock._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._parameters"><code class="docutils literal notranslate"><span class="pre">ResBlock._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResBlock._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock.forward"><code class="docutils literal notranslate"><span class="pre">ResBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResBlock.training"><code class="docutils literal notranslate"><span class="pre">ResBlock.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#resfcblock">ResFCBlock</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock"><code class="docutils literal notranslate"><span class="pre">ResFCBlock</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock.__init__"><code class="docutils literal notranslate"><span class="pre">ResFCBlock.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._buffers"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._modules"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._parameters"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResFCBlock._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock.forward"><code class="docutils literal notranslate"><span class="pre">ResFCBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.res_block.ResFCBlock.training"><code class="docutils literal notranslate"><span class="pre">ResFCBlock.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-resnet">network.resnet</a><ul>
<li><a class="reference internal" href="#to-2tuple">to_2tuple</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.to_2tuple"><code class="docutils literal notranslate"><span class="pre">to_2tuple()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-same-padding">get_same_padding</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.get_same_padding"><code class="docutils literal notranslate"><span class="pre">get_same_padding()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pad-same">pad_same</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.pad_same"><code class="docutils literal notranslate"><span class="pre">pad_same()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#avg-pool2d-same">avg_pool2d_same</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.avg_pool2d_same"><code class="docutils literal notranslate"><span class="pre">avg_pool2d_same()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#avgpool2dsame">AvgPool2dSame</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.__init__"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.ceil_mode"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.ceil_mode</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.count_include_pad"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.count_include_pad</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.forward"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.kernel_size"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.kernel_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.padding"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.padding</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.AvgPool2dSame.stride"><code class="docutils literal notranslate"><span class="pre">AvgPool2dSame.stride</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#create-classifier">create_classifier</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.create_classifier"><code class="docutils literal notranslate"><span class="pre">create_classifier()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#classifierhead">ClassifierHead</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead"><code class="docutils literal notranslate"><span class="pre">ClassifierHead</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead.__init__"><code class="docutils literal notranslate"><span class="pre">ClassifierHead.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._buffers"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._modules"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._parameters"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ClassifierHead._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead.forward"><code class="docutils literal notranslate"><span class="pre">ClassifierHead.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ClassifierHead.training"><code class="docutils literal notranslate"><span class="pre">ClassifierHead.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#create-attn">create_attn</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.create_attn"><code class="docutils literal notranslate"><span class="pre">create_attn()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-padding">get_padding</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.get_padding"><code class="docutils literal notranslate"><span class="pre">get_padding()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#basicblock">BasicBlock</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock"><code class="docutils literal notranslate"><span class="pre">BasicBlock</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock.__init__"><code class="docutils literal notranslate"><span class="pre">BasicBlock.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._backward_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._buffers"><code class="docutils literal notranslate"><span class="pre">BasicBlock._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">BasicBlock._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BasicBlock._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">BasicBlock._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">BasicBlock._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._modules"><code class="docutils literal notranslate"><span class="pre">BasicBlock._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">BasicBlock._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._parameters"><code class="docutils literal notranslate"><span class="pre">BasicBlock._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">BasicBlock._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock.expansion"><code class="docutils literal notranslate"><span class="pre">BasicBlock.expansion</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock.forward"><code class="docutils literal notranslate"><span class="pre">BasicBlock.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock.training"><code class="docutils literal notranslate"><span class="pre">BasicBlock.training</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.BasicBlock.zero_init_last_bn"><code class="docutils literal notranslate"><span class="pre">BasicBlock.zero_init_last_bn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#bottleneck">Bottleneck</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck"><code class="docutils literal notranslate"><span class="pre">Bottleneck</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck.__init__"><code class="docutils literal notranslate"><span class="pre">Bottleneck.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._buffers"><code class="docutils literal notranslate"><span class="pre">Bottleneck._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Bottleneck._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Bottleneck._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Bottleneck._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Bottleneck._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._modules"><code class="docutils literal notranslate"><span class="pre">Bottleneck._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Bottleneck._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._parameters"><code class="docutils literal notranslate"><span class="pre">Bottleneck._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Bottleneck._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck.expansion"><code class="docutils literal notranslate"><span class="pre">Bottleneck.expansion</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck.forward"><code class="docutils literal notranslate"><span class="pre">Bottleneck.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck.training"><code class="docutils literal notranslate"><span class="pre">Bottleneck.training</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.Bottleneck.zero_init_last_bn"><code class="docutils literal notranslate"><span class="pre">Bottleneck.zero_init_last_bn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#downsample-conv">downsample_conv</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.downsample_conv"><code class="docutils literal notranslate"><span class="pre">downsample_conv()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#downsample-avg">downsample_avg</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.downsample_avg"><code class="docutils literal notranslate"><span class="pre">downsample_avg()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#drop-blocks">drop_blocks</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.drop_blocks"><code class="docutils literal notranslate"><span class="pre">drop_blocks()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#make-blocks">make_blocks</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.make_blocks"><code class="docutils literal notranslate"><span class="pre">make_blocks()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#resnet">ResNet</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet"><code class="docutils literal notranslate"><span class="pre">ResNet</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.__init__"><code class="docutils literal notranslate"><span class="pre">ResNet.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._buffers"><code class="docutils literal notranslate"><span class="pre">ResNet._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ResNet._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResNet._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ResNet._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ResNet._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._modules"><code class="docutils literal notranslate"><span class="pre">ResNet._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ResNet._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._parameters"><code class="docutils literal notranslate"><span class="pre">ResNet._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ResNet._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.forward"><code class="docutils literal notranslate"><span class="pre">ResNet.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.forward_features"><code class="docutils literal notranslate"><span class="pre">ResNet.forward_features()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.get_classifier"><code class="docutils literal notranslate"><span class="pre">ResNet.get_classifier()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.init_weights"><code class="docutils literal notranslate"><span class="pre">ResNet.init_weights()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.reset_classifier"><code class="docutils literal notranslate"><span class="pre">ResNet.reset_classifier()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.ResNet.training"><code class="docutils literal notranslate"><span class="pre">ResNet.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#resnet18">resnet18</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.resnet.resnet18"><code class="docutils literal notranslate"><span class="pre">resnet18()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-rnn">network.rnn</a><ul>
<li><a class="reference internal" href="#is-sequence">is_sequence</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.is_sequence"><code class="docutils literal notranslate"><span class="pre">is_sequence()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#sequence-mask">sequence_mask</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.sequence_mask"><code class="docutils literal notranslate"><span class="pre">sequence_mask()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#lstmforwardwrapper">LSTMForwardWrapper</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper"><code class="docutils literal notranslate"><span class="pre">LSTMForwardWrapper</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._after_forward"><code class="docutils literal notranslate"><span class="pre">LSTMForwardWrapper._after_forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTMForwardWrapper._before_forward"><code class="docutils literal notranslate"><span class="pre">LSTMForwardWrapper._before_forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#lstm">LSTM</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM"><code class="docutils literal notranslate"><span class="pre">LSTM</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM.__init__"><code class="docutils literal notranslate"><span class="pre">LSTM.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._backward_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._buffers"><code class="docutils literal notranslate"><span class="pre">LSTM._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">LSTM._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LSTM._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">LSTM._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._init"><code class="docutils literal notranslate"><span class="pre">LSTM._init()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">LSTM._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._modules"><code class="docutils literal notranslate"><span class="pre">LSTM._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">LSTM._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._parameters"><code class="docutils literal notranslate"><span class="pre">LSTM._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">LSTM._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM.forward"><code class="docutils literal notranslate"><span class="pre">LSTM.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.LSTM.training"><code class="docutils literal notranslate"><span class="pre">LSTM.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pytorchlstm">PytorchLSTM</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.batch_first"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.batch_first</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.bias"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.bias</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.bidirectional"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.bidirectional</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.dropout"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.dropout</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.forward"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.hidden_size"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.hidden_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.input_size"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.input_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.mode"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.mode</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.num_layers"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.num_layers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.PytorchLSTM.proj_size"><code class="docutils literal notranslate"><span class="pre">PytorchLSTM.proj_size</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#gru">GRU</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU"><code class="docutils literal notranslate"><span class="pre">GRU</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.__init__"><code class="docutils literal notranslate"><span class="pre">GRU.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.bias"><code class="docutils literal notranslate"><span class="pre">GRU.bias</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.forward"><code class="docutils literal notranslate"><span class="pre">GRU.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.hidden_size"><code class="docutils literal notranslate"><span class="pre">GRU.hidden_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.input_size"><code class="docutils literal notranslate"><span class="pre">GRU.input_size</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.weight_hh"><code class="docutils literal notranslate"><span class="pre">GRU.weight_hh</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.GRU.weight_ih"><code class="docutils literal notranslate"><span class="pre">GRU.weight_ih</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#get-lstm">get_lstm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.rnn.get_lstm"><code class="docutils literal notranslate"><span class="pre">get_lstm()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-scatter-connection">network.scatter_connection</a><ul>
<li><a class="reference internal" href="#shape-fn-scatter-connection">shape_fn_scatter_connection</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.shape_fn_scatter_connection"><code class="docutils literal notranslate"><span class="pre">shape_fn_scatter_connection()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#scatterconnection">ScatterConnection</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection"><code class="docutils literal notranslate"><span class="pre">ScatterConnection</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.__init__"><code class="docutils literal notranslate"><span class="pre">ScatterConnection.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._buffers"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._modules"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._parameters"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScatterConnection._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.forward"><code class="docutils literal notranslate"><span class="pre">ScatterConnection.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.training"><code class="docutils literal notranslate"><span class="pre">ScatterConnection.training</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.scatter_connection.ScatterConnection.xy_forward"><code class="docutils literal notranslate"><span class="pre">ScatterConnection.xy_forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-soft-argmax">network.soft_argmax</a><ul>
<li><a class="reference internal" href="#softargmax">SoftArgmax</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax"><code class="docutils literal notranslate"><span class="pre">SoftArgmax</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.__init__"><code class="docutils literal notranslate"><span class="pre">SoftArgmax.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._backward_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._buffers"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._modules"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._parameters"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">SoftArgmax._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.forward"><code class="docutils literal notranslate"><span class="pre">SoftArgmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.soft_argmax.SoftArgmax.training"><code class="docutils literal notranslate"><span class="pre">SoftArgmax.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#network-transformer">network.transformer</a><ul>
<li><a class="reference internal" href="#attention">Attention</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention"><code class="docutils literal notranslate"><span class="pre">Attention</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention.__init__"><code class="docutils literal notranslate"><span class="pre">Attention.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._buffers"><code class="docutils literal notranslate"><span class="pre">Attention._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Attention._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Attention._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Attention._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Attention._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._modules"><code class="docutils literal notranslate"><span class="pre">Attention._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Attention._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._parameters"><code class="docutils literal notranslate"><span class="pre">Attention._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Attention._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention.forward"><code class="docutils literal notranslate"><span class="pre">Attention.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention.split"><code class="docutils literal notranslate"><span class="pre">Attention.split()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Attention.training"><code class="docutils literal notranslate"><span class="pre">Attention.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#transformerlayer">TransformerLayer</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer"><code class="docutils literal notranslate"><span class="pre">TransformerLayer</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer.__init__"><code class="docutils literal notranslate"><span class="pre">TransformerLayer.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._backward_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._buffers"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._modules"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._parameters"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TransformerLayer._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer.forward"><code class="docutils literal notranslate"><span class="pre">TransformerLayer.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.TransformerLayer.training"><code class="docutils literal notranslate"><span class="pre">TransformerLayer.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#transformer">Transformer</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer"><code class="docutils literal notranslate"><span class="pre">Transformer</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer.__init__"><code class="docutils literal notranslate"><span class="pre">Transformer.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._backward_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._buffers"><code class="docutils literal notranslate"><span class="pre">Transformer._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">Transformer._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Transformer._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">Transformer._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Transformer._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._modules"><code class="docutils literal notranslate"><span class="pre">Transformer._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">Transformer._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._parameters"><code class="docutils literal notranslate"><span class="pre">Transformer._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Transformer._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer.forward"><code class="docutils literal notranslate"><span class="pre">Transformer.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.Transformer.training"><code class="docutils literal notranslate"><span class="pre">Transformer.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#scaleddotproductattention">ScaledDotProductAttention</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.__init__"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._buffers"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._modules"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._parameters"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.forward"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.network.transformer.ScaledDotProductAttention.training"><code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#backend-helper">backend_helper</a><ul>
<li><a class="reference internal" href="#enable-tf32">enable_tf32</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.backend_helper.enable_tf32"><code class="docutils literal notranslate"><span class="pre">enable_tf32()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#checkpoint-helper">checkpoint_helper</a><ul>
<li><a class="reference internal" href="#build-checkpoint-helper">build_checkpoint_helper</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.build_checkpoint_helper"><code class="docutils literal notranslate"><span class="pre">build_checkpoint_helper()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#checkpointhelper">CheckpointHelper</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.__init__"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._add_prefix"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper._add_prefix()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._load_matched_model_state_dict"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper._load_matched_model_state_dict()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper._remove_prefix"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper._remove_prefix()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.load"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper.load()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CheckpointHelper.save"><code class="docutils literal notranslate"><span class="pre">CheckpointHelper.save()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#countvar">CountVar</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar"><code class="docutils literal notranslate"><span class="pre">CountVar</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar.__init__"><code class="docutils literal notranslate"><span class="pre">CountVar.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar.add"><code class="docutils literal notranslate"><span class="pre">CountVar.add()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar.update"><code class="docutils literal notranslate"><span class="pre">CountVar.update()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.CountVar.val"><code class="docutils literal notranslate"><span class="pre">CountVar.val</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#auto-checkpoint">auto_checkpoint</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.checkpoint_helper.auto_checkpoint"><code class="docutils literal notranslate"><span class="pre">auto_checkpoint()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#data-helper">data_helper</a><ul>
<li><a class="reference internal" href="#to-device">to_device</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_device"><code class="docutils literal notranslate"><span class="pre">to_device()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#to-dtype">to_dtype</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_dtype"><code class="docutils literal notranslate"><span class="pre">to_dtype()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#to-tensor">to_tensor</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_tensor"><code class="docutils literal notranslate"><span class="pre">to_tensor()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#to-ndarray">to_ndarray</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_ndarray"><code class="docutils literal notranslate"><span class="pre">to_ndarray()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#to-list">to_list</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_list"><code class="docutils literal notranslate"><span class="pre">to_list()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#tensor-to-list">tensor_to_list</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.tensor_to_list"><code class="docutils literal notranslate"><span class="pre">tensor_to_list()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#to-item">to_item</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.to_item"><code class="docutils literal notranslate"><span class="pre">to_item()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#same-shape">same_shape</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.same_shape"><code class="docutils literal notranslate"><span class="pre">same_shape()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#logdict">LogDict</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.LogDict"><code class="docutils literal notranslate"><span class="pre">LogDict</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.LogDict._transform"><code class="docutils literal notranslate"><span class="pre">LogDict._transform()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.LogDict.update"><code class="docutils literal notranslate"><span class="pre">LogDict.update()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#build-log-buffer">build_log_buffer</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.build_log_buffer"><code class="docutils literal notranslate"><span class="pre">build_log_buffer()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#cudafetcher">CudaFetcher</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.CudaFetcher"><code class="docutils literal notranslate"><span class="pre">CudaFetcher</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.CudaFetcher.__init__"><code class="docutils literal notranslate"><span class="pre">CudaFetcher.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.CudaFetcher._producer"><code class="docutils literal notranslate"><span class="pre">CudaFetcher._producer()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.CudaFetcher.close"><code class="docutils literal notranslate"><span class="pre">CudaFetcher.close()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.CudaFetcher.run"><code class="docutils literal notranslate"><span class="pre">CudaFetcher.run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#get-tensor-data">get_tensor_data</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.get_tensor_data"><code class="docutils literal notranslate"><span class="pre">get_tensor_data()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#unsqueeze">unsqueeze</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.unsqueeze"><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#squeeze">squeeze</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.squeeze"><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-null-data">get_null_data</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.get_null_data"><code class="docutils literal notranslate"><span class="pre">get_null_data()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#zeros-like">zeros_like</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.data_helper.zeros_like"><code class="docutils literal notranslate"><span class="pre">zeros_like()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#dataparallel">dataparallel</a><ul>
<li><a class="reference internal" href="#id1">DataParallel</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel"><code class="docutils literal notranslate"><span class="pre">DataParallel</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel.__init__"><code class="docutils literal notranslate"><span class="pre">DataParallel.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._backward_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._buffers"><code class="docutils literal notranslate"><span class="pre">DataParallel._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">DataParallel._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DataParallel._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">DataParallel._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">DataParallel._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._modules"><code class="docutils literal notranslate"><span class="pre">DataParallel._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">DataParallel._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._parameters"><code class="docutils literal notranslate"><span class="pre">DataParallel._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">DataParallel._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel.parameters"><code class="docutils literal notranslate"><span class="pre">DataParallel.parameters()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.dataparallel.DataParallel.training"><code class="docutils literal notranslate"><span class="pre">DataParallel.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#distribution">distribution</a><ul>
<li><a class="reference internal" href="#pd">Pd</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd"><code class="docutils literal notranslate"><span class="pre">Pd</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd.entropy"><code class="docutils literal notranslate"><span class="pre">Pd.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd.mode"><code class="docutils literal notranslate"><span class="pre">Pd.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd.neglogp"><code class="docutils literal notranslate"><span class="pre">Pd.neglogp()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd.noise_mode"><code class="docutils literal notranslate"><span class="pre">Pd.noise_mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.Pd.sample"><code class="docutils literal notranslate"><span class="pre">Pd.sample()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#categoricalpd">CategoricalPd</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd"><code class="docutils literal notranslate"><span class="pre">CategoricalPd</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.__init__"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.entropy"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.mode"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.neglogp"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.neglogp()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.noise_mode"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.noise_mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.sample"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.sample()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPd.update_logits"><code class="docutils literal notranslate"><span class="pre">CategoricalPd.update_logits()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#categoricalpdpytorch">CategoricalPdPytorch</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.__init__"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.entropy"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.entropy()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.mode"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.mode()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.neglogp"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.neglogp()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.sample"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.sample()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.update_logits"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.update_logits()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.distribution.CategoricalPdPytorch.update_probs"><code class="docutils literal notranslate"><span class="pre">CategoricalPdPytorch.update_probs()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#lr-scheduler">lr_scheduler</a><ul>
<li><a class="reference internal" href="#get-lr-ratio">get_lr_ratio</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.lr_scheduler.get_lr_ratio"><code class="docutils literal notranslate"><span class="pre">get_lr_ratio()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#cos-lr-scheduler">cos_lr_scheduler</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.lr_scheduler.cos_lr_scheduler"><code class="docutils literal notranslate"><span class="pre">cos_lr_scheduler()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#math-helper">math_helper</a><ul>
<li><a class="reference internal" href="#cov">cov</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.math_helper.cov"><code class="docutils literal notranslate"><span class="pre">cov()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#metric">metric</a><ul>
<li><a class="reference internal" href="#levenshtein-distance">levenshtein_distance</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.metric.levenshtein_distance"><code class="docutils literal notranslate"><span class="pre">levenshtein_distance()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#hamming-distance">hamming_distance</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.metric.hamming_distance"><code class="docutils literal notranslate"><span class="pre">hamming_distance()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#model-helper">model_helper</a><ul>
<li><a class="reference internal" href="#get-num-params">get_num_params</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.model_helper.get_num_params"><code class="docutils literal notranslate"><span class="pre">get_num_params()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#nn-test-helper">nn_test_helper</a><ul>
<li><a class="reference internal" href="#is-differentiable">is_differentiable</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.nn_test_helper.is_differentiable"><code class="docutils literal notranslate"><span class="pre">is_differentiable()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#optimizer-helper">optimizer_helper</a><ul>
<li><a class="reference internal" href="#calculate-grad-norm">calculate_grad_norm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.calculate_grad_norm"><code class="docutils literal notranslate"><span class="pre">calculate_grad_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#calculate-grad-norm-without-bias-two-norm">calculate_grad_norm_without_bias_two_norm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.calculate_grad_norm_without_bias_two_norm"><code class="docutils literal notranslate"><span class="pre">calculate_grad_norm_without_bias_two_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#grad-ignore-norm">grad_ignore_norm</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.grad_ignore_norm"><code class="docutils literal notranslate"><span class="pre">grad_ignore_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#grad-ignore-value">grad_ignore_value</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.grad_ignore_value"><code class="docutils literal notranslate"><span class="pre">grad_ignore_value()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#adam">Adam</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam"><code class="docutils literal notranslate"><span class="pre">Adam</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam.__init__"><code class="docutils literal notranslate"><span class="pre">Adam.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_step_post_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_step_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._optimizer_step_pre_hooks"><code class="docutils literal notranslate"><span class="pre">Adam._optimizer_step_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam._state_init"><code class="docutils literal notranslate"><span class="pre">Adam._state_init()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam.get_grad"><code class="docutils literal notranslate"><span class="pre">Adam.get_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.Adam.step"><code class="docutils literal notranslate"><span class="pre">Adam.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#rmsprop">RMSprop</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop"><code class="docutils literal notranslate"><span class="pre">RMSprop</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop.__init__"><code class="docutils literal notranslate"><span class="pre">RMSprop.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_post_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_step_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._optimizer_step_pre_hooks"><code class="docutils literal notranslate"><span class="pre">RMSprop._optimizer_step_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop._state_init"><code class="docutils literal notranslate"><span class="pre">RMSprop._state_init()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop.get_grad"><code class="docutils literal notranslate"><span class="pre">RMSprop.get_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.RMSprop.step"><code class="docutils literal notranslate"><span class="pre">RMSprop.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#pcgrad">PCGrad</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad"><code class="docutils literal notranslate"><span class="pre">PCGrad</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad.__init__"><code class="docutils literal notranslate"><span class="pre">PCGrad.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._flatten_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad._flatten_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._pack_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad._pack_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._project_conflicting"><code class="docutils literal notranslate"><span class="pre">PCGrad._project_conflicting()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._retrieve_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad._retrieve_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._set_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad._set_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad._unflatten_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad._unflatten_grad()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad.optimizer"><code class="docutils literal notranslate"><span class="pre">PCGrad.optimizer</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad.pc_backward"><code class="docutils literal notranslate"><span class="pre">PCGrad.pc_backward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad.step"><code class="docutils literal notranslate"><span class="pre">PCGrad.step()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.PCGrad.zero_grad"><code class="docutils literal notranslate"><span class="pre">PCGrad.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#configure-weight-decay">configure_weight_decay</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.optimizer_helper.configure_weight_decay"><code class="docutils literal notranslate"><span class="pre">configure_weight_decay()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#parameter">parameter</a><ul>
<li><a class="reference internal" href="#nonegativeparameter">NonegativeParameter</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter.__init__"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._backward_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._buffers"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._modules"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._parameters"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter.forward"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter.set_data"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter.set_data()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.NonegativeParameter.training"><code class="docutils literal notranslate"><span class="pre">NonegativeParameter.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tanhparameter">TanhParameter</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter"><code class="docutils literal notranslate"><span class="pre">TanhParameter</span></code></a><ul>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter.__init__"><code class="docutils literal notranslate"><span class="pre">TanhParameter.__init__()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._backward_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._backward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._backward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._backward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._buffers"><code class="docutils literal notranslate"><span class="pre">TanhParameter._buffers</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._forward_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks_always_called"><code class="docutils literal notranslate"><span class="pre">TanhParameter._forward_hooks_always_called</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._forward_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TanhParameter._forward_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._forward_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._forward_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._forward_pre_hooks_with_kwargs"><code class="docutils literal notranslate"><span class="pre">TanhParameter._forward_pre_hooks_with_kwargs</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._is_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">TanhParameter._is_full_backward_hook</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._load_state_dict_post_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._load_state_dict_post_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._load_state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._load_state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._modules"><code class="docutils literal notranslate"><span class="pre">TanhParameter._modules</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._non_persistent_buffers_set"><code class="docutils literal notranslate"><span class="pre">TanhParameter._non_persistent_buffers_set</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._parameters"><code class="docutils literal notranslate"><span class="pre">TanhParameter._parameters</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._state_dict_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._state_dict_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter._state_dict_pre_hooks"><code class="docutils literal notranslate"><span class="pre">TanhParameter._state_dict_pre_hooks</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter.forward"><code class="docutils literal notranslate"><span class="pre">TanhParameter.forward()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter.set_data"><code class="docutils literal notranslate"><span class="pre">TanhParameter.set_data()</span></code></a></li>
<li><a class="reference internal" href="#ding.torch_utils.parameter.TanhParameter.training"><code class="docutils literal notranslate"><span class="pre">TanhParameter.training</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#reshape-helper">reshape_helper</a><ul>
<li><a class="reference internal" href="#fold-batch">fold_batch</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.reshape_helper.fold_batch"><code class="docutils literal notranslate"><span class="pre">fold_batch()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#unfold-batch">unfold_batch</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.reshape_helper.unfold_batch"><code class="docutils literal notranslate"><span class="pre">unfold_batch()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#unsqueeze-repeat">unsqueeze_repeat</a><ul>
<li><a class="reference internal" href="#ding.torch_utils.reshape_helper.unsqueeze_repeat"><code class="docutils literal notranslate"><span class="pre">unsqueeze_repeat()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>