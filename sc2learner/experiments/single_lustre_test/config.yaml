common:
    config_name: ppo_train
    time_wrapper_type: cuda
env:
    game_version: '4.10'
    disable_fog: False
    step_mul: 32
    bot_difficulties: '1,2,4,6,9,A'
    use_all_combat_actions: False
    game_steps_per_episode: 43200
    use_region_features: False
    use_action_mask: True
    use_reward_shaping: False
model:
    policy: mlp  # [mlp, lstm]
train:
    batch_size: 256  # 32 * unroll_split
    learner_use_cuda: True
    learner_data_queue_size: 8192  # 1024 * unroll_split
    actor_data_queue_size: 1
    enable_save_data: False
    unroll_length: 128
    unroll_split: 8
    learning_rate: 1e-5
    discount_gamma: 0.998
    lambda_return: 0.95
    entropy_coeff: 0.01
    value_coeff: 0.5
    ppo_clip_range: 0.1
    use_value_clip: False
    grad_clip_type: max_norm  # [max_norm, clip_value]
    grad_clip_value: 0.5
communication:
    ip:
        learner: '10.198.6.36'
        learner_manager: '10.198.6.31'
        actor_manager: '10.198.6.31'  # if single lustre, set learner_manager == actor_manager
    port:
        learner: 5710
        learner_manager: 5711
        actor_manager: 5712
        actor: 5713

        learner_manager_model: 5716
        actor_manager_model: 5717
        actor_model: 5718
    HWM:
        learner: 1
        learner_manager: 10
        actor_manager: 10
        actor: 1
    queue_size:
        learner_manager: 20
        actor_manager: 50
    model_time_interval: 10  # 10 second 128 step rollout
logger:
    print_freq: 500
    save_freq: 5000
