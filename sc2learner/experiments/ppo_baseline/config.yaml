common:
    config_name: ppo_train
    time_wrapper_type: cuda
env:
    game_version: '4.10'
    disable_fog: False
    step_mul: 32
    bot_difficulties: '1,2,4,6,9,A'
    use_all_combat_actions: False
    game_steps_per_episode: 43200
    use_region_features: False
    use_action_mask: True
    use_reward_shaping: False
model:
    policy: mlp  # [mlp, lstm]
train:
    batch_size: 256  # 32 * unroll_split
    learner_use_cuda: True
    learner_data_queue_size: 8192  # 1024 * unroll_split
    learner_episode_queue_size: 1024  # episode = data_queue / unroll_split 
    actor_data_queue_size: 1
    enable_save_data: False
    unroll_length: 128
    unroll_split: 8
    learning_rate: 1e-5
    discount_gamma: 0.998
    lambda_return: 0.95
    entropy_coeff: 0.01
    value_coeff: 0.5
    ppo_clip_range: 0.1
    use_value_clip: False
    grad_clip_type: max_norm  # [max_norm, clip_value]
    grad_clip_value: 0.5
communication:
    learner_ip: '10.198.6.62'
    port:
        learner: 5710
        actor: 5711
logger:
    print_freq: 500
    save_freq: 1000
