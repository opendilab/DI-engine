common:
    name: AlphastarRL
    time_wrapper_type: cuda
    save_path: '.'
    load_path: ''
    only_evaluate: False
api:
    learner_port: 18193               # learner port
    coordinator_ip: 10.5.36.31       # learner and coordinator are in same cluster
    coordinator_port: 18194
    manager_ip: 10.5.36.31           # could be all cluster
    manager_port: 18195
    ceph_path: s3://zhangming/
    use_ceph: False
    manager:
        auto_run_actor: False
        use_partitions: ['VI_Face_1080TI', 'VI_SP_Y_V100_B']
        actor_num: [2, 2]
    coordinator:
        use_fake_data: True
        fake_model_path: iterations_2200.pth.tar
        fake_stat_path: Zerg_Zerg_6280_d35c22b2d7e462f1481621cbf765709961e3f9a2a99f8f6c6fa814ccffc831d6.stat_processed
replay_buffer:
    meta_maxlen: 4096
    max_reuse: 2
    min_sample_ratio: 2
    alpha: 0.6
    beta: 0.5
    cache_maxlen: 16
    timeout: 8  # times of the seconds of per learning iteration
data:
    train:
        batch_size: 2
        dataloader_type: 'online'
train:
    use_cuda: False
    use_distributed: False
    max_iterations: 10
    batch_size: 2
    trajectory_len: 3
model:
    use_value_network: True
logger:
    print_freq: 1
    save_freq: 200
    eval_freq: 200
    var_record_type: 'alphastar'
