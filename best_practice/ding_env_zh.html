

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>如何将自己的环境迁移到DI-engine中 &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="N-step TD" href="nstep_td.html" />
    <link rel="prev" title="最佳实践" href="index_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index_zh.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">使用者指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index_zh.html">安装说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index_zh.html">快速上手</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index_zh.html">强化学习算法攻略合集</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index_zh.html">强化学习环境示例手册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index_zh.html">分布式</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">最佳实践</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">如何将自己的环境迁移到DI-engine中</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">基础</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">进阶</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dingenvwrapper">DingEnvWrapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="#q-a">Q &amp; A</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nstep_td.html">N-step TD</a></li>
<li class="toctree-l2"><a class="reference internal" href="priority.html">How to Use PER(Prioritized Experience Replay)</a></li>
<li class="toctree-l2"><a class="reference internal" href="IL.html">Imitation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="IRL.html">Inverse RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn.html">How to use RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="random_seed.html">Random seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_discrete.html">Multi-Discrete Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_gpu_example.html">How to Use Multi-GPUs to Train Your Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="random_collect_size.html">How to randomly collect some data sample at the beginning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_generated_folders.html">How to understand training generated folders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="learner_log.html">Learner log</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_wrapper.html">How to Customize Model Wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="env_wrapper.html">How to Customize an Env Wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="episode_buffer.html">How to use Episode Replay Buffer?</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_buffer.html">How to use multiple buffers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="maac.html">Multi-Agent Actor-Critic RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="customization1_dynamic_update_step.html">Customization 1: Dynamic Update Step</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_zh.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index_zh.html">特性介绍</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index_zh.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index_zh.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index_zh.html">Docs</a> &raquo;</li>
        
          <li><a href="index_zh.html">最佳实践</a> &raquo;</li>
        
      <li>如何将自己的环境迁移到DI-engine中</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/best_practice/ding_env_zh.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="di-engine">
<h1>如何将自己的环境迁移到DI-engine中<a class="headerlink" href="#di-engine" title="Permalink to this headline">¶</a></h1>
<p>虽然已经在 <code class="docutils literal notranslate"><span class="pre">DI-zoo</span></code> 中提供了大量的强化学习常用环境（ <a class="reference external" href="https://github.com/opendilab/DI-engine#environment-versatility">已支持的环境</a> ），但用户还是会需要将自己的环境迁移到 <code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> 中。因此在本节中，将会介绍如何一步步进行上述迁移，以满足 <code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> 的基础环境基类 <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> 的规范，从而轻松应用在训练的 pipeline 中。</p>
<p>下面的介绍将分为 <strong>基础</strong> 和 <strong>进阶</strong> 两部分。 <strong>基础</strong> 表明如果想跑通 pipeline 必须实现的功能和注意的细节； <strong>进阶</strong> 则表示一些拓展的功能。</p>
<div class="section" id="id2">
<h2>基础<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>本节将介绍迁移环境时，用户必须满足的规范约束、以及必须实现的功能。</p>
<p>如果要在 DI-engine 中使用环境，需要实现一个继承自 <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> 的子类环境，例如 <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code> 。 <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code> 和你自己的环境之间是 <a class="reference external" href="https://www.cnblogs.com/chinxi/p/7349768.html">组合</a> 关系，即在一个 <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code> 实例中，会持有一个你自己的环境的实例。</p>
<p>强化学习的环境有一些普遍地、被大多数环境实现了的主要接口，如 <code class="docutils literal notranslate"><span class="pre">reset()</span></code>, <code class="docutils literal notranslate"><span class="pre">step()</span></code>, <code class="docutils literal notranslate"><span class="pre">seed()</span></code> 等。在 DI-engine 中， <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> 将对这些接口进行进一步的封装，下面大部分情况下将以 Atari 为例进行说明。具体代码可以参考 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/envs/atari_env.py">Atari Env</a> 和 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/envs/atari_wrappers.py">Atari Env Wrapper</a></p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code></p>
<p>一般情况下，可能会在 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 方法中将环境实例化，但是在 DI-engine 中，为了便于支持 <code class="docutils literal notranslate"><span class="pre">EnvManager</span></code> 中的“环境向量化”等并行操作，环境实例一般采用 <strong>Lazy Init</strong> 的方式，即 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 方法不初始化真正的原始环境实例，只是设置相关 <strong>参数配置值</strong> ，在第一次调用 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法时，才会进行实际的环境初始化。</p>
<p>以 Atari 为例。 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 并不实例化环境，只是设置配置项 <code class="docutils literal notranslate"><span class="pre">self._cfg</span></code> ，以及初始化变量 <code class="docutils literal notranslate"><span class="pre">self._init_flag</span></code> 用于记录是否是第一次调用 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法（即环境是否还没有被初始化）。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">seed()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">seed</span></code> 用于设定环境中的随机种子，环境中有两部分随机种子需要设置，一是 <strong>原始环境</strong> 的随机种子，二是各种 <strong>环境变换</strong> 中调用随机库时的随机种子（例如 <code class="docutils literal notranslate"><span class="pre">random</span></code>， <code class="docutils literal notranslate"><span class="pre">np.random</span></code>）。如果你的环境完全没有任何随机性（包括“原始环境”与“环境变换”），那么也可以不实现这个方法。</p>
<p>随机库的种子的设置较为简单，直接在环境的 <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法中进行设置。</p>
<p>但原始环境的种子，在 <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法中只是进行了赋值，并没有真的设置；真正的设置是在调用环境的 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法内部，具体的原始环境 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 之前进行设置。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dynamic_seed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">seed</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_seed</span> <span class="o">=</span> <span class="n">dynamic_seed</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span><span class="p">)</span>
</pre></div>
</div>
<p>针对原始环境的种子，DI-engine 中有 <strong>静态种子</strong> 和 <strong>动态种子</strong> 的概念。</p>
<p><strong>静态种子</strong> 用于测试环境，保证每个 episode 的随机种子相同，即 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 时只会采用 <code class="docutils literal notranslate"><span class="pre">self._seed</span></code> 这个固定的静态种子数值。需要在 <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法中手动传入 <code class="docutils literal notranslate"><span class="pre">dynamic_seed</span></code> 参数为 <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p>
<p><strong>动态种子</strong> 用于训练环境，尽量使得每个 episode 的随机种子都不相同，它们都在 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法中由一个随机数发生器 <code class="docutils literal notranslate"><span class="pre">100</span> <span class="pre">*</span> <span class="pre">np.random.randint(1,</span> <span class="pre">1000)</span></code> 产生（但这个随机数发生器的种子是通过环境的 <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法固定的，因此能保证实验的可复现性）。需要在 <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法中不传入 <code class="docutils literal notranslate"><span class="pre">dynamic_seed</span></code> 参数，或者传入参数为 <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">reset()</span></code></p>
<p>在 <code class="docutils literal notranslate"><span class="pre">__init__</span></code> 方法中已经介绍了 DI-engine 的 <strong>Lazy Init</strong> 初始化方式，即实际的环境初始化是在 <strong>第一次调用</strong> <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法时进行的。</p>
<p><code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法中会根据 <code class="docutils literal notranslate"><span class="pre">self._init_flag</span></code> 判断是否需要实例化实际环境，并进行随机种子的设置，然后调用原始环境的 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法得到初始状态下的观测值，并转换为 <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> 数据格式（将在 5. 中详细讲解），并初始化 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 的值（将在 4. 中详细讲解），在 Atari 中 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 指的是一整个 episode 所获得的真实 reward 的累积和，用于评价 agent 在该环境上的性能，不用于训练。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">False</span>

   <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_env</span><span class="p">(</span><span class="n">only_info</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_seed&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dynamic_seed&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_seed</span><span class="p">:</span>
         <span class="n">np_seed</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">+</span> <span class="n">np_seed</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_seed&#39;</span><span class="p">):</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span><span class="p">)</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="k">return</span> <span class="n">obs</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">step()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">step</span></code> 方法负责接收当前时刻的 <code class="docutils literal notranslate"><span class="pre">action</span></code> ，然后给出当前时刻的 <code class="docutils literal notranslate"><span class="pre">reward</span></code> 和 下一时刻的 <code class="docutils literal notranslate"><span class="pre">obs</span></code>，在 DI-engine中，还需要给出：当前episode是否结束的标志 <code class="docutils literal notranslate"><span class="pre">done</span></code>、字典形式的其它信息 <code class="docutils literal notranslate"><span class="pre">info</span></code> （比如 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> ）。</p>
<p>在得到 <code class="docutils literal notranslate"><span class="pre">reward</span></code> <code class="docutils literal notranslate"><span class="pre">obs</span></code> <code class="docutils literal notranslate"><span class="pre">done</span></code> <code class="docutils literal notranslate"><span class="pre">info</span></code> 等数据后，需要进行处理，转化为 <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> 格式，以保证符合 DI-engine 的规范。在每一个时间步中 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 都会累加当前的真实 reward，并在 episode 结束（ <code class="docutils literal notranslate"><span class="pre">done</span> <span class="pre">==</span> <span class="pre">True</span></code> ）的时候返回该累加值。</p>
<p>最终，将上述四个数据放入定义为 <code class="docutils literal notranslate"><span class="pre">namedtuple</span></code> 的 <code class="docutils literal notranslate"><span class="pre">BaseEnvTimestep</span></code> 中并返回（定义为： <code class="docutils literal notranslate"><span class="pre">BaseEnvTimestep</span> <span class="pre">=</span> <span class="pre">namedtuple('BaseEnvTimestep',</span> <span class="pre">['obs',</span> <span class="pre">'reward',</span> <span class="pre">'done',</span> <span class="pre">'info'])</span></code> ）</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.envs</span> <span class="kn">import</span> <span class="n">BaseEnvTimestep</span>

<span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseEnvTimestep</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span> <span class="o">+=</span> <span class="n">rew</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
      <span class="n">rew</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">([</span><span class="n">rew</span><span class="p">])</span>  <span class="c1"># Transformed to an array with shape (1,)</span>
      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
         <span class="n">info</span><span class="p">[</span><span class="s1">&#39;final_eval_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span>
      <span class="k">return</span> <span class="n">BaseEnvTimestep</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code></p>
<p>在 Atari 环境中， <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 是指一个 episode 的全部 reward 的累加和。</p>
<blockquote>
<div><ul class="simple">
<li><p>在 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法中，将当前 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 置0；</p></li>
<li><p>在 <code class="docutils literal notranslate"><span class="pre">step</span></code> 方法中，将每个时间步获得的 reward 加到 <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> 中。</p></li>
<li><p>在 <code class="docutils literal notranslate"><span class="pre">step</span></code> 方法中，如果当前 episode 已经结束（ <code class="docutils literal notranslate"><span class="pre">done</span> <span class="pre">==</span> <span class="pre">True</span></code> 此处要求 <code class="docutils literal notranslate"><span class="pre">done</span></code> 必须是 <code class="docutils literal notranslate"><span class="pre">bool</span></code> 类型，不能是 <code class="docutils literal notranslate"><span class="pre">np.bool</span></code> ），那么就添加到 <code class="docutils literal notranslate"><span class="pre">info</span></code> 这个字典中并返回： <code class="docutils literal notranslate"><span class="pre">info['final_eval_reward']</span> <span class="pre">=</span> <span class="pre">self._final_eval_reward</span></code></p></li>
</ul>
</div></blockquote>
<p>但是，在其他的环境中，可能需要的不是一个 episode 的 reward 之和。例如，在 smac 中，需要当前 episode 的胜率，因此就需要修改第二步 <code class="docutils literal notranslate"><span class="pre">step</span></code> 方法中简单的累加，改为记录对局情况，并最终在 episode 结束时返回计算得到的胜率。</p>
</li>
<li><p>数据规格</p>
<p>DI-engine 中要求环境中每个方法的输入输出的数据必须为 <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> 格式，数据类型dtype 需要是 <code class="docutils literal notranslate"><span class="pre">np.int64</span></code> (整数) 或 <code class="docutils literal notranslate"><span class="pre">np.float32</span></code> (浮点数)。包括：</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法返回的 <code class="docutils literal notranslate"><span class="pre">obs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code> 方法接收的 <code class="docutils literal notranslate"><span class="pre">action</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code> 方法返回的 <code class="docutils literal notranslate"><span class="pre">obs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code> 方法返回的 <code class="docutils literal notranslate"><span class="pre">reward</span></code>，此处还要求 <code class="docutils literal notranslate"><span class="pre">reward</span></code> 必须为 <strong>一维</strong> ，而不能是零维，例如 Atari 中的代码 <code class="docutils literal notranslate"><span class="pre">rew</span> <span class="pre">=</span> <span class="pre">to_ndarray([rew])</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step</span></code> 方法返回的 <code class="docutils literal notranslate"><span class="pre">done</span></code>，必须是 <code class="docutils literal notranslate"><span class="pre">bool</span></code> 类型，不能是 <code class="docutils literal notranslate"><span class="pre">np.bool</span></code></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="id4">
<h2>进阶<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>环境预处理wrapper</p>
<p>很多环境如果要用于强化学习的训练中，都需要进行一些预处理，来达到增加随机性、数据归一化、易于训练等目的。这些预处理通过 wrapper 的形式实现（wrapper 的介绍可以参考 <a class="reference external" href="../feature/wrapper_hook_overview_zh.html#wrapper">这里</a> ）。</p>
<p>环境预处理的每个 wrapper 都是 <code class="docutils literal notranslate"><span class="pre">gym.Wrapper</span></code> 的一个子类。例如， <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code> 是在 episode 最开始时，执行随机数量的 No-Operation 动作，是增加随机性的一种手段，其使用方法是：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;PongNoFrameskip-v4&#39;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">NoopResetEnv</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
<p>由于 <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code> 中实现了 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法，因此在 <code class="docutils literal notranslate"><span class="pre">env.reset()</span></code> 时就会执行 <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code> 中的相应逻辑。</p>
<p>DI-engine 中已经实现了以下 env wrapper：(in <code class="docutils literal notranslate"><span class="pre">ding/envs/env_wrappers/env_wrappers.py</span></code>)</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code>: 在 episode 最开始时，执行随机数量的 No-Operation 动作</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MaxAndSkipEnv</span></code>: 返回几帧中的最大值，可认为是时间步上的一种 max pooling</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WarpFrame</span></code>: 将原始的图像画面利用 <code class="docutils literal notranslate"><span class="pre">cv2</span></code> 库的 <code class="docutils literal notranslate"><span class="pre">cvtColor</span></code> 转换颜色编码，并 resize 为一定长宽的图像（一般为 84x84）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ScaledFloatFrame</span></code>: 将 observation 归一化到 [0, 1] 区间内（保持 dtype 为 <code class="docutils literal notranslate"><span class="pre">np.float32</span></code> ）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ClipRewardEnv</span></code>: 将 reward 通过一个符号函数，变为 <code class="docutils literal notranslate"><span class="pre">{+1,</span> <span class="pre">0,</span> <span class="pre">-1}</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FrameStack</span></code>: 将一定数量（一般为4）的 frame 堆叠在一起，作为新的 observation，可被用于处理 POMDP 的情况，例如，单帧信息无法知道运动的速度方向</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ObsTransposeWrapper</span></code>: 将 <code class="docutils literal notranslate"><span class="pre">(H,</span> <span class="pre">W,</span> <span class="pre">C)</span></code> 的图像转换为 <code class="docutils literal notranslate"><span class="pre">(C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code> 的图像</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ObsNormEnv</span></code>: 利用 <code class="docutils literal notranslate"><span class="pre">RunningMeanStd</span></code> 将 observation 进行滑动窗口归一化</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RewardNormEnv</span></code>: 利用 <code class="docutils literal notranslate"><span class="pre">RunningMeanStd</span></code> 将 reward 进行滑动窗口归一化</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RamWrapper</span></code>: 将 Ram 类型的环境的 observation 的 shape 转换为类似图像的 (128, 1, 1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EpisodicLifeEnv</span></code>: 将内置多条生命的环境（例如Qbert），将每条生命看作一个 episode</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FireResetEnv</span></code>: 在环境 reset 后立即执行动作1（开火）</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GymHybridDictActionWrapper</span></code>: 将 Gym-Hybrid 环境原始的 <code class="docutils literal notranslate"><span class="pre">gym.spaces.Tuple</span></code> 类型的动作空间，转换为 <code class="docutils literal notranslate"><span class="pre">gym.spaces.Dict</span></code> 类型的动作空间.</p></li>
</ul>
</div></blockquote>
<p>如果上述 wrapper 不能满足你的需要，也可以自行定制 wrapper。</p>
<p>值得一提的是，每个 wrapper 不仅要完成对相应的 observation/action/reward 值的变化，还要对应地修改其 space （当且仅当 shpae, dtype 等被修改时），这个方法将在下一节中详细介绍。</p>
</li>
<li><p>三个空间属性 observation/action/reward space</p>
<p>如果希望可以根据环境的维度自动创建神经网络，或是在 <code class="docutils literal notranslate"><span class="pre">EnvManager</span></code> 中使用 <code class="docutils literal notranslate"><span class="pre">shared_memory</span></code> 技术加快环境返回的大型张量数据的传输速度，就需要让环境支持提供属性 <code class="docutils literal notranslate"><span class="pre">observation_space</span></code> <code class="docutils literal notranslate"><span class="pre">action_space</span></code> <code class="docutils literal notranslate"><span class="pre">reward_space</span></code>。</p>
<p>这里的 space 都是 <code class="docutils literal notranslate"><span class="pre">gym.spaces.Space</span></code> 的子类的实例，最常用的 <code class="docutils literal notranslate"><span class="pre">gym.spaces.Space</span></code> 包括 <code class="docutils literal notranslate"><span class="pre">Discrete</span></code> <code class="docutils literal notranslate"><span class="pre">Box</span></code> <code class="docutils literal notranslate"><span class="pre">Tuple</span></code> <code class="docutils literal notranslate"><span class="pre">Dict</span></code> 等。space 中需要给出 <strong>shape</strong> 和 <strong>dtype</strong> 。在 gym 原始环境中，大多都会支持 <code class="docutils literal notranslate"><span class="pre">observation_space</span></code> <code class="docutils literal notranslate"><span class="pre">action_space</span></code> 和 <code class="docutils literal notranslate"><span class="pre">reward_range</span></code>，在 DI-engine 中，将 <code class="docutils literal notranslate"><span class="pre">reward_range</span></code> 也扩充成了 <code class="docutils literal notranslate"><span class="pre">reward_space</span></code>，使这三者保持一致。</p>
<p>例如，这个是 cartpole 的三个属性：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CartpoleEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{})</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span>
            <span class="n">low</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.8</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="o">-</span><span class="mf">0.42</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)]),</span>
            <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.8</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="mf">0.42</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)]),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
      <span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reward_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">observation_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_space</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">action_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">reward_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_space</span>
</pre></div>
</div>
<p>由于 cartpole 没有使用任何 wrapper，因此其三个 space 是固定不变的。但如果像 Atari 这种经过了多重 wrapper 装饰的环境，就需要在每个 wrapper 对原始环境进行包装之后，修改其对应的 space。例如，Atari 会使用 <code class="docutils literal notranslate"><span class="pre">ScaledFloatFrameWrapper</span></code>，将 observation 归一化到 [0, 1] 区间内，那么相应地，就会修改其 <code class="docutils literal notranslate"><span class="pre">observation_space</span></code>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ScaledFloatFrameWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">ObservationWrapper</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
      <span class="c1"># ...</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_save_replay()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> 并没有强制要求实现 <code class="docutils literal notranslate"><span class="pre">render</span></code> 方法，如果想完成可视化，我们推荐实现 <code class="docutils literal notranslate"><span class="pre">enable_save_replay</span></code> 方法，对游戏视频进行保存。</p>
<p>该方法在 <code class="docutils literal notranslate"><span class="pre">reset</span></code> 方法之前， <code class="docutils literal notranslate"><span class="pre">seed</span></code> 方法之后被调用，在该方法中指定录像存储的路径。需要注意的是，该方法并 <strong>不直接存储录像</strong>，只是设置一个是否保存录像的 flag。真正存储录像的代码和逻辑需要自己实现。（由于可能会开启多个环境，每个环境运行多个 episode，因此我们建议在文件名中用 episode_id 和 env_id 进行区分）</p>
<p>此处，给出 DI-engine 中的一个例子，该例子利用 <code class="docutils literal notranslate"><span class="pre">gym</span></code> 提供的装饰器封装环境，如代码所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">enable_save_replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replay_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">replay_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">replay_path</span> <span class="o">=</span> <span class="s1">&#39;./video&#39;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_replay_path</span> <span class="o">=</span> <span class="n">replay_path</span>
      <span class="c1"># this function can lead to the meaningless result</span>
      <span class="c1"># disable_gym_view_window()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_path</span><span class="p">,</span> <span class="n">video_callable</span><span class="o">=</span><span class="k">lambda</span> <span class="n">episode_id</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="kc">True</span>
      <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>训练环境和测试环境使用不同 config</p>
<p>用于训练的环境（collector_env）和用于测试的环境（evaluator_env）可能使用不同的配置项，可以在环境中实现一个静态方法来实现对于不同环境配置项的自定义配置，以 Atari 为例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="nd">@staticmethod</span>
   <span class="k">def</span> <span class="nf">create_collector_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
      <span class="n">collector_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collector_env_num&#39;</span><span class="p">)</span>
      <span class="n">cfg</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
      <span class="n">cfg</span><span class="o">.</span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">cfg</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">collector_env_num</span><span class="p">)]</span>

   <span class="nd">@staticmethod</span>
   <span class="k">def</span> <span class="nf">create_evaluator_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
      <span class="n">evaluator_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;evaluator_env_num&#39;</span><span class="p">)</span>
      <span class="n">cfg</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
      <span class="n">cfg</span><span class="o">.</span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">cfg</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)]</span>
</pre></div>
</div>
<p>在实际使用时，可以对原始的配置项 <code class="docutils literal notranslate"><span class="pre">cfg</span></code> 进行转换：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># env_fn is an env class</span>
<span class="n">collector_env_cfg</span> <span class="o">=</span> <span class="n">env_fn</span><span class="o">.</span><span class="n">create_collector_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">evaluator_env_cfg</span> <span class="o">=</span> <span class="n">env_fn</span><span class="o">.</span><span class="n">create_evaluator_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
<p>设置 <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span></code> 项，将相应地在 wrapper 中使用不同的修饰方式。例如，若 <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span> <span class="pre">==</span> <span class="pre">True</span></code> ，则将对 reward 使用符号函数映射至 <code class="docutils literal notranslate"><span class="pre">{+1,</span> <span class="pre">0,</span> <span class="pre">-1}</span></code> 方便训练，若 <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span> <span class="pre">==</span> <span class="pre">False</span></code> 则将保留原 reward 值，方便测试时评估 agent 的性能。</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_action()</span></code></p>
<p>一些 off-policy 算法希望可以在训练开始之前，用随机策略收集一些数据填充 buffer，完成 buffer 的初始化。出于这样的需求，DI-engine 鼓励实现 <code class="docutils literal notranslate"><span class="pre">random_action</span></code> 方法。</p>
<p>由于环境已经实现了 <code class="docutils literal notranslate"><span class="pre">action_space</span></code>，所以可以直接调用 gym 中提供的 <code class="docutils literal notranslate"><span class="pre">Space.sample()</span></code> 方法来随机选取动作。但需要注意的是，由于 DI-engine 要求所有返回的 action 需要是 <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> 格式的，所以可能需要做一些必要的格式转换。例如：</p>
</li>
</ol>
</div>
<div class="section" id="dingenvwrapper">
<h2>DingEnvWrapper<a class="headerlink" href="#dingenvwrapper" title="Permalink to this headline">¶</a></h2>
<p>(in <code class="docutils literal notranslate"><span class="pre">ding/envs/env/ding_env_wrapper.py</span></code>)</p>
<p><code class="docutils literal notranslate"><span class="pre">DingEnvWrapper</span></code> 可以快速将 ClassicControl, Box2d, Atari, Mujoco, GymHybrid 等简单环境转换为符合 <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> 的环境。</p>
<p>可以查看 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/envs/env/tests/test_ding_env_wrapper.py">使用实例</a> 获取更多信息。</p>
</div>
<div class="section" id="q-a">
<h2>Q &amp; A<a class="headerlink" href="#q-a" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>MARL 环境应当如何迁移？</p>
<p>可以参考 <a class="reference external" href="../env_tutorial/competitive_rl_zh.html">Competitive RL</a></p>
<ul class="simple">
<li><p>如果环境既支持 single-agent，又支持 double-agent 甚至 multi-agent，那么要针对不同的模式分类考虑</p></li>
<li><p>在 multi-agent 环境中，action 和 observation 和 agent 个数匹配，但 reward 和 done 却不一定，需要搞清楚 reward 的定义</p></li>
<li><p>注意原始环境要求 action 和 observation 怎样组合在一起（元组、列表、字典、stacked array…）</p></li>
</ul>
</li>
<li><p>混合动作空间的环境应当如何迁移？</p>
<p>可以参考 <a class="reference external" href="../env_tutorial/gym_hybrid_zh.html">Gym-Hybrid</a></p>
<ul class="simple">
<li><p>Gym-Hybrid 中部分离散动作（Accelerate，Turn）是需要给出对应的 1 维连续参数的，以表示加速度和旋转角度，因此类似的环境需要主要关注其动作空间的定义</p></li>
</ul>
</li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nstep_td.html" class="btn btn-neutral float-right" title="N-step TD" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index_zh.html" class="btn btn-neutral float-left" title="最佳实践" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>