

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>SQIL &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="A2C" href="a2c.html" />
    <link rel="prev" title="SQN" href="sqn.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>SQIL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/sqil.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sqil">
<h1>SQIL<a class="headerlink" href="#sqil" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Soft Q imitation learning (SQIL) is an off-policy maximum entropy Q learning algorithm together with imitation learning. SQIL was first proposed in <a class="reference external" href="https://arxiv.org/abs/1905.11108">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</a>, which combines soft Q-learning with imitation learning. In the domain of discrete action spaces, soft Q learning proposed in &lt;<a class="reference external" href="https://arxiv.org/abs/1702.08165">https://arxiv.org/abs/1702.08165</a>&gt;` learns stochastic (maximum entropy) policies instead of determistic policies comparing to the deep Q learning algorithm.
SQIL and SQL can be easily generalised to continuous action spaces</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>SQIL is a <strong>model-free</strong> and <strong>value-based</strong> RL algorithm.</p></li>
<li><p>SQIL is SQL incorporated with Imitation learning</p></li>
<li><p>SQIL supports both <strong>discrete</strong> and <strong>continuous</strong> action spaces.</p></li>
<li><p>SQIL is an <strong>off-policy</strong> algorithm.</p></li>
<li><p>In DI-engine, SQIL uses <strong>eps-greedy</strong> for exploration.</p></li>
<li><p>The DI-engine implementation of SQIL only supports <strong>discrete</strong> action spaces for now.</p></li>
<li><p>The advantages for SQIL include more robustness in the face of uncertain dynamics and Naturally incorporation with exploration.</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>SQL considers a more general maximum entropy policy, such that the optimal policy aims to maximize its entropy at each visited state:</p>
<img alt="../_images/policy_sqil_2.png" src="../_images/policy_sqil_2.png" />
<p>where <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">{\alpha}</span>)</p>
<p>latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2020-02-02&gt; patch level 2
L3 programming layer &lt;2020-02-14&gt;
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2019/12/20 v1.4l Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size12.clo))
(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amscls/amsthm.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))

! LaTeX Error: File `anyfontsize.sty' not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: sty)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.8 \usepackage
               {bm}^^M
No pages of output.
Transcript written on math.log.
</p>
</div>
   is an optional but convenient parameter that can be used to determine the relative importance of entropy and reward. In practice, <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">{\alpha}</span>)</p>
<p>latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2020-02-02&gt; patch level 2
L3 programming layer &lt;2020-02-14&gt;
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2019/12/20 v1.4l Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size12.clo))
(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amscls/amsthm.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))

! LaTeX Error: File `anyfontsize.sty' not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: sty)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.8 \usepackage
               {bm}^^M
No pages of output.
Transcript written on math.log.
</p>
</div>
  is a hyperparameter that has to be tuned (This is not a parameter to learn).</p>
<p>With respect to discrete action spaces, one can write down the Bellman’s equation for action-value function:</p>
<img alt="../_images/Q_sqil.png" src="../_images/Q_sqil.png" />
<p>Therefore, the value function is given by:</p>
<img alt="../_images/V_sqil.png" src="../_images/V_sqil.png" />
<p>By defining policy to be proportional to a exponential function of some energy function (In this context, the energy function is Q), one can write down the (normalised) optimal policy in the form of Boltzmann distribution over actions,:</p>
<img alt="../_images/pi_sqil.png" src="../_images/pi_sqil.png" />
<p>Therefore, the Q values with the best action is of the following form:</p>
<a class="reference internal image-reference" href="../_images/ul_V_sqil_2.png"><img alt="../_images/ul_V_sqil_2.png" src="../_images/ul_V_sqil_2.png" style="width: 142.5px; height: 30.5px;" /></a>
<p>SQIL performs SQL with three small but important, modifications:</p>
<ol class="arabic simple">
<li><p>It initially fills the agent’s experience replay buffer with demonstrations, where the rewards are set
to a constant r = +1.</p></li>
<li><p>As the agent interacts with the environment and accumulates new experiences, it adds them to the replay buffer, and sets the rewards for these new experiences to a constant r = 0</p></li>
<li><p>It balances the number of demonstration experiences and new experiences (50% each) in each sample from the replay buffer</p></li>
</ol>
<p>BC is a simple approach that seeks to imitate the expert’s actions using supervised learning – in particular, greedily maximizing the conditional likelihood of the demonstrated actions given the demonstrated states, without reasoning about the consequences of actions.
Theoretically, It can be shown that SQIL is equivalent to augmenting BC with a regularization term that incorporates information about the state transition dynamics into the imitation policy, and thus enables long-horizon imitation.</p>
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<p>SQIL = SQL + Imitation learning. The pseudo code is as follows:</p>
<img alt="../_images/SQIL_algo.png" src="../_images/SQIL_algo.png" />
<div class="line-block">
<div class="line">where</div>
</div>
<img alt="../_images/SQIL_part.png" src="../_images/SQIL_part.png" />
</div>
<div class="section" id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.sql.SQLPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.sql.</span></span><span class="sig-name descname"><span class="pre">SQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sql.html#SQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.sql.SQLPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of SQL algorithm.</p>
</dd>
</dl>
</dd></dl>

<p>The bellman updates of SQIL/SQL and the Q-value function updates are defined in the function <code class="docutils literal notranslate"><span class="pre">q_nstep_sql_td_error</span></code> of <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/td.py</span></code>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_nstep_sql_td_error</span><span class="p">(</span>
      <span class="n">data</span><span class="p">:</span> <span class="n">namedtuple</span><span class="p">,</span>
      <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
      <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
      <span class="n">nstep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
      <span class="n">cum_reward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
      <span class="n">value_gamma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">criterion</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
   <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">   Overview:</span>
<span class="sd">      Multistep (1 step or n step) td_error for q-learning based algorithm</span>
<span class="sd">   Arguments:</span>
<span class="sd">      - data (:obj:`q_nstep_td_data`): the input data, q_nstep_sql_td_data to calculate loss</span>
<span class="sd">      - gamma (:obj:`float`): discount factor</span>
<span class="sd">      - alpha (:obj:｀float`): A parameter to weight entropy term in a policy equation</span>
<span class="sd">      - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data</span>
<span class="sd">      - value_gamma (:obj:`torch.Tensor`): gamma discount value for target soft_q_value</span>
<span class="sd">      - criterion (:obj:`torch.nn.modules`): loss function criterion</span>
<span class="sd">      - nstep (:obj:`int`): nstep num, default set to 1</span>
<span class="sd">   Returns:</span>
<span class="sd">      - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor</span>
<span class="sd">      - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor</span>
<span class="sd">   Shapes:</span>
<span class="sd">      - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\</span>
<span class="sd">            [&#39;q&#39;, &#39;next_n_q&#39;, &#39;action&#39;, &#39;reward&#39;, &#39;done&#39;]</span>
<span class="sd">      - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]</span>
<span class="sd">      - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`</span>
<span class="sd">      - action (:obj:`torch.LongTensor`): :math:`(B, )`</span>
<span class="sd">      - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`</span>
<span class="sd">      - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)</span>
<span class="sd">      - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep</span>
<span class="sd">      - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`</span>
<span class="sd">   &quot;&quot;&quot;</span>
   <span class="n">q</span><span class="p">,</span> <span class="n">next_n_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_n_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">data</span>
   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span>
   <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

   <span class="n">batch_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
   <span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">batch_range</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
   <span class="n">target_v</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">next_n_q</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
   <span class="n">target_v</span><span class="p">[</span><span class="n">target_v</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">20</span>
   <span class="n">target_v</span><span class="p">[</span><span class="n">target_v</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">20</span>
   <span class="c1"># For an appropriate hyper-parameter alpha, these hardcodes can be removed.</span>
   <span class="c1"># However, algorithms may face the danger of explosion for other alphas.</span>
   <span class="c1"># The hardcodes above are to prevent this situation from happening</span>
   <span class="n">record_target_v</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">target_v</span><span class="p">)</span> <span class="c1">#add the value function into tensorboard</span>
   <span class="k">if</span> <span class="n">cum_reward</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">value_gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">target_v</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">**</span> <span class="n">nstep</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_v</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
            <span class="n">target_v</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">value_gamma</span> <span class="o">*</span> <span class="n">target_v</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
   <span class="k">else</span><span class="p">:</span>
      <span class="n">target_v</span> <span class="o">=</span> <span class="n">nstep_return</span><span class="p">(</span><span class="n">nstep_return_data</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">target_v</span><span class="p">,</span> <span class="n">done</span><span class="p">),</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">nstep</span><span class="p">,</span> <span class="n">value_gamma</span><span class="p">)</span>
   <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">q_s_a</span><span class="p">,</span> <span class="n">target_v</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
   <span class="k">return</span> <span class="p">(</span><span class="n">td_error_per_sample</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">td_error_per_sample</span><span class="p">,</span> <span class="n">record_target_v</span>
</pre></div>
</div>
</div></blockquote>
<p>We use an epsilon-greedy strategy when implementing the SQIL/SQL policy.  How we pick actions is implemented in  <code class="docutils literal notranslate"><span class="pre">EpsGreedySampleWrapper_sql</span></code> of <code class="docutils literal notranslate"><span class="pre">ding/model/wrappers/model_wrappers.py</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EpsGreedySampleWrapperSql</span><span class="p">(</span><span class="n">IModelWrapper</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">)</span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;model output must be dict, but find </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
      <span class="n">logit</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">]</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="p">[</span><span class="n">logit</span><span class="p">]</span>
      <span class="k">if</span> <span class="s1">&#39;action_mask&#39;</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;action_mask&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
               <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="mf">1e8</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">mask</span><span class="p">)]</span>
      <span class="k">else</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">action</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">logit</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
               <span class="n">prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
               <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">prob</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">pi_action</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
               <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pi_action</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
               <span class="k">if</span> <span class="n">mask</span><span class="p">:</span>
                  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_action</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span>
               <span class="k">else</span><span class="p">:</span>
                  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">l</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">logit</span> <span class="o">=</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">output</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">action</span>
      <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>We have two buffers: one buffer is for new data by interacting with the environment and the other one is for demonstration data. We obtain the demonstration data online. That is,
we use a well-trained model to generate data in the collecting stage and push them into the demonstration buffer. In learning process, we sample from these two buffers separately shown as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># During the learning stage</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">update_per_collect</span><span class="p">):</span>
   <span class="n">train_data_new</span> <span class="o">=</span> <span class="n">replay_buffer_new</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
         <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span>
   <span class="p">)</span>
   <span class="n">train_data_demonstration</span> <span class="o">=</span> <span class="n">replay_buffer_demonstration</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
         <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span>
   <span class="p">)</span>
   <span class="k">if</span> <span class="n">train_data_new</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">train_data_demonstration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">train_data</span> <span class="o">=</span> <span class="kc">None</span>
   <span class="k">else</span><span class="p">:</span>
         <span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data_new</span> <span class="o">+</span> <span class="n">train_data_demonstration</span>
   <span class="k">if</span> <span class="n">train_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<p>We also need to modify rewards for new data and demonstration data. Taking the CartPole environment as an example:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect_data</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">eps</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_data</span><span class="p">)):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
    <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Regrading the demonstration data, we can leave these rewards as they were. For a general reward modification, please refer to <code class="docutils literal notranslate"><span class="pre">ding//entry/serial_entry_sqil.py</span></code>.</p>
<p>Regrading its performance, we drew a table below to compare with DQN, SQL in lunarlander and pong environments</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 26%" />
<col style="width: 28%" />
<col style="width: 29%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>env / method</p></th>
<th class="head"><p>DQN</p></th>
<th class="head"><p>SQL</p></th>
<th class="head"><p>SQIL</p></th>
<th class="head"><p>alpha</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LunarLander</p></td>
<td><p>153392 / 277 / 23900 (both off)
83016  / 155 / 12950 (both on)</p></td>
<td><p>693664 / 1017 / 32436 (both off)
1149592 / 1388/ 53805 (both on)</p></td>
<td><p>35856   / 238  / 1683   (both off)
31376   / 197  / 1479   (both on)</p></td>
<td><p>0.08</p></td>
</tr>
<tr class="row-odd"><td><p>Pong</p></td>
<td><p>765848 / 482 / 80000 (both on)</p></td>
<td><p>2682144 / 1750 / 278250 (both on)</p></td>
<td><p>2390608 / 1665 / 247700 (both on)</p></td>
<td><p>0.12</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The stopping values for Lunarlander and Pong are 200 and 20 respectively.</p></li>
<li><p>both on：cuda = True； base env manger = subprocess</p></li>
<li><p>both off：cuda = False； base env manager = base</p></li>
</ul>
</div>
<img alt="../_images/pong.png" src="../_images/pong.png" />
<p>The  above tensorboard diagram corresponds to the convergence of SQIL in the pong environment when alpha = 0.12, as shown in the above table.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Siddharth Reddy, Anca D. Dragan, Sergey Levine: “SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards”, 2019; [<a class="reference external" href="https://arxiv.org/abs/1905.11108">https://arxiv.org/abs/1905.11108</a> arXiv:1905.11108].</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="a2c.html" class="btn btn-neutral float-right" title="A2C" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sqn.html" class="btn btn-neutral float-left" title="SQN" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>