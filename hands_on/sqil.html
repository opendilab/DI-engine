

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>SQIL &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GAIL" href="gail.html" />
    <link rel="prev" title="DQfD" href="dqfd.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>SQIL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/sqil.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sqil">
<h1>SQIL<a class="headerlink" href="#sqil" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Soft Q imitation learning (SQIL) is an off-policy maximum entropy Q learning algorithm together with imitation learning. SQIL was first proposed in <a class="reference external" href="https://arxiv.org/abs/1905.11108">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</a>, which combines soft Q-learning with imitation learning. In the domain of discrete action spaces, soft Q learning proposed in <a class="reference external" href="https://arxiv.org/abs/1702.08165">Reinforcement Learning with Deep Energy-Based Policies</a> learns stochastic (maximum entropy) policies instead of determistic policies comparing to the deep Q learning algorithm.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>SQIL is a <strong>model-free</strong> and <strong>value-based</strong> RL algorithm.</p></li>
<li><p>SQIL is SQL incorporated with Imitation learning.</p></li>
<li><p>SQIL supports both <strong>discrete</strong> and <strong>continuous</strong> action spaces, which is the same as SQL.</p></li>
<li><p>SQIL is an <strong>off-policy</strong> algorithm.</p></li>
<li><p>In DI-engine, SQIL uses <strong>eps-greedy</strong> for exploration.</p></li>
<li><p>The DI-engine implementation of SQIL only supports <strong>discrete</strong> action spaces for now.</p></li>
<li><p>The advantages of SQIL include: i. More robustness in the face of uncertain dynamics; ii. A natural incorporation with exploration.</p></li>
<li><p>SQIL can be regarded as <strong>regularized behavior cloning</strong>, which gains advantage over general behavior cloning.</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>SQIL is a behavior cloning variant of SQL, so you may refer to <a class="reference external" href="./sql.html">SQL doc</a> first.</p>
<p>SQIL performs three small but important modifications on SQL:</p>
<ol class="arabic simple">
<li><p>It initially fills the agent’s experience replay buffer with demonstrations, where the rewards are set
to a constant r = +1.</p></li>
<li><p>As the agent interacts with the environment and accumulates new experiences, it adds them to the replay buffer, and sets the rewards for these new experiences to a constant r = 0.</p></li>
<li><p>It balances the number of demonstration experiences and new experiences (50% each) in each sample from the replay buffer.</p></li>
</ol>
<p>BC is a simple approach that seeks to imitate the expert’s actions using supervised learning – in particular, greedily maximizing the conditional likelihood of the demonstrated actions given the demonstrated states, without reasoning about the consequences of actions.
Theoretically, It can be shown that SQIL is equivalent to augmenting BC with a regularization term that incorporates information about the state transition dynamics into the imitation policy, and thus enables long-horizon imitation.</p>
<p>Specifically, recalling from <a class="reference external" href="./sql.html">SQL</a>, the soft Q values are a function of the rewards and dynamics, given
by the the soft Bellman equation:</p>
<img alt="../_images/sqil_soft_q.png" src="../_images/sqil_soft_q.png" />
<p>The policy <span class="math notranslate nohighlight">\(\pi\)</span> forms a Boltzmann distribution over actions:</p>
<img alt="../_images/sqil_pi.png" src="../_images/sqil_pi.png" />
<p>Rearrange the above function, we obtain the formula for the reward:</p>
<img alt="../_images/sqil_implied_reward.png" src="../_images/sqil_implied_reward.png" />
<p>Therefore, the regular BC can be derived as follows:</p>
<img alt="../_images/sqil_bc1.png" src="../_images/sqil_bc1.png" />
<img alt="../_images/sqil_bc2.png" src="../_images/sqil_bc2.png" />
<p>The regularized BC is formulated through the prior work <a class="reference external" href="http://www.ifaamas.org/Proceedings/aamas2014/aamas/p1249.pdf">Boosted and reward regularized classification for apprenticeship learning</a> where Q is regularized with a sparsity prior on the above rewards.
By adding BC with this regularization, our final regularized BC target incorporates information about the state transition dynamics
into the imitation learning objective, since <span class="math notranslate nohighlight">\(R_{q}(s,a)\)</span> is a function of an expectation over next state <span class="math notranslate nohighlight">\(s'\)</span>. Formally, its formula is given as follows:</p>
<img alt="../_images/sqil_rbc.png" src="../_images/sqil_rbc.png" />
<p>where where <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}_{&gt;0}\)</span>  is a constant hyperparameter, and  <span class="math notranslate nohighlight">\(\delta\)</span> denotes the squared soft Bellman error defined
in the equation in the Pseudo-code part below. The BC loss encourages <span class="math notranslate nohighlight">\(Q\)</span>  to output high values for demonstrated actions at
demonstrated states, and the penalty term propagates those high values to nearby states. In other
words, <span class="math notranslate nohighlight">\(Q\)</span>  outputs high values for actions that lead to states from which the demonstrated states are
reachable. Hence, when the agent finds itself far from the demonstrated states, it takes actions that
lead it back to the demonstrated states.</p>
<p>Surprisingly, it can be proved that the gradient of the regularized BC loss in the above equation is proportional to the gradient of the SQIL
loss in line 4 of Algorithm 1 below, plus an additional term that penalizes the soft value of the initial state <span class="math notranslate nohighlight">\(s_{0}\)</span></p>
<img alt="../_images/sqil_equa.png" src="../_images/sqil_equa.png" />
<p>For the full derivation, please refer to Section A.1 of the appendix in the original paper.</p>
<p>In summary, SQIL solves a similar optimization problem to regularized BC proposed in <a class="reference external" href="http://www.ifaamas.org/Proceedings/aamas2014/aamas/p1249.pdf">Boosted and reward regularized classification for apprenticeship learning</a>. However, SQIL can be motivated as a more practical and trivial way to implement the ideas for the regularized BC.
Furthermore, the ablation study from the experiments in the paper suggests that SQIL actually performs better than the regularized BC.</p>
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<p>SQIL = SQL + Imitation learning. The pseudo code is as follows:</p>
<img alt="../_images/SQIL_algo.png" src="../_images/SQIL_algo.png" />
<div class="line-block">
<div class="line">where</div>
</div>
<img alt="../_images/SQIL_part.png" src="../_images/SQIL_part.png" />
</div>
<div class="section" id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.sql.SQLPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.sql.</span></span><span class="sig-name descname"><span class="pre">SQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sql.html#SQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.sql.SQLPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of SQL algorithm.</p>
</dd>
</dl>
</dd></dl>

<p>The Bellman update of SQIL/SQL and the Q-value function update are defined in the function <code class="docutils literal notranslate"><span class="pre">q_nstep_sql_td_error</span></code> of <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/td.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_nstep_sql_td_error</span><span class="p">(</span>
      <span class="n">data</span><span class="p">:</span> <span class="n">namedtuple</span><span class="p">,</span>
      <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
      <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
      <span class="n">nstep</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
      <span class="n">cum_reward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
      <span class="n">value_gamma</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
      <span class="n">criterion</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Overview:</span>
<span class="sd">      Multistep (1 step or n step) td_error for q-learning based algorithm</span>
<span class="sd">Arguments:</span>
<span class="sd">      - data (:obj:`q_nstep_td_data`): the input data, q_nstep_sql_td_data to calculate loss</span>
<span class="sd">      - gamma (:obj:`float`): discount factor</span>
<span class="sd">      - alpha (:obj:｀float`): A parameter to weight entropy term in a policy equation</span>
<span class="sd">      - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data</span>
<span class="sd">      - value_gamma (:obj:`torch.Tensor`): gamma discount value for target soft_q_value</span>
<span class="sd">      - criterion (:obj:`torch.nn.modules`): loss function criterion</span>
<span class="sd">      - nstep (:obj:`int`): nstep num, default set to 1</span>
<span class="sd">Returns:</span>
<span class="sd">      - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor</span>
<span class="sd">      - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor</span>
<span class="sd">Shapes:</span>
<span class="sd">      - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\</span>
<span class="sd">            [&#39;q&#39;, &#39;next_n_q&#39;, &#39;action&#39;, &#39;reward&#39;, &#39;done&#39;]</span>
<span class="sd">      - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]</span>
<span class="sd">      - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`</span>
<span class="sd">      - action (:obj:`torch.LongTensor`): :math:`(B, )`</span>
<span class="sd">      - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`</span>
<span class="sd">      - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)</span>
<span class="sd">      - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep</span>
<span class="sd">      - td_error_per_sample (:obj:`torch.FloatTensor`): :math:`(B, )`</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">q</span><span class="p">,</span> <span class="n">next_n_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_n_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">data</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">shape</span>
<span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="n">batch_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">batch_range</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
<span class="n">target_v</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">next_n_q</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">target_v</span><span class="p">[</span><span class="n">target_v</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">target_v</span><span class="p">[</span><span class="n">target_v</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-Inf&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">20</span>
<span class="c1"># For an appropriate hyper-parameter alpha, these hardcodes can be removed.</span>
<span class="c1"># However, algorithms may face the danger of explosion for other alphas.</span>
<span class="c1"># The hardcodes above are to prevent this situation from happening</span>
<span class="n">record_target_v</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">target_v</span><span class="p">)</span> <span class="c1">#add the value function into tensorboard</span>
<span class="k">if</span> <span class="n">cum_reward</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">value_gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">target_v</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">**</span> <span class="n">nstep</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_v</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
            <span class="n">target_v</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">value_gamma</span> <span class="o">*</span> <span class="n">target_v</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
      <span class="n">target_v</span> <span class="o">=</span> <span class="n">nstep_return</span><span class="p">(</span><span class="n">nstep_return_data</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">target_v</span><span class="p">,</span> <span class="n">done</span><span class="p">),</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">nstep</span><span class="p">,</span> <span class="n">value_gamma</span><span class="p">)</span>
<span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">q_s_a</span><span class="p">,</span> <span class="n">target_v</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<span class="k">return</span> <span class="p">(</span><span class="n">td_error_per_sample</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">td_error_per_sample</span><span class="p">,</span> <span class="n">record_target_v</span>
</pre></div>
</div>
<p>We use an epsilon-greedy strategy when implementing the SQIL/SQL policy.  How we pick actions is implemented in  <code class="docutils literal notranslate"><span class="pre">EpsGreedySampleWrapper_sql</span></code> of <code class="docutils literal notranslate"><span class="pre">ding/model/wrappers/model_wrappers.py</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EpsGreedySampleWrapperSql</span><span class="p">(</span><span class="n">IModelWrapper</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="n">eps</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">)</span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;model output must be dict, but find </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
      <span class="n">logit</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">]</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="p">[</span><span class="n">logit</span><span class="p">]</span>
      <span class="k">if</span> <span class="s1">&#39;action_mask&#39;</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;action_mask&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
               <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="mf">1e8</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">))</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">mask</span><span class="p">)]</span>
      <span class="k">else</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">action</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">logit</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
               <span class="n">prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
               <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">prob</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
               <span class="n">pi_action</span> <span class="o">=</span> <span class="n">pi_action</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
               <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pi_action</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
               <span class="k">if</span> <span class="n">mask</span><span class="p">:</span>
                  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_action</span><span class="p">(</span><span class="n">prob</span><span class="o">=</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span>
               <span class="k">else</span><span class="p">:</span>
                  <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">l</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">logit</span> <span class="o">=</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">output</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">action</span>
      <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>We have two buffers: one buffer is for new data which is collected through interacting with the environment and the other one is for demonstration data. We obtain the demonstration data online. That is,
we use a well-trained model to generate data in the collecting stage and push them into the demonstration buffer. In learning process, we sample from these two buffers separately shown as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># During the learning stage</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">update_per_collect</span><span class="p">):</span>
   <span class="n">train_data_new</span> <span class="o">=</span> <span class="n">replay_buffer_new</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
         <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span>
   <span class="p">)</span>
   <span class="n">train_data_demonstration</span> <span class="o">=</span> <span class="n">replay_buffer_demonstration</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
         <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span>
   <span class="p">)</span>
   <span class="k">if</span> <span class="n">train_data_new</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">train_data_demonstration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">train_data</span> <span class="o">=</span> <span class="kc">None</span>
   <span class="k">else</span><span class="p">:</span>
         <span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data_new</span> <span class="o">+</span> <span class="n">train_data_demonstration</span>
   <span class="k">if</span> <span class="n">train_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<p>We also need to modify rewards for new data and demonstration data. Taking the CartPole environment as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect_data</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">eps</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_data</span><span class="p">)):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
    <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Regarding the demonstration data, we can leave these rewards unchanged. For a general reward modification, please refer to <code class="docutils literal notranslate"><span class="pre">ding//entry/serial_entry_sqil.py</span></code>.</p>
</div>
<div class="section" id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pong</p>
<p>(PongNoFrameskip-v4)</p>
</td>
<td><p>20</p></td>
<td><img alt="../_images/pong_sqil.png" src="../_images/pong_sqil.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/pong/pong_sqil_config.py">config_link_p</a></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Qbert</p>
<p>(QbertNoFrameskip-v4)</p>
</td>
<td><p>14941</p></td>
<td><img alt="../_images/qbert_sqil.png" src="../_images/qbert_sqil.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/qbert/qbert_sqil_config.py">config_link_q</a></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>SpaceInvaders</p>
<p>(SpaceInvadersNoFrame
skip-v4)</p>
</td>
<td><p>1002</p></td>
<td><img alt="../_images/spaceinvaders_sqil.png" src="../_images/spaceinvaders_sqil.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sqil_config.py">config_link_s</a></p></td>
<td></td>
</tr>
</tbody>
</table>
<p>P.S.：</p>
<ol class="arabic simple">
<li><p>The above results are obtained by running the same configuration on five different random seeds (0, 1, 2, 3, 4)</p></li>
</ol>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Siddharth Reddy, Anca D. Dragan, Sergey Levine: “SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards”, 2019; [<a class="reference external" href="https://arxiv.org/abs/1905.11108">https://arxiv.org/abs/1905.11108</a> arXiv:1905.11108].</p>
<p>Haarnoja, Tuomas, et al. “Reinforcement learning with deep energy-based policies.” International Conference on Machine Learning. PMLR, 2017.</p>
<p>Piot, Bilal, Matthieu Geist, and Olivier Pietquin. “Boosted and reward-regularized classification for apprenticeship learning.” Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. 2014.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gail.html" class="btn btn-neutral float-right" title="GAIL" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dqfd.html" class="btn btn-neutral float-left" title="DQfD" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>