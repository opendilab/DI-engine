

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>R2D3 &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MBPO" href="mbpo.html" />
    <link rel="prev" title="Guided Cost Learning" href="guided_cost_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index_zh.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">使用者指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index_zh.html">安装说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index_zh.html">快速上手</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习算法攻略合集</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dqn_zh.html">DQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="rainbow.html">Rainbow</a></li>
<li class="toctree-l2"><a class="reference internal" href="c51.html">C51</a></li>
<li class="toctree-l2"><a class="reference internal" href="qrdqn.html">QRDQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="iqn.html">IQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="sql.html">SQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="sqn.html">SQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="r2d2.html">R2D2</a></li>
<li class="toctree-l2"><a class="reference internal" href="a2c.html">A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppo.html">PPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="acer.html">ACER</a></li>
<li class="toctree-l2"><a class="reference internal" href="impala.html">IMPALA</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppg.html">PPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="ddpg.html">DDPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="d4pg.html">D4PG</a></li>
<li class="toctree-l2"><a class="reference internal" href="td3.html">TD3</a></li>
<li class="toctree-l2"><a class="reference internal" href="sac.html">SAC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cql.html">CQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="td3_bc.html">TD3BC</a></li>
<li class="toctree-l2"><a class="reference internal" href="qmix.html">QMIX</a></li>
<li class="toctree-l2"><a class="reference internal" href="coma.html">COMA</a></li>
<li class="toctree-l2"><a class="reference internal" href="wqmix.html">WQMIX</a></li>
<li class="toctree-l2"><a class="reference internal" href="collaq.html">CollaQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="atoc.html">ATOC</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnd.html">RND</a></li>
<li class="toctree-l2"><a class="reference internal" href="her.html">HER</a></li>
<li class="toctree-l2"><a class="reference internal" href="dqfd.html">DQfD</a></li>
<li class="toctree-l2"><a class="reference internal" href="sqil.html">SQIL</a></li>
<li class="toctree-l2"><a class="reference internal" href="gail.html">GAIL</a></li>
<li class="toctree-l2"><a class="reference internal" href="trex.html">TREX</a></li>
<li class="toctree-l2"><a class="reference internal" href="icm_zh.html">ICM</a></li>
<li class="toctree-l2"><a class="reference internal" href="guided_cost_zh.html">Guided Cost Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">R2D3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">核心要点</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">关键方程或关键框图</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">伪代码</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">重要的实现细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">基准算法性能</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="mbpo.html">MBPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="vpn.html">VPN</a></li>
<li class="toctree-l2"><a class="reference internal" href="mcts.html">MCTS</a></li>
<li class="toctree-l2"><a class="reference internal" href="alphago.html">AlphaGo</a></li>
<li class="toctree-l2"><a class="reference internal" href="alphago.html#alphagozero">AlphaGoZero</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index_zh.html">强化学习环境示例手册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index_zh.html">分布式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_zh.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index_zh.html">特性介绍</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index_zh.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index_zh.html">中间件（middleware）编写规范</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index_zh.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index_zh.html">Docs</a> &raquo;</li>
        
          <li><a href="index_zh.html">强化学习算法攻略合集</a> &raquo;</li>
        
      <li>R2D3</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/r2d3_zh.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="r2d3">
<h1>R2D3<a class="headerlink" href="#r2d3" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>R2D3 (Recurrent Replay Distributed DQN from Demonstrations) 首次在论文
<a class="reference external" href="https://arxiv.org/abs/1909.01387">Making Efficient Use of Demonstrations to Solve Hard Exploration Problems</a> 中提出, 它可以有效地利用专家演示轨迹来解决具有以下3个属性的问题：初始条件高度可变、部分可观察、困难探索。
此外他们还介绍了一组结合这三个属性的八个任务，并表明R2D3可以解决像这些任务，值得注意的是，在类似这样的任务上，其他一些最先进的方法，无论有还是没有专家演示轨迹，在数百亿次的探索步骤之后甚至可能
无法看到一条成功的轨迹。R2D3本质上是有效结合了R2D2算法的分布式框架和循环神经网络结构，以及DQfD中为从专家轨迹中学习而特别设计的损失函数。</p>
</div>
<div class="section" id="id2">
<h2>核心要点<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>1.R2D3的基线强化学习算法是 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2.py">R2D2</a>, 可以参考我们的实现 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2.py">r2d2</a> ,
它本质上是一个基于分布式框架，采用了双Q网络(Double Q Networks), 决斗Q结构(Dueling Architecture)，多步时间差分损失函数(n-step TD loss)的DQN算法。</p>
<p>2.R2D3利用了DQfD的损失函数，包括：一步和n步的时序差分损失，神经网络参数的L2正则化损失(可选)，监督大间隔分类损失(supervised large margin classification loss)。
主要区别在于R2D3损失函数中的所有Q值都是序列样本通过循环神经Q网络后计算得到的，而原始DQfD中的Q值是通过一步的样本通过卷积网络和(或)前向全连接网络得到。</p>
<p>3.由于R2D3是在序列样本上进行运算的，所以其专家轨迹也应该以序列样本的方式给出。在具体实现中，我们往往是用另一个基线强化学习算法(如PPO或R2D2)收敛后得到的专家模型来
产生对应的专家演示轨迹，为此我们专门写了对应的策略函数来从这样的专家模型中产生专家演示,
参见 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo_offpolicy_collect_traj.py">ppo_offpolicy_collect_traj.py</a> 和 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2_collect_traj.py">r2d2_collect_traj.py</a> .</p>
<p>4.在训练Q网络时，采样的mini-batch中的每一条序列样本，有pho的概率是专家演示序列样本，有1-pho的概率是智能体与环境交互的经验序列样本。</p>
<p>5.R2D3的提出是为了解决初始条件高度可变、部分可观察环境中的困难探索问题，其他探索相关的论文，读者可以参考 <a class="reference external" href="https://arxiv.org/abs/2002.06038">NGU</a> ，它是融合了
<a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">ICM</a> 和 <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">RND</a> 等多种探索方法的一个综合体。</p>
<div class="section" id="id4">
<h3>关键方程或关键框图<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>R2D3算法的总体分布式训练流程如下：</p>
<a class="reference internal image-reference" href="../_images/r2d3_overview.png"><img alt="../_images/r2d3_overview.png" class="align-center" src="../_images/r2d3_overview.png" style="width: 440.8px; height: 284.0px;" /></a>
<p>learner中用于训练Q网络而采样的mini_batch包含了2部分: 1. 专家演示轨迹, 2. 智能体在训练过程中与环境交互产生的经验轨迹。
专家演示和智能体经验之间的比率是一个关键的超参数, 必须仔细调整以实现良好的性能。</p>
<p>R2D3算法的Q网络结构图如下：</p>
<a class="reference internal image-reference" href="../_images/r2d3_q_net.png"><img alt="../_images/r2d3_q_net.png" class="align-center" src="../_images/r2d3_q_net.png" style="width: 549.6px; height: 302.40000000000003px;" /></a>
<p>(a)R2D3智能体使用的recurrent head。 (b)DQfD智能体使用的feedforward head。(c)表示输入的是大小为96x72的图像帧，
接着通过一个ResNet，然后将前一时刻的动作，前一时刻的奖励和当前时刻的其他本体感受特征(proprioceptive features) <span class="math notranslate nohighlight">\(f_{t}\)</span> （包括加速度、avatar是否握住物体以及手与avatar的相对距离等辅助信息
连接(concat)为一个新的向量，传入a)和b)中的head，用于计算Q值。</p>
<p>下面描述r2d3的损失函数设置，和DQfD一样，不过这里所有的Q值都是通过上面所描述的循环神经网络计算得到。包括：
一步时序差分损失，n步时序差分损失，监督大间隔分类损失，神经网络参数的L2正则化损失(可选)。
时序差分损失确保网络满足贝尔曼方程，监督损失用于使得专家演示者的动作Q值至少比其他动作的Q值高一个间隔(一个常数值)，网络权重和偏差的L2正则化损失用于防止Q网络在相对数量较小的专家演示数据集上过拟合。</p>
<ul class="simple">
<li><p>除了通常的1-step turn, R2D3还增加n-step return，有助于将专家轨迹的Q值传播到所有的早期的状态，从而获得更好的学习效果。
n步return为：</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/r2d3_nstep_return.png"><img alt="../_images/r2d3_nstep_return.png" class="align-center" src="../_images/r2d3_nstep_return.png" style="width: 363.20000000000005px; height: 34.4px;" /></a>
<ul>
<li><p>监督损失对于训练的效果至关重要。由于存在以下的情况：
1.专家演示数据可能只覆盖完整状态空间的一小部分,
2.数据中并不包含,(某一特定状态，所有可能执行的动作)的状态动作对,
因此许多 <em>状态动作对</em> 从未在专家样本中出现过。如果我们仅使用Q-learning的loss朝着下一个状态的最大Q值方向来更新Q网络，网络将倾向于朝着那些不准确的Q值中的最高方向更新,
并且网络将在整个学习过程中通过Q函数传播这些误差，造成误差累计造成过估计问题。这里采用了 <a class="reference external" href="https://arxiv.org/pdf/1606.01128.pdf">监督大边际分类损失(supervised large margin classification loss)</a>  来缓解这个问题，
其计算公式为：</p>
<a class="reference internal image-reference" href="../_images/r2d3_slmcl.png"><img alt="../_images/r2d3_slmcl.png" class="align-center" src="../_images/r2d3_slmcl.png" style="width: 341.6px; height: 38.400000000000006px;" /></a>
<p>其中 <span class="math notranslate nohighlight">\(a_{E}\)</span> 表示专家执行的动作。 <span class="math notranslate nohighlight">\(l(a_{E}, a)\)</span>  是一个边际函数，当:math:<cite>a = a_{E}</cite> 时为 0，否则为一个正的常数。
最小化这个监督损失，可以迫使除专家演示者执行的动作以外的 <strong>其他动作的Q值至少比专家演示者的动作Q值低一个间隔</strong>。
通过加上这个损失，将专家数据集合中没有遇到过的动作的Q值变成合理范围内的值，并使学得的值函数导出的贪婪策略模仿专家演示者的策略。</p>
<p>我们在DI-engine中的具体实现如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">margin_function</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
<span class="n">JE</span> <span class="o">=</span> <span class="n">is_expert</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="n">l</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_s_a</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>最终用于更新Q网络的整体损失是以上所有四种损失的线性组合：</p>
<a class="reference internal image-reference" href="../_images/r2d3_loss.png"><img alt="../_images/r2d3_loss.png" class="align-center" src="../_images/r2d3_loss.png" style="width: 365.6px; height: 32.800000000000004px;" /></a>
</div>
<div class="section" id="id5">
<h3>伪代码<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>下面是R2D3智能体learner和actor的伪代码。单个学习器进程(learner process)从专家演示缓冲区和智能体经验缓冲区中采样数据样本用于计算损失函数，更新其Q网络参数。
A个并行的行动者进程(actor process)与不同的独立的A个环境实例交互以快速获得多样化的数据，然后将数据放入智能体经验缓冲区。
A个actor会定期获取learner上最新的参数。</p>
<a class="reference internal image-reference" href="../_images/r2d3_pseudo_code_actor.png"><img alt="../_images/r2d3_pseudo_code_actor.png" class="align-center" src="../_images/r2d3_pseudo_code_actor.png" style="width: 691.2px; height: 126.4px;" /></a>
<a class="reference internal image-reference" href="../_images/r2d3_pseudo_code_learner.png"><img alt="../_images/r2d3_pseudo_code_learner.png" class="align-center" src="../_images/r2d3_pseudo_code_learner.png" style="width: 700.8000000000001px; height: 269.6px;" /></a>
</div>
<div class="section" id="id6">
<h3>重要的实现细节<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>1. 用于计算损失函数的mini-batch是从专家演示缓冲区和智能体经验缓冲区采样得到的，mini-batch包含&lt;batch_size&gt;个序列样本，以pho的概率从专家演示缓冲区中采样，以1-pho的概率从智能体经验缓冲区中采样。
其具体实现方式如下，通过从&lt;batch_size&gt;大小个的[0，1]均匀分布中采样，如果采样值大于pho则选择一个专家演示轨迹
这&lt;batch_size&gt;个采样值中大于pho的采样值的个数即为本次mini-batch中专家演示所占的个数。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The hyperparameter pho, the demo ratio, control the propotion of data coming</span>
<span class="c1"># from expert demonstrations versus from the agent&#39;s own experience.</span>
<span class="n">expert_batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">pho</span>
               <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">agent_batch_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">))</span> <span class="o">-</span> <span class="n">expert_batch_size</span>
<span class="n">train_data_agent</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent_batch_size</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
<span class="n">train_data_expert</span> <span class="o">=</span> <span class="n">expert_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">expert_batch_size</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
</pre></div>
</div>
<p>2.由于基线算法R2D2采用有优先级的采样，对于一个序列样本，每一时刻的TD error是 使用1步TD error和n步TD error的和的绝对值，TD error在这个序列经历的所有时刻上的平均值和最大值的加权和
作为整个序列样本的优先级。由于专家数据和经验数据对应的loss函数不一样， 在R2D2中我们设置了独立的2个replay_buffer, 分别为专家演示的 <code class="docutils literal notranslate"><span class="pre">expert_buffer</span></code> ，和智能体经验的 <code class="docutils literal notranslate"><span class="pre">replay_buffer</span></code> ，
并且分开进行优先级采样和buffer中相关参数的更新。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># using the mixture of max and mean absolute n-step TD-errors as the priority of the sequence</span>
<span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">td_error</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.9</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">td_error</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">td_error</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
<span class="c1"># td_error shape list(&lt;self._unroll_len_add_burnin_step-self._burnin_step-self._nstep&gt;, B), for example, (75,64)</span>
<span class="c1"># torch.sum(torch.stack(td_error), dim=0) can also be replaced with sum(td_error)</span>
<span class="o">...</span>
<span class="k">if</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;priority&#39;</span><span class="p">):</span>
    <span class="c1"># When collector, set replay_buffer_idx and replay_unique_id for each data item, priority = 1.\</span>
    <span class="c1"># When learner, assign priority for each data item according their loss</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">][</span>
        <span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">][</span>
        <span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">][</span><span class="n">agent_batch_size</span><span class="p">:]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">][</span>
        <span class="n">agent_batch_size</span><span class="p">:]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">][</span>
        <span class="n">agent_batch_size</span><span class="p">:]</span>

    <span class="c1"># Expert data and demo data update their priority separately.</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">)</span>
    <span class="n">expert_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">)</span>
</pre></div>
</div>
<p>3.对于专家演示样本和智能体经验样本，我们分别对原数据增加一个键 <code class="docutils literal notranslate"><span class="pre">is_expert</span></code> 加以区分, 如果是专家演示样本，此键值为1，
如果是智能体经验样本，此键值为0，</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 如果是专家演示样本，此键值为1，</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">expert_data</span><span class="p">)):</span>
    <span class="c1"># for rnn/sequence-based alg.</span>
    <span class="n">expert_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;is_expert&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">expert_cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">unroll_len</span>
<span class="o">...</span>
<span class="c1"># 如果是智能体经验样本，此键值为0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_data</span><span class="p">)):</span>
    <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;is_expert&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">expert_cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">unroll_len</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>预训练。在智能体与环境交互之前，我们可以先利用专家演示样本预训练Q网络，期望能得到一个好的初始化参数，加速后续的训练进程。</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">per_train_iter_k</span><span class="p">):</span>  <span class="c1"># pretrain</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="c1"># Learn policy from collected demo data</span>
    <span class="c1"># Expert_learner will train ``update_per_collect == 1`` times in one iteration.</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">expert_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;priority&#39;</span><span class="p">):</span>
        <span class="n">expert_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3>实现<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>r2d3的策略 <code class="docutils literal notranslate"><span class="pre">R2D3Policy</span></code> 的接口定义如下：</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.r2d3.</span></span><span class="sig-name descname"><span class="pre">R2D3Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/r2d3.html#R2D3Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of r2d3, from paper <cite>Making Efficient Use of Demonstrations to Solve Hard Exploration Problems</cite> .</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>dqn</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line">to correct biased update. If True,</div>
<div class="line">priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.997,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>3,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">burnin_step</span></code></p></td>
<td><p>int</p></td>
<td><p>2</p></td>
<td><div class="line-block">
<div class="line">The timestep of burnin operation,</div>
<div class="line">which is designed to RNN hidden state</div>
<div class="line">difference caused by off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>64</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.value_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">rescale</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether use value_rescale function for</div>
<div class="line">predicted value</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>100</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>[8, 128]</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d3.html#R2D3Policy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward and backward function of learn mode.
Acquire the data, calculate the loss and optimize learner model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, including at least </dt><dd><p>[‘main_obs’, ‘target_obs’, ‘burnin_obs’, ‘action’, ‘reward’, ‘done’, ‘weight’]</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): Including cur_lr and total_loss</dt><dd><ul>
<li><p>cur_lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Current learning rate</p></li>
<li><p>total_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The calculated loss</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>dqfd的损失函数 <code class="docutils literal notranslate"><span class="pre">nstep_td_error_with_rescale</span></code> 的接口定义如下：</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dqfd_nstep_td_error_with_rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_n_step_td:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_supervised_loss:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_one_step_td:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">margin_function:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em>, <em class="sig-param"><span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em>, <em class="sig-param"><span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em>, <em class="sig-param"><span class="pre">trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_transform&gt;</span></em>, <em class="sig-param"><span class="pre">inv_trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_inv_transform&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#dqfd_nstep_td_error_with_rescale"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dqfd_nstep_td_data</span></code>): the input data, dqfd_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): discount factor</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): gamma discount value for target q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 10</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code>): the q_nstep_td_data containing            [‘q’, ‘next_n_q’, ‘action’, ‘next_n_action’, ‘reward’, ‘done’, ‘weight’                , ‘new_n_q_one_step’, ‘next_n_action_one_step’, ‘is_expert’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>new_n_q_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>next_n_action_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>is_expert (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) : 0 or 1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>我们目前的r2d3策略实现中网络的输入只是时刻t的状态观测，不包含时刻t-1的动作和奖励, 也不包括额外的信息向量 <span class="math notranslate nohighlight">\(f_{t}\)</span> .</p>
</div>
</div>
<div class="section" id="id8">
<h3>基准算法性能<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>我们在PongNoFrameskip-v4环境上，做了一系列对比实验，以验证：1.用于训练的一个mini-batch中专家样本的占比pho, 2.专家演示所占比例, 3.是否利用预训练与l2正则化等不同参数设置对r2d3算法最终性能的影响。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>我们的专家数据通过 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo_offpolicy_collect_traj.py">ppo_offpolicy_collect_traj.py</a> 产生,
其专家模型来自于r2d2算法在该环境上训练到收敛后得到的专家模型。以下所有实验seed=0。</p>
<p>r2d2基线算法设置记为r2d2_n5_bs2_ul40_upc8_tut0.001_ed1e5_rbs1e5_bs64, 其中：</p>
<ul class="simple">
<li><p>n表示nstep,</p></li>
<li><p>bs表示burnin_step,</p></li>
<li><p>ul表示unroll_len,</p></li>
<li><p>upc表示update_per_collect,</p></li>
<li><p>tut表示target_update_theta,</p></li>
<li><p>ed表示eps_decay,</p></li>
<li><p>rbs表示replay_buffer_size,</p></li>
<li><p>bs表示batch_size,</p></li>
</ul>
<p>具体参见 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/pong/pong_r2d2_config.py">r2d2 pong config</a> .</p>
</div>
<ul>
<li><dl class="simple">
<dt>测试在用于训练的一个mini-batch中专家样本的占比的影响。观测1: pho需要适中，取1/4</dt><dd><ul class="simple">
<li><p>蓝线 pong_r2d2_rbs1e4</p></li>
<li><p>橙线 pong_r2d3_r2d2expert_k0_pho1-4_rbs1e4_1td_l2_ds5e3</p></li>
<li><p>灰线 pong_r2d3_r2d2expert_k0_pho1-16_rbs1e4_1td_l2_ds5e3</p></li>
<li><p>红线 pong_r2d3_r2d2expert_k0_pho1-2_rbs1e4_1td_l2_ds5e3</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_pho.png"><img alt="../_images/r2d3_pong_pho.png" class="align-center" src="../_images/r2d3_pong_pho.png" style="width: 684.0px; height: 161.0px;" /></a>
</li>
<li><dl class="simple">
<dt>测试总的专家样本库的大小的影响。观测2：demo size需要适中，取5e3</dt><dd><ul class="simple">
<li><p>橙线 pong_r2d2_rbs2e4</p></li>
<li><p>天蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds5e3</p></li>
<li><p>蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds1e3</p></li>
<li><p>绿线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds1e4</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_demosize.png"><img alt="../_images/r2d3_pong_demosize.png" class="align-center" src="../_images/r2d3_pong_demosize.png" style="width: 818.0px; height: 195.0px;" /></a>
</li>
<li><dl class="simple">
<dt>测试是否预训练以及L2正则化的影响。观测3：预训练和L2正则化影响不大</dt><dd><ul class="simple">
<li><p>橙线 r2d2_rbs2e4_rbs2e4</p></li>
<li><p>蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2</p></li>
<li><p>粉红线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_nol2</p></li>
<li><p>深红线 pong_r2d3_r2d2expert_k100_pho1-4_rbs2e4_1td_l2</p></li>
<li><p>绿线 pong_r2d3_r2d2expert_k100_pho1-4_rbs2e4_1td_nol2</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_l2_pretrain.png"><img alt="../_images/r2d3_pong_l2_pretrain.png" class="align-center" src="../_images/r2d3_pong_l2_pretrain.png" style="width: 683.0px; height: 160.0px;" /></a>
</li>
</ul>
</div>
<div class="section" id="id10">
<h3>参考资料<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Paine T L, Gulcehre C, Shahriari B, et al. Making efficient use of demonstrations to solve hard exploration problems[J]. arXiv preprint arXiv:1909.01387, 2019.</p></li>
<li><p>Kapturowski S, Ostrovski G, Quan J, et al. Recurrent experience replay in distributed reinforcement learning[C]//International conference on learning representations. 2018.</p></li>
<li><p>Badia A P, Sprechmann P, Vitvitskyi A, et al. Never give up: Learning directed exploration strategies[J]. arXiv preprint arXiv:2002.06038, 2020.</p></li>
<li><p>Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">https://arxiv.org/abs/1810.12894v1</a>. arXiv:1810.12894, 2018.</p></li>
<li><p>Pathak D, Agrawal P, Efros A A, et al. Curiosity-driven exploration by self-supervised prediction[C]//International conference on machine learning. PMLR, 2017: 2778-2787.</p></li>
<li><p>Piot, B.; Geist, M.; and Pietquin, O. 2014a. Boosted bellman residual minimization handling expert demonstrations. In European Conference on Machine Learning (ECML).</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mbpo.html" class="btn btn-neutral float-right" title="MBPO" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="guided_cost_zh.html" class="btn btn-neutral float-left" title="Guided Cost Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>