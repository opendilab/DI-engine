

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CQL &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TD3BC" href="td3_bc.html" />
    <link rel="prev" title="SAC" href="sac.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>CQL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/cql.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cql">
<h1>CQL<a class="headerlink" href="#cql" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Offline reinforcement learning (RL) is a re-emerging area of study that aims to learn behaviors using large, previously collected
datasets, without further environment interaction. It has the potential to make tremendous progress in a number of real-world decision-making problems where active data collection is expensive (e.g., in robotics, drug discovery, dialogue generation, recommendation systems) or unsafe/dangerous (e.g., healthcare, autonomous driving, or education).
Besides, the quantities of data that can be gathered
online are substantially lower than the offline datasets. Such a paradigm promises to resolve a key challenge to bringing reinforcement learning algorithms out of constrained lab settings to the real world.</p>
<p>However, directly utilizing existing value-based off-policy RL algorithms in an offline setting generally results
in poor performance, due to issues with bootstrapping from out-of-distribution actions and overfitting. Thus, many constrain techniques are added to basic online RL algorithms.
Conservative Q-learning (CQL), first proposed in <a class="reference external" href="https://arxiv.org/abs/2006.04779">Conservative Q-Learning for Offline Reinforcement Learning</a>, is one of them which learns conservative Q functions of which the expected value is lower-bounded
via a simple modification to standard value-based RL algorithms.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>CQL is an offline RL algorithm.</p></li>
<li><p>CQL can be implemented with less than 20 lines of code on top of a
number of standard, online RL algorithms</p></li>
<li><p>CQL supports both <strong>discrete</strong> and <strong>continuous</strong> action spaces.</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>CQL can be implemented with less than 20 lines of code on top of a
number of standard, online RL algorithms, simply by adding the CQL regularization terms to
the Q-function update.</p>
<p>In general, for the conservative off-policy evaluation, the Q-function is trained via an iterative update:</p>
<a class="reference internal image-reference" href="../_images/cql_policy_evaluation.png"><img alt="../_images/cql_policy_evaluation.png" class="align-center" src="../_images/cql_policy_evaluation.png" style="width: 700.7px; height: 103.4px;" /></a>
<p>Taking a closer look at the above equation, it consists of two parts - the regularization term and the usual Bellman error with a tradeoff factor alpha. Inside the the regularization term, the first term always pushes the Q value down on the (s,a) pairs sampled from <span class="math notranslate nohighlight">\(\mu\)</span> whereas the second term pushes Q value up on the (s,a) samples drawn from the offline data set.</p>
<p>According to the following theorem, the above equation lower-bounds the expected value under the policy <span class="math notranslate nohighlight">\(\pi\)</span>, when <span class="math notranslate nohighlight">\(\mu\)</span> = <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>For suitable <span class="math notranslate nohighlight">\(\alpha\)</span>, the bound holds under sampling
error and function approximation. We also note that as more data becomes available and |D(s; a)| increases, the theoretical value of <span class="math notranslate nohighlight">\(\alpha\)</span> that is needed to guarantee a lower bound decreases, which
indicates that in the limit of infinite data, a lower bound can be obtained by using extremely small
values of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Note that the analysis presented below assumes that no function approximation is used in the Q-function,
meaning that each iterate can be represented exactly. the result in this theorem can be further generalized to the case of both linear function approximators and non-linear neural network function
approximators, where the latter builds on the neural tangent kernel (NTK) framework. For more details, please refer to the Theorem D.1 and Theorem D.2 in Appendix D.1 in the original paper.</p>
<a class="reference internal image-reference" href="../_images/cql_theorem.png"><img alt="../_images/cql_theorem.png" class="align-center" src="../_images/cql_theorem.png" style="width: 1169.3000000000002px; height: 363.00000000000006px;" /></a>
<p>So, how should we utilize this for policy optimization? We could alternate between performing full off-policy evaluation for each policy iterate, <span class="math notranslate nohighlight">\(\hat{\pi}^{k}(a|s)\)</span>, and one
step of policy improvement. However, this can be computationally expensive. Alternatively, since the
policy <span class="math notranslate nohighlight">\(\hat{\pi}^{k}(a|s)\)</span> is typically derived from the Q-function, we could instead choose <span class="math notranslate nohighlight">\(\mu(a|s)\)</span> to approximate
the policy that would maximize the current Q-function iterate, thus giving rise to an online algorithm. So, for a complete offline RL algorithm, Q-function in general updates as follows:</p>
<a class="reference internal image-reference" href="../_images/cql_general_3.png"><img alt="../_images/cql_general_3.png" class="align-center" src="../_images/cql_general_3.png" style="width: 680.9000000000001px; height: 101.2px;" /></a>
<p>where <span class="math notranslate nohighlight">\(CQL(R)\)</span> is characterized by a particular choice of regularizer <span class="math notranslate nohighlight">\(R(\mu)\)</span>. If <span class="math notranslate nohighlight">\(R(\mu)\)</span> is chosen to be the KL-divergence against a prior distribution, <span class="math notranslate nohighlight">\(\rho(a|s)\)</span>, then we get <span class="math notranslate nohighlight">\(\mu(a|s)\approx \rho(a|s)exp(Q(s,a))\)</span>. Firstly, if <span class="math notranslate nohighlight">\(\rho(a|s)\)</span> = Unif(a), then the first term above corresponds to a soft-maximum
of the Q-values at any state s and gives rise to the following variant, called CQL(H):</p>
<a class="reference internal image-reference" href="../_images/cql_equation_4.png"><img alt="../_images/cql_equation_4.png" class="align-center" src="../_images/cql_equation_4.png" style="width: 899.8000000000001px; height: 81.4px;" /></a>
<p>Secondly, if <span class="math notranslate nohighlight">\(\rho(a|s)\)</span> is chosen to be the previous policy <span class="math notranslate nohighlight">\(\hat{\pi}^{k-1}\)</span>, the first term in Equation (4) above is replaced by
an exponential weighted average of Q-values of actions from the chosen <span class="math notranslate nohighlight">\(\hat{\pi}^{k-1}(a|s)\)</span>.</p>
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<p>The pseudo-code is shown in Algorithm 1, with differences from conventional actor critic algorithms (e.g., SAC) and deep Q-learning algorithms (e.g.,DQN) in red</p>
<a class="reference internal image-reference" href="../_images/cql.png"><img alt="../_images/cql.png" class="align-center" src="../_images/cql.png" style="width: 547.8000000000001px; height: 276.1px;" /></a>
<p>The equation (4) in above pseudo-code is:</p>
<a class="reference internal image-reference" href="../_images/cql_equation_4.png"><img alt="../_images/cql_equation_4.png" class="align-center" src="../_images/cql_equation_4.png" style="width: 654.4000000000001px; height: 59.2px;" /></a>
<p>Note that during implementation, the first term in the equation (4) will be computed under <cite>torch.logsumexp</cite>, which consumes lots of running time.</p>
</div>
<div class="section" id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The default config of CQLPolicy is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.cql.</span></span><span class="sig-name descname"><span class="pre">CQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/cql.html#CQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of CQL algorithm.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 33%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>td3</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 10000 for</div>
<div class="line">SAC, 25000 for DDPG/</div>
<div class="line">TD3.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.policy_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for policy</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.soft_q_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for soft q</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.value_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for value</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to None when</div>
<div class="line">model.value_network</div>
<div class="line">is False.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_q</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for soft q</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3, when</div>
<div class="line">model.value_network</div>
<div class="line">is True.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_policy</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for policy</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3, when</div>
<div class="line">model.value_network</div>
<div class="line">is True.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_value</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for policy</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to None when</div>
<div class="line">model.value_network</div>
<div class="line">is False.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.2</p></td>
<td><div class="line-block">
<div class="line">Entropy regularization</div>
<div class="line">coefficient.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">alpha is initiali-</div>
<div class="line">zation for auto</div>
<div class="line"><cite>alpha</cite>, when</div>
<div class="line">auto_alpha is True</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.repara_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">meterization</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Determine whether to use</div>
<div class="line">reparameterization trick.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to use</div>
<div class="line">auto temperature parameter</div>
<div class="line"><cite>alpha</cite>.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Temperature parameter</div>
<div class="line">determines the</div>
<div class="line">relative importance</div>
<div class="line">of the entropy term</div>
<div class="line">against the reward.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in halfcheetah env.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver</div>
<div class="line">aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.cql.</span></span><span class="sig-name descname"><span class="pre">CQLDiscretePolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/cql.html#CQLDiscretePolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of CQL algorithm in discrete environments.</p>
</dd>
</dl>
<p>Config:
== ==================== ======== ============== ======================================== =======================
ID Symbol               Type     Default Value  Description                              Other(Shape)
== ==================== ======== ============== ======================================== =======================
1  <code class="docutils literal notranslate"><span class="pre">type</span></code>             str      qrdqn          | RL policy register name, refer to      | this arg is optional,</p>
<blockquote>
<div><div class="line-block">
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code>           | a placeholder</div>
</div>
</div></blockquote>
<dl>
<dt>2  <code class="docutils literal notranslate"><span class="pre">cuda</span></code>             bool     False          | Whether to use cuda for network        | this arg can be diff-</dt><dd><div class="line-block">
<div class="line">erent from modes</div>
</div>
</dd>
<dt>3  <code class="docutils literal notranslate"><span class="pre">on_policy</span></code>        bool     False          | Whether the RL algorithm is on-policy</dt><dd><div class="line-block">
<div class="line">or off-policy</div>
</div>
</dd>
<dt>4  <code class="docutils literal notranslate"><span class="pre">priority</span></code>         bool     True           | Whether use priority(PER)              | priority sample,</dt><dd><div class="line-block">
<div class="line">update priority</div>
</div>
</dd>
<dt>6  | <code class="docutils literal notranslate"><span class="pre">other.eps</span></code>      float    0.05           | Start value for epsilon decay. It’s</dt><dd><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">.start</span></code>                                 | small because rainbow use noisy net.</div>
</div>
</dd>
<dt>7  | <code class="docutils literal notranslate"><span class="pre">other.eps</span></code>      float    0.05           | End value for epsilon decay.</dt><dd><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">.end</span></code></div>
</div>
</dd>
<dt>8  | <code class="docutils literal notranslate"><span class="pre">discount_</span></code>      float    0.97,          | Reward’s future discount factor, aka.  | may be 1 when sparse</dt><dd><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code>                  [0.95, 0.999]  | gamma                                  | reward env</div>
</div>
</dd>
<dt>9  <code class="docutils literal notranslate"><span class="pre">nstep</span></code>            int      3,             | N-step reward discount sum for target</dt><dd><p>[3, 5]         | q_value estimation</p>
</dd>
<dt>10 | <code class="docutils literal notranslate"><span class="pre">learn.update</span></code>   int      3              | How many updates(iterations) to train  | this args can be vary</dt><dd><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code>                            | after collector’s one collection. Only | from envs. Bigger val
| valid in serial training               | means more off-policy</div>
</div>
</dd>
</dl>
<p>11 <code class="docutils literal notranslate"><span class="pre">learn.kappa</span></code>      float    /              | Threshold of Huber loss
== ==================== ======== ============== ======================================== =======================</p>
</dd></dl>

</div>
<div class="section" id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p>
<p>(Medium Expert)</p>
</td>
<td><p>57.6
<span class="math notranslate nohighlight">\(\pm\)</span>
3.7</p></td>
<td><img alt="../_images/halfcheetah_cql.png" src="../_images/halfcheetah_cql.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/config/halfcheetah_cql_medium_expert_config.py">config_link_ha</a></p></td>
<td><p>CQL Repo (75.6
<span class="math notranslate nohighlight">\(\pm\)</span> 25.7)</p></td>
</tr>
<tr class="row-odd"><td><p>Walker2d</p>
<p>(Medium Expert)</p>
</td>
<td><p>109.7
<span class="math notranslate nohighlight">\(\pm\)</span>
0.8</p></td>
<td><img alt="../_images/walker2d_cql.png" src="../_images/walker2d_cql.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/config/walker2d_cql_medium_expert_config.py">config_link_w</a></p></td>
<td><p>CQL Repo (107.9
<span class="math notranslate nohighlight">\(\pm\)</span> 1.6)</p></td>
</tr>
<tr class="row-even"><td><p>Hopper</p>
<p>(Medium Expert)</p>
</td>
<td><p>85.4
<span class="math notranslate nohighlight">\(\pm\)</span>
14.8</p></td>
<td><img alt="../_images/hopper_cql.png" src="../_images/hopper_cql.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/config/hopper_sac_cql_medium_expert_config.py">config_link_ho</a></p></td>
<td><p>CQL Repo (105.6
<span class="math notranslate nohighlight">\(\pm\)</span> 12.9)</p></td>
</tr>
</tbody>
</table>
<p>Specifically for each dataset, our implementation results are as follows:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>random</p></th>
<th class="head"><p>medium replay</p></th>
<th class="head"><p>medium expert</p></th>
<th class="head"><p>medium</p></th>
<th class="head"><p>expert</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p></td>
<td><p>18.7 <span class="math notranslate nohighlight">\(\pm\)</span>
1.2</p></td>
<td><p>47.1 <span class="math notranslate nohighlight">\(\pm\)</span>
0.3</p></td>
<td><p>57.6 <span class="math notranslate nohighlight">\(\pm\)</span>
3.7</p></td>
<td><p>49.7 <span class="math notranslate nohighlight">\(\pm\)</span>
0.4</p></td>
<td><p>75.1 <span class="math notranslate nohighlight">\(\pm\)</span>
18.4</p></td>
</tr>
<tr class="row-odd"><td><p>Walker2d</p></td>
<td><p>22.0 <span class="math notranslate nohighlight">\(\pm\)</span>
0.0</p></td>
<td><p>82.6 <span class="math notranslate nohighlight">\(\pm\)</span>
3.4</p></td>
<td><p>109.7 <span class="math notranslate nohighlight">\(\pm\)</span>
0.8</p></td>
<td><p>82.4 <span class="math notranslate nohighlight">\(\pm\)</span>
1.9</p></td>
<td><p>109.2 <span class="math notranslate nohighlight">\(\pm\)</span>
0.3</p></td>
</tr>
<tr class="row-even"><td><p>Hopper</p></td>
<td><p>3.1 <span class="math notranslate nohighlight">\(\pm\)</span>
2.6</p></td>
<td><p>98.3 <span class="math notranslate nohighlight">\(\pm\)</span>
1.8</p></td>
<td><p>85.4 <span class="math notranslate nohighlight">\(\pm\)</span>
14.8</p></td>
<td><p>79.6 <span class="math notranslate nohighlight">\(\pm\)</span>
8.5</p></td>
<td><p>105.4  <span class="math notranslate nohighlight">\(\pm\)</span>
7.2</p></td>
</tr>
</tbody>
</table>
<p>P.S.：</p>
<ol class="arabic">
<li><p>The above results are obtained by running the same configuration on four different random seeds (5, 10, 20, 30)</p></li>
<li><p>The above benchmark is for HalfCheetah-v2, Hopper-v2, Walker2d-v2.</p></li>
<li><p>The comparison results above is obtained via the paper <a class="reference external" href="https://openreview.net/pdf?id=Y4cs1Z3HnqL">Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning</a>.
The complete table is depicted below</p>
<a class="reference internal image-reference" href="../_images/cql_official.png"><img alt="../_images/cql_official.png" class="align-center" src="../_images/cql_official.png" style="width: 769.6px; height: 451.6px;" /></a>
</li>
<li><p>The above Tensorboard results illustrate the unnormalized results</p></li>
</ol>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Kumar, Aviral, et al. “Conservative q-learning for offline reinforcement learning.” arXiv preprint arXiv:2006.04779 (2020).</p></li>
<li><p>Chenjia Bai, et al. “Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning.”</p></li>
</ul>
</div>
<div class="section" id="other-public-implementations">
<h2>Other Public Implementations<a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/aviralkumar2907/CQL">CQL release repo</a></p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="td3_bc.html" class="btn btn-neutral float-right" title="TD3BC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sac.html" class="btn btn-neutral float-left" title="SAC" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>