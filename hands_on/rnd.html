

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>RND &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="HER" href="her.html" />
    <link rel="prev" title="ATOC" href="atoc.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>RND</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/rnd.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rnd">
<h1>RND<a class="headerlink" href="#rnd" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>RND (Random Network Distillation) was first proposed in
<a class="reference external" href="https://arxiv.org/abs/1810.12894v1">Exploration by Random Network Distillation</a> , which introduces
an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal
overhead to the computation performed. The exploration bonus is the error of a neural network predicting features
of the observations given by a fixed randomly initialized neural network. RND claims that it is the first method that achieves
better than average human performance on Montezuma’s Revenge without using demonstrations or having access to the underlying state of the game.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>The insight behind exploration approaches is that we first establish a method to measure the <strong>novelty of states</strong>, namely,
how well we know this state, or the number of times we have visited a state similar to it.
Then we assign a exploration reward in proportional to the novelty measure of the state.
If the visited state is more novel, or say the state is explored very few times, the agent will get a bigger intrinsic reward. On the contrary,
if the agent is more familiar with this state, or say, the state have been explored many times,
the agent will get a smaller intrinsic reward on this state.</p></li>
<li><p>RND is a <strong>prediction-error-based</strong> exploration approach that can be applied in non-tabular cases.
The main idea of prediction-error-based approaches is that defining the intrinsic reward as the prediction error
for a problem related to the agent’s transitions, such as learning forward dynamics model, learning
inverse dynamics model, or even a randomly generated problem, which is the case in RND algorithm.</p></li>
<li><p>RND involves <strong>two neural networks</strong>: a fixed and randomly initialized target network which sets the prediction problem,
and a predictor network trained on data collected by the agent.</p></li>
<li><p>In RND paper, the underlying base RL algorithm is off-policy PPO. Generally, RND intrinsic reward generation model can be combined with
many different RL algorithms such as DDPG, TD3, SAC conveniently.</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>The following two graphs are from <a class="reference external" href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">OpenAI’s blog</a>.
The overall sketch of RND is as follows:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/rnd.png"><img alt="" src="../_images/rnd.png" style="width: 443.5px; height: 229.0px;" /></a>
</div>
<p>The overall sketch of next_sate_prediction exploration method is as follows:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/rnd_next_state_prediction.png"><img alt="" src="../_images/rnd_next_state_prediction.png" style="width: 442.5px; height: 230.0px;" /></a>
</div>
<p>In RND paper, the authors point out that prediction errors can be attributed to the following factors:</p>
<ol class="arabic simple">
<li><p><strong>Amount of training data</strong>. Prediction error is high where few similar examples were seen by the predictor.</p></li>
<li><p><strong>Stochasticity</strong>. Prediction error is high because the target function is stochastic. Stochastic transitions are a source of such error for forward dynamics prediction.</p></li>
<li><p><strong>Model misspecification</strong>. Prediction error is high because information necessary for the prediction is missing, or the model class of predictors is too limited to fit the complexity of the target function.</p></li>
</ol>
<p>Factor 1 is a useful source of error since it quantifies the novelty of experience, whereas factors 2 and 3 cause the noisy-TV problem, that is
serious in the next_sate_prediction based exploration method.
RND obviates factors 2 and 3 since the target network is chosen to be deterministic and has the identical network structure with
the model-class of the predictor network.</p>
<p>In RND, the target network takes an observation to an embedding <span class="math notranslate nohighlight">\(f: O → R^k\)</span> and the predictor neural network <span class="math notranslate nohighlight">\(\hat{f}: O → R^k\)</span> is trained by
gradient descent to minimize the expected MSE <span class="math notranslate nohighlight">\(|f (x; θ) − f (x)|\)</span> with respect to its parameters <span class="math notranslate nohighlight">\(θ_\hat{f}\)</span>.
In unfamiliar states it’s hard to predict the output of the fixed randomly initialized neural network, and hence the prediction error is higher, so RND can consider
the prediction error as a measure of the novelty of visited states.</p>
<p>But RND is still facing some problems, one of which is that the RND bonus reward could gradually disappear over time, because with the training progresses advancing,
the predictor network can fit the output of the randomly initialized neural network better and better.
Interested readers can refer to the follow-up improvement work <a class="reference external" href="https://arxiv.org/abs/2002.06038">Never Give Up: Learning Directed Exploration Strategies</a>
and <a class="reference external" href="https://arxiv.org/abs/2003.13350">Agent57: Outperforming the Atari Human Benchmark</a>.</p>
<div class="section" id="the-implementation-details-that-matters">
<h3>The implementation details that matters<a class="headerlink" href="#the-implementation-details-that-matters" title="Permalink to this headline">¶</a></h3>
<p>1. <strong>intrinsic reward normalization and weight factors</strong>.
Normalize the intrinsic reward by the min-max normalization method, i.e.,
first minus the mini-batch min and divide it by the difference between the mini-batch max and the the mini-batch min. After the min-max normalization,
the RND intrinsic reward is scaled into [0,1]. And we should also use some weight factor to control the balance of exploration and exploitation.
For MiniGrid, in each episode, we let the last non-zero positive original reward times 1000 (more general, the weight factor could be the max length of the game) as
the final fusion-reward to enlarge the effect of its original goal. And our experiment results in minigrid empty8 here shows that the balance of exploration and exploitation
(here in exploration RL alg, i.e. the weight factor of intrinsic reward) is critical to achieve good performance in MiniGrid envs.</p>
<p>2.  <strong>observation normalization</strong>.
Whiten each dimension by subtracting the running mean and then dividing by the running standard deviation.
Then clip the normalized observations to be between -5 and 5.
Initialize the normalization parameters by stepping a random agent in the environment for a small number of steps before beginning optimization.
Use the same observation normalization for both predictor and target networks but not the policy network.</p>
<p>3. <strong>Non-episodic intrinsic reward and two value heads</strong>.
Non-episodic setting (which means the return is not truncated at “game over” status) resulted in more exploration than the episodic setting when exploring without any extrinsic rewards.
In order to combine episodic and non-episodic reward streams, the rnd author recommend two value heads.
When combing the episodic and non-episodic returns, a higher discount factor for the extrinsic rewards leads to better performance,
while for intrinsic rewards it hurts exploration. So the rnd author recommend to set discount factor as 0.999 for extrinsic rewards and 0.99 for intrinsic rewards.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In our implementation, for simplicity, we don’t adopt the following tricks: non-episodic intrinsic reward, two value heads and different discount factors.</p>
</div>
<p>In order to understand the reasons behind these tricks more clearly, it is recommended to read the original <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">rnd paper</a> .</p>
</div>
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-Code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/rnd_pseudo_code.png"><img alt="" src="../_images/rnd_pseudo_code.png" style="width: 500.4px; height: 449.4px;" /></a>
</div>
</div>
<div class="section" id="code-implementation">
<h2>Code Implementation<a class="headerlink" href="#code-implementation" title="Permalink to this headline">¶</a></h2>
<p>The interface of RND reward model is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.reward_model.rnd_reward_model.</span></span><span class="sig-name descname"><span class="pre">RndRewardModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tb_logger</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">SummaryWriter</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/reward_model/rnd_reward_model.html#RndRewardModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">estimate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/reward_model/rnd_reward_model.html#RndRewardModel.estimate"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Rewrite the reward key in each row of the data.</p>
</dd></dl>

</dd></dl>

<p>The interface of on policy PPO is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.ppo.</span></span><span class="sig-name descname"><span class="pre">PPOPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of on policy version PPO algorithm.</p>
</dd>
</dl>
</dd></dl>

<p>Note: <code class="docutils literal notranslate"><span class="pre">...</span></code> indicates the omitted code snippet. For the complete code, please refer to our
<a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py">implementation</a> in DI-engine.</p>
<div class="section" id="rndnetwork">
<h3>RndNetwork<a class="headerlink" href="#rndnetwork" title="Permalink to this headline">¶</a></h3>
<p>First, we define the class <code class="docutils literal notranslate"><span class="pre">RndNetwork</span></code> involves two neural networks: the fixed and randomly initialized target network <code class="docutils literal notranslate"><span class="pre">self.target</span></code>,
and the predictor network <code class="docutils literal notranslate"><span class="pre">self.predictor</span></code> trained on data collected by the agent.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RndNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceType</span><span class="p">],</span> <span class="n">hidden_size_list</span><span class="p">:</span> <span class="n">SequenceType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
       <span class="nb">super</span><span class="p">(</span><span class="n">RndNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
       <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">FCEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">FCEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
       <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">ConvEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">ConvEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
               <span class="s2">&quot;not support obs_shape for pre-defined encoder: </span><span class="si">{}</span><span class="s2">, please customize your own RND model&quot;</span><span class="o">.</span>
               <span class="nb">format</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span>
           <span class="p">)</span>
       <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
           <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
       <span class="n">predict_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
       <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
           <span class="n">target_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
       <span class="k">return</span> <span class="n">predict_feature</span><span class="p">,</span> <span class="n">target_feature</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="rndrewardmodel">
<h3>RndRewardModel<a class="headerlink" href="#rndrewardmodel" title="Permalink to this headline">¶</a></h3>
<p>Then, we initialize reward model, optimizer and self._running_mean_std_rnd in <code class="docutils literal notranslate"><span class="pre">_init_</span></code> of class <code class="docutils literal notranslate"><span class="pre">RndRewardModel</span></code>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RndRewardModel</span><span class="p">(</span><span class="n">BaseRewardModel</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">EasyDict</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">:</span> <span class="s1">&#39;SummaryWriter&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># noqa</span>
       <span class="o">...</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">reward_model</span> <span class="o">=</span> <span class="n">RndNetwork</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size_list</span><span class="p">)</span>
       <span class="o">...</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_model</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">config</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
       <span class="o">...</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd</span> <span class="o">=</span> <span class="n">RunningMeanStd</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="train-rndrewardmodel">
<h3>Train RndRewardModel<a class="headerlink" href="#train-rndrewardmodel" title="Permalink to this headline">¶</a></h3>
<p>Afterwards, we calculate the reward model loss and update the RND predictor network: <code class="docutils literal notranslate"><span class="pre">self.reward_model.predictor</span></code>.  Note that, according to the
original paper, we adopt the observation normalization trick that is transforming the original observations to mean 0, std 1, and clip the normalized observations
to be between -5 and 5, which is empirically important especially when using a random network as a target, for the detailed explanation, please refer the chapter 2.4 in the RND paper.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
       <span class="n">train_data</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
       <span class="n">train_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

       <span class="c1"># observation normalization:  transform to mean 0, std 1</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd_obs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
       <span class="n">train_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_data</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd_obs</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span> <span class="o">/</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd_obs</span><span class="o">.</span><span class="n">std</span><span class="p">)</span>
       <span class="n">train_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

       <span class="n">predict_feature</span><span class="p">,</span> <span class="n">target_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_model</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
       <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">predict_feature</span><span class="p">,</span> <span class="n">target_feature</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
       <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="calculate-rnd-reward">
<h3>Calculate RND Reward<a class="headerlink" href="#calculate-rnd-reward" title="Permalink to this headline">¶</a></h3>
<p>Finally, we calculate MSE loss according to the RND reward model and do the necessary subsequent processing
and rewrite the reward key in the data in <code class="docutils literal notranslate"><span class="pre">estimate</span></code> method of class <code class="docutils literal notranslate"><span class="pre">RndRewardModel</span></code>. And note that
we adapt the reward normalization trick that is transforming the original RND reward to (mean 0, std 1), empirically we found this normalization way works well
than only dividing the self._running_mean_std_rnd.std in some sparse reward environment, such as minigrid.</p>
<blockquote>
<div><ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">calculate</span> <span class="pre">the</span> <span class="pre">RND</span> <span class="pre">pseudo</span> <span class="pre">reward</span></code></p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="n">collect_states</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># observation normalization:  transform to mean 0, std 1</span>
<span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs</span> <span class="o">-</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd_obs</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span> <span class="o">/</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd_obs</span><span class="o">.</span><span class="n">std</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predict_feature</span><span class="p">,</span> <span class="n">target_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_model</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">predict_feature</span><span class="p">,</span> <span class="n">target_feature</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_running_mean_std_rnd</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="c1"># reward normalization: transform to [0,1], empirically we found this normalization way works well</span>
    <span class="c1"># than only dividing the self._running_mean_std_rnd.std</span>
    <span class="n">rnd_reward</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">reward</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">reward</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-11</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>2. <code class="docutils literal notranslate"><span class="pre">combine</span> <span class="pre">the</span> <span class="pre">RND</span> <span class="pre">pseudo</span> <span class="pre">reward</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">original</span> <span class="pre">reward</span></code>. Here, we should also use some weight factor to control the
balance of exploration and exploitation. For minigrid, we let the last non-zero original reward times 1000 to enlarge the effect
of original goal. We also conduct the experiment on minigrid empty8 to verify the importance of tradeoff between the exploration
and exploitation. In the experiment, we found that if we don’t use the weight factor 1000, the rnd agent can’t learn to reach the goal at all because
the proportion of exploration is too large.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">rnd_rew</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
   <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">intrinsic_reward_type</span> <span class="o">==</span> <span class="s1">&#39;add&#39;</span><span class="p">:</span>
       <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rew</span>
       <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># for minigrid</span>
           <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">rnd_rew</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rnd_rew</span>
   <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">intrinsic_reward_type</span> <span class="o">==</span> <span class="s1">&#39;new&#39;</span><span class="p">:</span>
       <span class="n">item</span><span class="p">[</span><span class="s1">&#39;intrinsic_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rnd_rew</span>
   <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">intrinsic_reward_type</span> <span class="o">==</span> <span class="s1">&#39;assign&#39;</span><span class="p">:</span>
       <span class="n">item</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rnd_rew</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
</div>
</div>
<div class="section" id="benchmark-results">
<h2>Benchmark Results<a class="headerlink" href="#benchmark-results" title="Permalink to this headline">¶</a></h2>
<p>Because in collection phase, we use multinomial_sample to increase the diversity of collected data,
in evaluation phase, we use the argmax action to interact with env, and we run 5 different seeds and report the mean reward.
In the experimental results below, the x-axis denotes the total env-steps interacting with the env in the training process.
In the graph labeled “collector_step”, the y-axis shows the rewards received during the collection phase, denoted as collect_reward;
in the graph labeled “evaluator_step”, the y-axis shows the rewards obtained during the evaluation phase, denoted as eval_reward.
In the sparse reward envs minigrid, only when the agent reach the goal, the agent will get a positive reward, zero otherwise, and its value will be inversely proportional to the steps used to reach the goal.
In the easiest env MiniGrid-Empty-8x8-v0, different seeds will have the same room configurations, the optimal reward is about 0.96.
But in MiniGrid-FourRooms-v0, different seeds will have different room configurations
and corresponding optimal reward is also different, when the mean of eval_reward is greater than 0.6, we consider the env have been solved.</p>
<ul>
<li><p>MiniGrid-Empty-8x8-v0（40k env steps，eval reward_mean&gt;0.95）</p>
<ul class="simple">
<li><p>green line is rnd-onppo-weight100</p></li>
<li><p>red line is onppo</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/rnd_empty8_rnd-weight100_vs_onppo_collect_mean.png"><img alt="../_images/rnd_empty8_rnd-weight100_vs_onppo_collect_mean.png" class="align-center" src="../_images/rnd_empty8_rnd-weight100_vs_onppo_collect_mean.png" style="width: 273.0px; height: 188.0px;" /></a>
<a class="reference internal image-reference" href="../_images/rnd_empty8_rnd-weight100_vs_onppo_eval_mean.png"><img alt="../_images/rnd_empty8_rnd-weight100_vs_onppo_eval_mean.png" class="align-center" src="../_images/rnd_empty8_rnd-weight100_vs_onppo_eval_mean.png" style="width: 277.0px; height: 192.0px;" /></a>
<ul class="simple">
<li><p>green line is rnd-onppo-weight100</p></li>
<li><p>grey line is rnd-onppo-noweight</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/rnd_empty8_weight100_vs_noweight_collect_mean.png"><img alt="../_images/rnd_empty8_weight100_vs_noweight_collect_mean.png" class="align-center" src="../_images/rnd_empty8_weight100_vs_noweight_collect_mean.png" style="width: 278.0px; height: 193.0px;" /></a>
</li>
<li><p>MiniGrid-FourRooms-v0（20M env steps，eval reward_mean&gt;0.6）</p>
<ul class="simple">
<li><p>red line is onppo</p></li>
<li><p>grey line is rnd-onppo-weight1000</p></li>
<li><p>green line is rnd-onppo-weight100</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/rnd_fourrooms_weight1000_weight100_onppo_collect_mean.png"><img alt="" class="align-center" src="../_images/rnd_fourrooms_weight1000_weight100_onppo_collect_mean.png" style="width: 275.0px; height: 191.0px;" /></a>
<a class="reference internal image-reference" href="../_images/rnd_fourrooms_weight1000_weight100_onppo_eval_mean.png"><img alt="" class="align-center" src="../_images/rnd_fourrooms_weight1000_weight100_onppo_eval_mean.png" style="width: 281.0px; height: 195.0px;" /></a>
<a class="reference internal image-reference" href="../_images/rnd_fourrooms_weight1000_weight100_onppo_eval_min.png"><img alt="" class="align-center" src="../_images/rnd_fourrooms_weight1000_weight100_onppo_eval_min.png" style="width: 279.0px; height: 192.0px;" /></a>
<p>We can found that in rnd using the weight factor 1000 is much better than using 100. We hypothesis that it’s due to the episode length
needed in fourrooms to solve the game is larger than in empty8 on average, so the total cumulated discounted intrinsic rewards in fourrooms is larger than in empty8.
we should make sure the original reward to be comparable with intrinsic reward to trade-off between exploitation and exploration. This reminds us that the weight factor of original reward
should be related to the total time-steps of solving the game.</p>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>How to determine the relative weight between the intrinsic reward and the original extrinsic reward can be valuable work in the future.</p>
</div>
</div>
<div class="section" id="author-s-tensorflow-implementation">
<h2>Author’s Tensorflow Implementation<a class="headerlink" href="#author-s-tensorflow-implementation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/openai/random-network-distillation.">RND</a></p></li>
</ul>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">https://arxiv.org/abs/1810.12894v1</a>. arXiv:1810.12894, 2018.</p></li>
<li><p><a class="reference external" href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/</a></p></li>
<li><p><a class="reference external" href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html#prediction-based-exploration">https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html#prediction-based-exploration</a></p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="her.html" class="btn btn-neutral float-right" title="HER" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="atoc.html" class="btn btn-neutral float-left" title="ATOC" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>