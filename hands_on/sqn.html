

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>SQN &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="R2D2" href="r2d2.html" />
    <link rel="prev" title="SQL" href="sql.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>SQN</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/sqn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sqn">
<h1>SQN<a class="headerlink" href="#sqn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Soft Q-learning is an extension from many previous work, including:
Leverage the Average: an Analysis of KL Regularization in Reinforcement
Learning , Soft Actor-Critic Algorithms and Applications, Equivalence
Between Policy Gradients and Soft Q-Learning and so on. It an off-policy
Q-learning algorithm, but adopt the maximum entropy framework, make an
connection between Policy Gradients and Q-learning.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>SQN is implemented for environments with <strong>Discrete</strong> action
spaces.(i.e. Atari, Go)</p></li>
<li><p>SQN is an <strong>off-policy</strong> and <strong>model-free</strong> algorithm, and use
Boltzmann policy to do exploration.</p></li>
<li><p>SQN is based on <strong>Q-learning</strong> algorithm, which optimizes an
Q-function and construct a policy from Q-function</p></li>
<li><p>SQN is implemented for <strong>Multi-Discrete</strong> action space as well</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>An entropy-regularized version of RL objective could lead to better
exploration and stability. The most general way to define
entropy-augmented return is</p>
<div class="math notranslate nohighlight">
\[\sum_{t=0}^{\infty} \gamma^{t}\left(r_{t}-\tau H_{t}\right),\]</div>
<p><span class="math notranslate nohighlight">\(\bar{\pi}\)</span> is some “reference” policy,
<span class="math notranslate nohighlight">\(\tau\)</span> is a “temperature” parameter, and
<span class="math notranslate nohighlight">\(H\)</span> is the Kullback-Leibler divergence. Note that the temperature
<span class="math notranslate nohighlight">\(\tau\)</span> can be eliminated by re-scaling the rewards. So the definition of
Q-function changing to:</p>
<div class="math notranslate nohighlight">
\[Q^{\pi}(s, a)=\mathbb{E}\left[r_{0}+\sum_{t=1}^{\infty} \gamma^{t}\left(r_{t}-\tau \mathrm{KL}_{t}\right) \mid s_{0}=s, a_{0}=a\right]\]</div>
<p>Additionally, an optimal policy called Boltzmann-Policy can be derived by
varying the action probabilities as a graded function of estimated value:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \pi^Q(\cdot \mid s)
    % &amp;=\underset{\pi}{\arg \max }\left\{\mathbb{E}_{a \sim \pi}[Q(s, a)]-\tau D_{\mathrm{KL}}[\pi \| \bar{\pi}](s)\right\} \\
    &amp;=\frac{\bar{\pi}(a \mid s) \exp (Q(s, a) / \tau)}{\underbrace{\mathbb{E}_{a^{\prime} \sim \bar{\pi}}\left[\exp \left(Q\left(s, a^{\prime}\right) / \tau\right)\right]}_{\text {normalizing constant }}}
    \end{aligned}\end{split}\]</div>
<p>Generally speaking, we can use an uniformly $bar{pi} \sim U$, so $r_t
- \tau \mathrm{KL}_t = r_t + \mathcal{H} - \log{mathcal{N}}$. Thus
optimal policy additionally aims to maximize its entropy at each visited
state and the TD-target becomes:</p>
<div class="math notranslate nohighlight">
\[y_t = r + \gamma\left[Q_{\bar{\theta}}\left(\mathbf{s}', \mathbf{a}'\right)-\tau \log \pi_{\phi}\left(\mathbf{a}' \mid \mathbf{s}')\right]\right.\]</div>
</div>
<div class="section" id="pseudocode">
<h2>Pseudocode<a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h2>
<img alt="../_images/sqn.png" class="align-center" src="../_images/sqn.png" />
<div class="highlight-tex notranslate"><div class="highlight"><pre><span></span><span class="k">\begin</span><span class="nb">{</span>algorithm<span class="nb">}</span>[tp]
<span class="k">\setstretch</span><span class="nb">{</span>1.35<span class="nb">}</span>
<span class="k">\DontPrintSemicolon</span>
<span class="k">\SetAlgoLined</span>
<span class="k">\SetKwInOut</span><span class="nb">{</span>Input<span class="nb">}{</span>Input<span class="nb">}</span><span class="k">\SetKwInOut</span><span class="nb">{</span>Output<span class="nb">}{</span>Output<span class="nb">}</span>
<span class="k">\Input</span><span class="nb">{</span>Initial Q functions parameters <span class="s">$</span><span class="nv">\Theta</span><span class="s">$</span> <span class="k">\\</span>
        Temperature <span class="s">$</span><span class="nv">\tau</span><span class="s">$</span> <span class="k">\\</span>
        Empty replay buffer <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{D}</span><span class="s">$</span> <span class="k">\\</span>
<span class="nb">}</span>

<span class="k">\textbf</span><span class="nb">{</span>Initialize: <span class="nb">}</span>
        <span class="s">$</span><span class="nv">\overline</span><span class="nb">{</span><span class="nv">\theta</span><span class="nb">^i}_</span><span class="m">1</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> {</span><span class="nv">\theta</span><span class="nb">^i}_</span><span class="m">1</span><span class="s">$</span>, <span class="s">$</span><span class="nv">\overline</span><span class="nb">{</span><span class="nv">\theta</span><span class="nb">^i}_</span><span class="m">2</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> {</span><span class="nv">\theta</span><span class="nb">^i}_</span><span class="m">2</span><span class="s">$</span>

<span class="k">\While</span>(Train)<span class="nb">{</span>not converge<span class="nb">}{</span>


        <span class="c">% \tcc{comments on code}</span>
        <span class="k">\For</span>(Collect)<span class="nb">{</span>each environment step<span class="nb">}{</span>
                <span class="s">$</span><span class="nb">a_t </span><span class="nv">\sim</span><span class="nb"> </span><span class="nv">\pi</span><span class="nb">^{Q}</span><span class="o">(</span><span class="nb">a_t|s_t</span><span class="o">)</span><span class="s">$</span> <span class="k">\\</span>
                <span class="s">$</span><span class="nb">s_{t</span><span class="o">+</span><span class="m">1</span><span class="nb">} </span><span class="nv">\sim</span><span class="nb"> p</span><span class="o">(</span><span class="nb">s_{t</span><span class="o">+</span><span class="m">1</span><span class="nb">}|s_t, a_t</span><span class="o">)</span><span class="s">$</span> <span class="k">\\</span>
                <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{D} </span><span class="nv">\cup</span><span class="nb"> </span><span class="nv">\{</span><span class="o">(</span><span class="nb">s_t, a_t, r_t, s_{t</span><span class="o">+</span><span class="m">1</span><span class="nb">}, d</span><span class="nv">\}</span><span class="s">$</span>
                <span class="nb">}</span>

        <span class="k">\For</span>(Update)<span class="nb">{</span>each gradient step<span class="nb">}{</span>
        <span class="s">$</span><span class="nv">\{</span><span class="o">(</span><span class="nb">s, a, r, s^</span><span class="nv">\prime</span><span class="nb">, d</span><span class="o">)</span><span class="nv">\}</span><span class="nb">^B_{i</span><span class="o">=</span><span class="m">1</span><span class="nb">} </span><span class="nv">\sim</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{D}</span><span class="s">$</span>

        Compute Q-loss <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{L}_Q</span><span class="o">(</span><span class="nv">\theta</span><span class="o">)</span><span class="s">$</span>


        <span class="s">$</span><span class="nv">\theta</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> </span><span class="nv">\theta</span><span class="nb"> </span><span class="o">-</span><span class="nb"> </span><span class="nv">\lambda</span><span class="nb">_{</span><span class="nv">\theta</span><span class="nb">} </span><span class="nv">\bigtriangledown</span><span class="nb">_{</span><span class="nv">\theta</span><span class="nb">} </span><span class="nv">\mathcal</span><span class="nb">{L}_Q</span><span class="o">(</span><span class="nv">\theta</span><span class="o">)</span><span class="s">$</span>

        Compute temperature loss <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{L}</span><span class="o">(</span><span class="nv">\tau</span><span class="o">)</span><span class="s">$</span>

        <span class="s">$</span><span class="nv">\tau</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> </span><span class="nv">\tau</span><span class="nb"> </span><span class="o">-</span><span class="nb"> </span><span class="nv">\lambda</span><span class="nb">_{</span><span class="nv">\tau</span><span class="nb">} </span><span class="nv">\bigtriangledown</span><span class="nb">_{</span><span class="nv">\tau</span><span class="nb">} </span><span class="nv">\mathcal</span><span class="nb">{L}</span><span class="o">(</span><span class="nv">\tau</span><span class="o">)</span><span class="s">$</span>

        <span class="c">% Update target network\\</span>
        Update Target: <span class="k">\\</span>
        <span class="s">$</span><span class="nv">\overline</span><span class="nb">{</span><span class="nv">\theta</span><span class="nb">}_j </span><span class="nv">\leftarrow</span><span class="nb"> </span><span class="nv">\rho</span><span class="nb"> {</span><span class="nv">\overline</span><span class="nb">{</span><span class="nv">\theta</span><span class="nb">}}_j </span><span class="o">+</span><span class="nb"> </span><span class="o">(</span><span class="m">1</span><span class="o">-</span><span class="nv">\rho</span><span class="o">)</span><span class="nb"> {</span><span class="nv">\theta</span><span class="nb">}_j, </span><span class="nv">\ \ \text</span><span class="nb">{for} </span><span class="nv">\ </span><span class="nb">j </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\{</span><span class="m">1</span><span class="nb">,</span><span class="m">2</span><span class="nv">\}</span><span class="s">$</span>
        <span class="nb">}</span>
<span class="nb">}</span>
<span class="k">\caption</span><span class="nb">{</span>SQN Algorithm<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>algorithm<span class="nb">}</span>
</pre></div>
</div>
</div>
<div class="section" id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<p>SQN can be combined with:</p>
<ul class="simple">
<li><p>SQN could use a separate policy network, which called
<a class="reference external" href="https://arxiv.org/abs/1910.07207">SAC-Discrete</a></p></li>
<li><p>SQN is closely related to general Regularization Reinforcement
Learning which could have many form, but our implementation utilize
auto adjust temperature and remove many unclear part <a class="reference external" href="https://arxiv.org/abs/2003.14089">Leverage the
Average</a>.</p></li>
<li><p>SQN is using Boltzmann policy for construct policy from Q-function,
it’s although called softmax policy.</p></li>
<li><p>Some analyst draw connection between Soft Q-learning and Policy
Gradient algorithms such as <a class="reference external" href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and
Soft Q-Learning</a>.</p></li>
<li><p>Some recent research treats RL as a problem in probabilistic
inference, like <a class="reference external" href="https://arxiv.org/abs/1806.06920">MPO</a>,
<a class="reference external" href="https://arxiv.org/abs/1909.12238">VMPO</a> they have close
relationship to SQN, SAC and the max-entropy framework, it an
activate area.</p></li>
</ul>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>Soft Q loss</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Target</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">q0_targ</span> <span class="o">=</span> <span class="n">target_q_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">q1_targ</span> <span class="o">=</span> <span class="n">target_q_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">q_targ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">q0_targ</span><span class="p">,</span> <span class="n">q1_targ</span><span class="p">)</span>
      <span class="c1"># discrete policy</span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
      <span class="c1"># TODO use q_targ or q0 for pi</span>
      <span class="n">log_pi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">q_targ</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_pi</span><span class="p">)</span>
      <span class="c1"># v = \sum_a \pi(a | s) (Q(s, a) - \alpha \log(\pi(a|s)))</span>
      <span class="n">target_v_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_targ</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">log_pi</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># q = r + \gamma v</span>
      <span class="n">q_backup</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">target_v_value</span>
      <span class="c1"># alpha_loss</span>
      <span class="n">entropy</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">pi</span> <span class="o">*</span> <span class="n">log_pi</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">expect_entropy</span> <span class="o">=</span> <span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_entropy</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Q loss</span>
<span class="n">q0_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">q0_a</span><span class="p">,</span> <span class="n">q_backup</span><span class="p">)</span>
<span class="n">q1_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">q1_a</span><span class="p">,</span> <span class="n">q_backup</span><span class="p">)</span>
<span class="n">total_q_loss</span> <span class="o">=</span> <span class="n">q0_loss</span> <span class="o">+</span> <span class="n">q1_loss</span>
</pre></div>
</div>
<p>Sample from policy</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">logits</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pi_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Alpha loss</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">entropy</span> <span class="o">-</span> <span class="n">expect_entropy</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="other-public-implementations">
<h2>Other Public Implementations<a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/createamind/DRL/tree/master/spinup/algos/sqn">DRL</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="r2d2.html" class="btn btn-neutral float-right" title="R2D2" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sql.html" class="btn btn-neutral float-left" title="SQL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>