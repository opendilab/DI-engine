

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>AlphaGo &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="RL Environments Tutorial" href="../env_tutorial/index.html" />
    <link rel="prev" title="MCTS" href="mcts.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>AlphaGo</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/alphago.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="alphago">
<h1>AlphaGo<a class="headerlink" href="#alphago" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>AlphaGo and its variation AlphaGoZero can be said to be some of the most eye-catching achievements in the field of deep reinforcement learning in recent years.
Google Deepmind’s AlphaGo defeated world champion Lee Sedol in 2016 and defeated Ke Jie in 2017.
Its improved version, AlphaGoZero, has become an unattainable peak for all Go players, completely changing the game and training methods of the Go world.</p>
<p>AlphaGo’s paper <a class="reference external" href="https://www.nature.com/articles/nature16961">Mastering the game of Go with deep neural networks and tree search</a>
was published by Google DeepMind on nature in 2016.</p>
<p>As the title says, the implementation of the AlphaGo is a combination of deep neural networks (Conv-net) and tree search (MCTS).
Based on MCTS, some improvements have been made,
including the PUCT algorithm proposed by <a class="reference external" href="http://gauss.ececs.uc.edu/Workshops/isaim2010/papers/rosin.pdf">Multi-armed bandits with episode context</a> ,
AlphaGo trained value network to evaluate board positions,
trained policy network to select actions, modified the algorithm to combine value and policy networks with MCTS,
and continuously train neural networks through self-play.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick-Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>AlphaGo can also be viewed as a combination of MCTS and VPN algorithm (which we introduced in <a class="reference external" href="./vpn.html">VPN</a> ).</p></li>
<li><p>In compare to basic MCTS algorithm, AlphaGo used <strong>deep neural network</strong> as the <strong>default policy</strong>, used improved bandit algorithm <strong>PUCT</strong> as the <strong>tree policy</strong>.</p></li>
<li><p>AlphaGo contains two separated neural networks, policy network and value network.</p></li>
<li><p>To solve the problem of low sample efficiency due to large search space, AlphaGo used supervised learning to learn from human expert games in the first stage of the pineline.</p></li>
</ol>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>AlphaGo’s neural network structure:</p>
<img alt="policy network and value network demo" src="../_images/Go-network.jpg" />
<p>AlphaGo’s supervised learning and reinforcement learning process:</p>
<img alt="AlphaGo's supervised learning and reinforcement learning pineline" src="../_images/Go-training.jpg" />
<p>After passing offline-RL and self-play online-RL training, AlphaGo’s level has reached and surpassed the level of top professional Go players:</p>
<img alt="../_images/Go-ranking.jpg" src="../_images/Go-ranking.jpg" />
<p>The PUCT algorithm:</p>
<p><span class="math notranslate nohighlight">\(u(s, a)=c_{\mathrm{puct}} P(s, a) \frac{\sqrt{\sum_{b} N_{r}(s, b)}}{1+N_{r}(s, a)}\)</span></p>
<p>The Monte Carlo Tree Search process of AlphaGo:</p>
<img alt="../_images/Go-mcts.jpg" src="../_images/Go-mcts.jpg" />
<p>The fly in the ointment is that AlphaGo used a lot of Human prior knowledge. In addition to using replays of professional Go players,
The input features include the remaining “qi” of the chess game, some a priori feasible moves, some a priori patterns of the chess game, etc.</p>
<p>The input features of neural network:</p>
<img alt="neural network input-features" src="../_images/Go-nn-input.jpg" />
<p>Rollout and tree policy’s input-features:</p>
<img alt="rollout and tree—policy's input-features" src="../_images/Go-tree-input.jpg" />
<p>Although AlphaGo is performing well, there are still many problems to be improved.
These problems were solved in 2017, as AlphaGoZero was born.</p>
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</div>
<div class="section" id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>AlphaGoZero</p></li>
</ul>
</div>
<div class="section" id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</div>
</div>
<div class="section" id="alphagozero">
<h1>AlphaGoZero<a class="headerlink" href="#alphagozero" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>Overview<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>AlphaGoZero’s paper <a class="reference external" href="https://www.nature.com/articles/nature24270.">Mastering the game of Go without human knowledge</a>
was published by Google DeepMind on nature in 2016.</p>
<p>As the title says, the most significant improvement in AlphaGoZero is that throughout the training process of AlphaGoZero, no Human prior knowledge is needed.</p>
<p>The network input of AlphaGoZero has no other features, apart from the position information of the board.</p>
<p>AlphaGoZero no longer relies on replays of professional players to do supervised learning
but trains a purely self-play deep learning algorithm starting from zero.</p>
<p>The modification of AlphaGoZero is rather simple, that it modified the neural network structures in two aspects:</p>
<blockquote>
<div><p>1.No longer separate value network and policy network, but a dueling network with dueling head (sep/dual).</p>
<p>2.Improve the basic structure of neural network，from basic convolution network “ConvNet” updated to residual network “ResNet” (conv/res).</p>
</div></blockquote>
</div>
<div class="section" id="id2">
<h2>Quick-Facts<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ol class="arabic simple">
<li><p>AlphaGoZero no longer need to learn from human expert games to reduce the exploration space. The algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules.</p></li>
<li><p>AlphaGoZero used a dueling network with dueling head to jointly train value network and policy network, which results in a huge performance gain.</p></li>
<li><p>AlphaGoZero improved the basic structure of neural network，from convolution network to residual network, which results in a huge performance gain.</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id3">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>The ablation study comparing AlphaGo and AlphaGoZero in two aspects(sep/dual and conv/res):</p>
<img alt="../_images/Zero-rating.jpg" class="align-center" src="../_images/Zero-rating.jpg" />
<p>The training process of AlphaGoZero:</p>
<img alt="../_images/Zero-training.jpg" src="../_images/Zero-training.jpg" />
<p>The compare between reinforcement learning and supervised learning using AlphaGoZero’s network structure:</p>
<img alt="../_images/Zero-RL-SL.jpg" src="../_images/Zero-RL-SL.jpg" />
<p>We can see that AlphaGoZero based on reinforcement learning not only achieves better results than supervised learning, but the policy is also more different from the current experts.</p>
</div>
<div class="section" id="id4">
<h2>Pseudo-code<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</div>
<div class="section" id="id5">
<h2>Extensions<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</div>
<div class="section" id="id6">
<h2>Implementations<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>TBD</p></li>
</ul>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>Silver, D., Schrittwieser, J., Simonyan, K. et al. Mastering the game of Go without human knowledge. Nature 550, 354–359 (2017). <a class="reference external" href="https://doi.org/10.1038/nature24270">https://doi.org/10.1038/nature24270</a></p>
<p>Rosin, Christopher D. “Multi-Armed Bandits with Episode Context.” Annals of Mathematics and Artificial Intelligence, vol. 61, no. 3, 2011, pp. 203–230., doi:10.1007/s10472-011-9258-6. http://gauss.ececs.uc.edu/Workshops/isaim2010/papers/rosin.pdf</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../env_tutorial/index.html" class="btn btn-neutral float-right" title="RL Environments Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="mcts.html" class="btn btn-neutral float-left" title="MCTS" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>