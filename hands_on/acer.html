

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ACER &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PPO" href="ppo.html" />
    <link rel="prev" title="A2C" href="a2c.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>ACER</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/acer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="acer">
<h1>ACER<a class="headerlink" href="#acer" title="Permalink to this headline">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>ACER, short for actor-critic with experience replay, presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on
challenging environments, including the discrete 57-game Atari domain and several continuous control problems. It greatly increases
the sample efficiency and decreases the data correlation by using the following tricks:</p>
<ul class="simple">
<li><p><strong>Truncated importance sampling with bias correction</strong>, which controls the stability of the off-policy estimator</p></li>
<li><p><strong>Retrace Q-value estimation</strong>, which is an off-policy, low variance, and return-based algorithm, and has been proven to converge</p></li>
<li><p><strong>Efficient TRPO (Trust Region Policy Optimization)</strong>, which scales well to large problems</p></li>
<li><p><strong>Stochastic Dueling Networks (SDNs)</strong>, which is designed to estimate both <img class="math" src="../_images/math/690c6faf70de5934834ab78b914de963c6a1032b.svg" alt="V^\pi"/> and <img class="math" src="../_images/math/b174e327b35cc02e6d8ada07f1e3385b2cb0056e.svg" alt="Q^\pi"/> off-policy while maintaining consistency between the two estimates</p></li>
</ul>
<p>You can find more details in the paper <a class="reference external" href="https://arxiv.org/abs/1611.01224">Sample Efficient Actor-Critic with Experience Replay</a>.</p>
</section>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>ACER is a <strong>model-free</strong> and <strong>off-policy</strong> RL algorithm.</p></li>
<li><p>ACER supports both <strong>discrete</strong> action spaces and <strong>continuous</strong> action spaces with several differences.</p></li>
<li><p>ACER is an <strong>actor-critic</strong> RL algorithm, which optimizes the actor and critic networks respectively.</p></li>
<li><p>ACER decouples acting from learning. Collectors in ACER need to record <strong>behavior probability distributions</strong>.</p></li>
</ol>
<p>In the following sections, we take the discrete case as an example to elaborate on ACER algorithm.</p>
</section>
<section id="key-equations">
<h2>Key Equations<a class="headerlink" href="#key-equations" title="Permalink to this headline">¶</a></h2>
<p>Loss used in ACER contains policy loss and value loss. They are updated separately, so it’s necessary to control their relative update speeds.</p>
<section id="retrace-q-value-estimation">
<h3>Retrace Q-value estimation<a class="headerlink" href="#retrace-q-value-estimation" title="Permalink to this headline">¶</a></h3>
<p>Given a trajectory generated under the behavior policy <img class="math" src="../_images/math/d53f85e57711cda553a4cb8ae5f52e0227ad49af.svg" alt="\mu"/>,  we retrieve a trajectory <img class="math" src="../_images/math/489944d8b8077cd280cd52140b16ef4f99769b12.svg" alt="{x_0, a_0, r_0, \mu(\cdot|x_0),..., x_k, a_k, r_k, \mu(\cdot|x_k)}"/>
the Retrace estimator can be expressed recursively as follows:</p>
<div class="math">
<p><img src="../_images/math/c3998fa38ab0e2898f3088818f3f485893700ccd.svg" alt="Q^{\text{ret}}(x_t,a_t)=r_t+\gamma\bar{\rho}_{t+1}[Q^{\text{ret}}(x_{t+
1},a_{t+1})]+\gamma V(x_{t+1})"/></p>
</div><p>where  <img class="math" src="../_images/math/281106e3f12a9c759ee6a80b2d6441aa0baf0102.svg" alt="\bar{\rho}_t"/> is the truncated importance weight, <img class="math" src="../_images/math/1da6753921b090a345b6037fd007f75af0d983b8.svg" alt="\bar{\rho}_t=\min\{c,\rho\}"/> with <img class="math" src="../_images/math/6852cd993572da7fa620c3b5979c5be22839749e.svg" alt="\frac{\pi(a_t|x_t)}{\mu(a_t|x_t)}"/>. <img class="math" src="../_images/math/62f4dddfd372f3c595ec5e86cbad69f811c94f15.svg" alt="\pi"/> is the target policy.
Retrace is an off-policy, return based algorithm which has low variance and is proven to converge to the value function of the target policy for any behavior policy.
We approximate the Q value by neural network <img class="math" src="../_images/math/38bca4f0731320f971ba5bc86691c479f5fafbdb.svg" alt="Q_{\theta}"/>. We use a mean squared error loss:</p>
<blockquote>
<div><div class="math">
<p><img src="../_images/math/bdf31c84924d20cf958e9d5c4f59443ff7054054.svg" alt="{\text{value}}=\frac{1}{2}(Q^{\text{ret}}(x_t,a_t)-Q_{\theta}(x_t,a_t))^2."/></p>
</div></div></blockquote>
</section>
<section id="policy-gradient">
<h3>Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permalink to this headline">¶</a></h3>
<p>To safe-guard against high variance, ACER uses truncated importance weights and introduces
correction term via the following decomposition of <img class="math" src="../_images/math/9f81c22c99b263651c71b98fe11928054decf7aa.svg" alt="g^{acer}"/>:</p>
<a class="reference internal image-reference" href="../_images/ACER_gacer.png"><img alt="../_images/ACER_gacer.png" class="align-center" src="../_images/ACER_gacer.png" style="width: 635.0px; height: 112.0px;" /></a>
<p>To ensure stability, ACER limits the per-step change to the policy by solving the following linearized KL divergence constraint:</p>
<div class="math">
<p><img src="../_images/math/a20d716bfdfacdcb28314e806338fc4f959c8f2a.svg" alt="\begin{split}
&amp;\text{minimize}_z\quad\frac{1}{2}\|g_t^{\text{acer}}-z\|_2^2\\
&amp;subjec\ to\quad \nabla_{\phi_{\theta}(x_t)}D_{KL}[f(\cdot|\phi_{\theta_a}(x_t))\|f(\cdot|\phi_{\theta}(x_t))]^\top\le\delta
\end{split}"/></p>
</div><p>The <img class="math" src="../_images/math/44370127d6d9ee2fbb53a1c7aec1cc6bf48dfaab.svg" alt="\phi(\theta)"/> is the target policy network and the <img class="math" src="../_images/math/06cd6fe38482dcdb08e6895e3a6ceb3d696aa7b1.svg" alt="\phi(\theta_a)"/> is the average policy network.
By letting <img class="math" src="../_images/math/a85f7f0c29ae37961e23a356deee477d3a41addd.svg" alt="k=\nabla_{\phi_{\theta}(x_t)}D_{KL}[f(\cdot|\phi_{\theta_a}(x_t))\|f(\cdot|\phi_{\theta}(x_t))]"/>, the solution can be easily derived in closed form using the KKT condition:</p>
<div class="math">
<p><img src="../_images/math/45c54c5d53944cbf359402490ec7df2a2ddb798a.svg" alt="z^*=g_{t}^{\text{acer}}-\max\{0,\frac{k^\top g_t^{\text{acer}}-\delta}{\|k\|_2^2}\}k"/></p>
</div></section>
</section>
<section id="key-graph-or-network-structure">
<h2>Key Graph or Network Structure<a class="headerlink" href="#key-graph-or-network-structure" title="Permalink to this headline">¶</a></h2>
<p>The following graph depicts the SDN structure (picture from paper Sample Efficient Actor-Critic with Experience Replay). In the drawing, <img class="math" src="../_images/math/c49f6b393aeabd1825d2669b58ab7bc7b57806bc.svg" alt="[u_1, .... , u_n]"/> are assumed
to be samples from <img class="math" src="../_images/math/25420cc6e6816a10e460ec6a839b6e44cac82878.svg" alt="\pi_\theta(·|x_t)"/>.</p>
<a class="reference internal image-reference" href="../_images/ACER_SDN.png"><img alt="../_images/ACER_SDN.png" class="align-center" src="../_images/ACER_SDN.png" style="width: 692.1px; height: 423.90000000000003px;" /></a>
</section>
<section id="pseudocode">
<h2>Pseudocode<a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h2>
<p>There are a few changes between ACER applied to discrete action spaces and that applied to continuous action space.</p>
<a class="reference internal image-reference" href="../_images/ACER_algo1_1.png"><img alt="../_images/ACER_algo1_1.png" class="align-center" src="../_images/ACER_algo1_1.png" style="width: 750.0px; height: 203.0px;" /></a>
<a class="reference internal image-reference" href="../_images/ACER_algo1_2.png"><img alt="../_images/ACER_algo1_2.png" class="align-center" src="../_images/ACER_algo1_2.png" style="width: 742.0px; height: 720.0px;" /></a>
<p>In continuous action space, it is impossible to enumerate all actions q value. So ACER uses sampled actions to replace the expectation.</p>
<a class="reference internal image-reference" href="../_images/ACER_algo2.png"><img alt="../_images/ACER_algo2.png" class="align-center" src="../_images/ACER_algo2.png" style="width: 737.0px; height: 716.0px;" /></a>
</section>
<section id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>Here we show the ACER algorithm on the discrete action space.
The default config is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.acer.ACERPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.acer.</span></span><span class="sig-name descname"><span class="pre">ACERPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/acer.html#ACERPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.acer.ACERPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of ACER algorithm.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 21%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 35%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>acer</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is</div>
<div class="line">on-policy or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">trust_region</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm use trust</div>
<div class="line">region constraint</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">trust_region_value</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><div class="line-block">
<div class="line">maximum range of the trust region</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">unroll_len</span></code></p></td>
<td><p>int</p></td>
<td><p>32</p></td>
<td><div class="line-block">
<div class="line">trajectory length to calculate</div>
<div class="line">Q retrace target</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">learn.update</span></code>
<code class="docutils literal notranslate"><span class="pre">per_collect</span></code></p></td>
<td><p>int</p></td>
<td><p>4</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to</div>
<div class="line">train after collector’s one</div>
<div class="line-block">
<div class="line">collection. Only</div>
</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line"><br /></div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">c_clip_ratio</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><div class="line-block">
<div class="line">clip ratio of importance weights</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>

<p>Usually, we hope to compute everything as a batch to improve efficiency. This is done in <code class="docutils literal notranslate"><span class="pre">policy._get_train_sample</span></code>.
Once we execute this function in collector, the length of samples will equal to unroll-len in config. For details, please
refer to doc of <code class="docutils literal notranslate"><span class="pre">ding.rl_utils.adder</span></code>.</p>
<p>You can find more information in <a class="reference internal" href="impala.html#ref2other"><span class="std std-ref">here</span></a></p>
<p>The whole code of ACER you can find <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/acer.py">here</a>.
Here we show some details of this algorithm.</p>
<p>First, we use the following functions to compute the retrace Q value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_q_retraces</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span><span class="n">v_pred</span><span class="p">,</span><span class="n">rewards</span><span class="p">,</span><span class="n">actions</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">ratio</span><span class="p">,</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overview:</span>
<span class="sd">                Get Retrace Q value</span>
<span class="sd">            Arguments:</span>
<span class="sd">                - q_values (:obj:`torch.Tensor`): Q values</span>
<span class="sd">                - v_pred (:obj:`torch.Tensor`): V values</span>
<span class="sd">                - reward (:obj:`torch.Tensor`): reward values</span>
<span class="sd">                - actions (:obj:`torch.Tensor`): The actions in replay buffer</span>
<span class="sd">                - weights (:obj:`torch.Tensor`): setting padding postion</span>
<span class="sd">                - ratio (:obj:`torch.Tensor`): ratio of new polcy with behavior policy</span>
<span class="sd">            Returns:</span>
<span class="sd">                - q_retraces (:obj:`torch.Tensor`):  retrace Q values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">q_retraces</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v_pred</span><span class="p">)</span>
    <span class="n">n_len</span> <span class="o">=</span> <span class="n">q_retraces</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">tmp_retraces</span> <span class="o">=</span> <span class="n">v_pred</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>
    <span class="n">q_retraces</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_pred</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>
    <span class="n">q_gather</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v_pred</span><span class="p">)</span>
    <span class="n">q_gather</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">actions</span><span class="p">)</span>
    <span class="n">ratio_gather</span> <span class="o">=</span> <span class="n">ratio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">actions</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_len</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
        <span class="n">q_retraces</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">weights</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">*</span><span class="n">tmp_retraces</span>
        <span class="n">tmp_retraces</span> <span class="o">=</span> <span class="n">ratio_gather</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">q_retraces</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">-</span><span class="n">q_gather</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">])</span><span class="o">+</span><span class="n">v_pred</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">q_retraces</span>
</pre></div>
</div>
<p>After that, we calculate the value of policy loss, it will calculate the actor loss with importance weights truncation and bias correction loss by the following function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">acer_policy_error</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span><span class="n">q_retraces</span><span class="p">,</span><span class="n">v_pred</span><span class="p">,</span><span class="n">target_pi</span><span class="p">,</span><span class="n">actions</span><span class="p">,</span><span class="n">ratio</span><span class="p">,</span><span class="n">c_clip_ratio</span><span class="o">=</span><span class="mf">10.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overview:</span>
<span class="sd">            Get ACER policy loss</span>
<span class="sd">        Arguments:</span>
<span class="sd">            - q_values (:obj:`torch.Tensor`): Q values</span>
<span class="sd">            - q_retraces (:obj:`torch.Tensor`): Q values (be calculated by retrace method)</span>
<span class="sd">            - v_pred (:obj:`torch.Tensor`): V values</span>
<span class="sd">            - target_pi (:obj:`torch.Tensor`): The new policy&#39;s probability</span>
<span class="sd">            - actions (:obj:`torch.Tensor`): The actions in replay buffer</span>
<span class="sd">            - ratio (:obj:`torch.Tensor`): ratio of new polcy with behavior policy</span>
<span class="sd">            - c_clip_ratio (:obj:`float`): clip value for ratio</span>
<span class="sd">        Returns:</span>
<span class="sd">            - actor_loss (:obj:`torch.Tensor`): policy loss from q_retrace</span>
<span class="sd">            - bc_loss (:obj:`torch.Tensor`): bias correct policy loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">advantage_retraces</span> <span class="o">=</span> <span class="n">q_retraces</span><span class="o">-</span><span class="n">v_pred</span>
        <span class="n">advantage_native</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">-</span><span class="n">v_pred</span>
    <span class="n">actor_loss</span> <span class="o">=</span> <span class="n">ratio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="n">c_clip_ratio</span><span class="p">)</span><span class="o">*</span><span class="n">advantage_retraces</span><span class="o">*</span><span class="p">(</span><span class="n">target_pi</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">actions</span><span class="p">)</span><span class="o">+</span><span class="n">EPS</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
    <span class="n">bc_loss</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">c_clip_ratio</span><span class="o">/</span><span class="p">(</span><span class="n">ratio</span><span class="o">+</span><span class="n">EPS</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span><span class="o">*</span><span class="n">target_pi</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">*</span><span class="n">advantage_native</span><span class="o">*</span><span class="p">(</span><span class="n">target_pi</span><span class="o">+</span><span class="n">EPS</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
    <span class="n">bc_loss</span><span class="o">=</span><span class="n">bc_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">actor_loss</span><span class="p">,</span><span class="n">bc_loss</span>
</pre></div>
</div>
<p>Then, we execute backward operation towards target_pi. Moreover, we need to calculate the correction gradient in the trust region:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">acer_trust_region_update</span><span class="p">(</span><span class="n">actor_gradients</span><span class="p">,</span><span class="n">target_pi</span><span class="p">,</span><span class="n">avg_pi</span><span class="p">,</span><span class="n">trust_region_value</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overview:</span>
<span class="sd">            calcuate gradient with trust region constrain</span>
<span class="sd">        Arguments:</span>
<span class="sd">            - actor_gradients (:obj:`list(torch.Tensor)`): gradients value&#39;s for different part</span>
<span class="sd">            - target_pi (:obj:`torch.Tensor`): The new policy&#39;s probability</span>
<span class="sd">            - avg_pi (:obj:`torch.Tensor`): The average policy&#39;s probability</span>
<span class="sd">            - trust_region_value (:obj:`float`): the range of trust region</span>
<span class="sd">        Returns:</span>
<span class="sd">            - update_gradients (:obj:`torch.Tensor`): gradients under trust region constraint</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">KL_gradients</span> <span class="o">=</span> <span class="p">[(</span><span class="n">avg_pi</span><span class="o">/</span><span class="p">(</span><span class="n">target_pi</span><span class="o">+</span><span class="n">EPS</span><span class="p">))]</span>
    <span class="n">update_gradients</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">actor_gradient</span><span class="p">,</span><span class="n">KL_gradient</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">actor_gradients</span><span class="p">,</span><span class="n">KL_gradients</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">actor_gradient</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">KL_gradient</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">trust_region_value</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span><span class="n">KL_gradient</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">KL_gradient</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="n">update_gradients</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">actor_gradient</span><span class="o">-</span><span class="n">scale</span><span class="o">*</span><span class="n">KL_gradient</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">update_gradients</span>
</pre></div>
</div>
<p>With the new gradients, we can continue to propagate backwardly and then update parameters accordingly.</p>
<p>Finally, we should calculate the mean squared loss for Q values to update Q-Network</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">acer_value_error</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span><span class="n">q_retraces</span><span class="p">,</span><span class="n">actions</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overview:</span>
<span class="sd">            Get ACER critic loss</span>
<span class="sd">        Arguments:</span>
<span class="sd">            - q_values (:obj:`torch.Tensor`): Q values</span>
<span class="sd">            - q_retraces (:obj:`torch.Tensor`): Q values (be calculated by retrace method)</span>
<span class="sd">            - actions (:obj:`torch.Tensor`): The actions in replay buffer</span>
<span class="sd">            - ratio (:obj:`torch.Tensor`): ratio of new polcy with behavior policy</span>
<span class="sd">        Returns:</span>
<span class="sd">            - critic_loss (:obj:`torch.Tensor`): critic loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="n">actions</span><span class="o">=</span><span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">critic_loss</span><span class="o">=</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">q_retraces</span><span class="o">-</span><span class="n">q_values</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">actions</span><span class="p">))</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">critic_loss</span>
</pre></div>
</div>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas: “Sample Efficient Actor-Critic with Experience Replay”, 2016; [<a class="reference external" href="https://arxiv.org/abs/1611.01224">https://arxiv.org/abs/1611.01224</a> arxiv:1611.01224].</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ppo.html" class="btn btn-neutral float-right" title="PPO" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="a2c.html" class="btn btn-neutral float-left" title="A2C" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>