

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>VPN &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MCTS" href="mcts.html" />
    <link rel="prev" title="MBPO" href="mbpo.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>VPN</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/vpn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vpn">
<h1>VPN<a class="headerlink" href="#vpn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Value Prediction Network(VPN) was first proposed in <a class="reference external" href="https://arxiv.org/abs/1707.03497">Value Prediction Network</a> from NeurIPS 2017.
Value prediction network is an improvement upon the traditional model-based reinforcement learning algorithm
and a predecessor of the Muzero algorithm.</p>
<p>The main motivation of the VPN algorithm is that the planning phase of RL only needs to predict future rewards and value
without predicting future observations.</p>
<p>VPN learns to predict values via Q-learning and rewards via supervised learning at the same time,
VPN performs lookahead planning to choose actions and compute bootstrapped target Q-values.</p>
<p>(i.e., learning the dynamics of an abstract state space
sufficient for computing future rewards and values)</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>VPN can be viewed as combination of model-based RL and model-free RL.</p></li>
<li><p>The model-based part of VPN is to learn the dynamics of an abstract state space sufficient for computing future rewards and values, and the model-free part of VPN is to map the learned abstract states to rewards and values.</p></li>
<li><p>VPN combines temporal-difference search and n-step Q-learning to train.</p></li>
<li><p>The major performance improvement VPN has compared to other model-based algorithms is that VPN is more robust to stochasticity in the environment than an observation-prediction model approach.</p></li>
<li><p>In order to solve some of the existing problems of model-based algorithms, the VPN proposes a method between traditional model-based algorithms and model-free algorithms, using dynamics model to model the environment.</p></li>
<li><p>From a model-based perspective, the dynamics model models the state transition process, reward and discount function of the environment by extracting the form of abstract state.</p></li>
<li><p>From a model-free perspective, the extraction of abstract state by dynamics model can be regarded as an auxiliary task of training critic to predict reward and value, in order to better extract the relevant representation of the state.</p></li>
<li><p>VPN uses simple rollout and beam search, VPN + Monte Carlo Tree Search(MCTS) <span class="math notranslate nohighlight">\(\approxeq\)</span> Muzero.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model-based algorithm has better sample efficiency, but is more complicated;
The model-free algorithm has lower sample efficiency, but is relatively simple and easier to implement.
The model-based algorithm is not as popular as the model-free algorithm, due to its complexity,
and is mostly used in environments that are costly to acquire data, such as robot-arm control.
This is because the model-base algorithm involves environment modeling, resulting in a complicated modeling process. Besides,
environment modeling also requires training,
the overall training process for the model-based method is much more difficult than the model-free method.
Most of the environmental models modeled by model-based algorithms use observation and action to predict the next observation and reward.
This kind of model is called an observation-prediction model.
In a relatively complex environment, observation usually has a higher dimension and is highly stochastic,
which makes it difficult for model-based algorithms to learn the observation-prediction model corresponding to the environment.</p>
</div>
</div>
<div class="section" id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>Value Prediction Network mainly consists of 4 parts:</p>
<p>The VPN consists of the following modules parameterized by <span class="math notranslate nohighlight">\(\theta=\left\{\theta^{\text {enc }}, \theta^{\text {value }}, \theta^{\text {out }}, \theta^{\text {trans }}\right\}\)</span>:</p>
<blockquote>
<div><p>Encoding <span class="math notranslate nohighlight">\(f_{\theta}^{\text {enc }}: \mathbf{x} \mapsto \mathbf{s} \quad\)</span></p>
<p>Value <span class="math notranslate nohighlight">\(f_{\theta}^{\text {value }}: \mathbf{s} \mapsto V_{\theta}(\mathbf{s})\)</span></p>
<p>Outcome <span class="math notranslate nohighlight">\(f_{\theta}^{\text {out }}: \mathbf{s}, \mathbf{o} \mapsto r, \gamma\)</span></p>
<p>Transition <span class="math notranslate nohighlight">\(f_{\theta}^{\text {trans }}:\)</span> s, <span class="math notranslate nohighlight">\(\mathbf{o} \mapsto \mathbf{s}^{\prime}\)</span></p>
</div></blockquote>
<ul class="simple">
<li><p>Encoding module maps the observation <span class="math notranslate nohighlight">\((\mathbf{x})\)</span> to the abstract state <span class="math notranslate nohighlight">\(\left(\mathbf{s} \in \mathbb{R}^{m}\right)\)</span> using neural networks (e.g., CNN for visual observations). Thus, <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> is an abstract-state representation which will be learned by the network (and not an environment state or even an approximation to one).</p></li>
<li><p>Value module estimates the value of the abstract-state <span class="math notranslate nohighlight">\(\left(V_{\theta}(\mathbf{s})\right)\)</span>. Note that the value module is not a function of the observation, but a function of the abstract-state.</p></li>
<li><p>Outcome module predicts the option-reward <span class="math notranslate nohighlight">\((r \in \mathbb{R})\)</span> for executing the option <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> at abstract-state s. If the option takes <span class="math notranslate nohighlight">\(k\)</span> primitive actions before termination, the outcome module should predict the discounted sum of the <span class="math notranslate nohighlight">\(k\)</span> immediate rewards as a scalar. The outcome module also predicts the option-discount <span class="math notranslate nohighlight">\((\gamma \in \mathbb{R})\)</span> induced by the number of steps taken by the option.</p></li>
<li><p>Transition module transforms the abstract-state to the next abstract-state <span class="math notranslate nohighlight">\(\left(\mathbf{s}^{\prime} \in \mathbb{R}^{m}\right)\)</span> in an optionconditional manner.</p></li>
</ul>
<p>The Q value can be estimated through the above four parts,
that is, input observation and the corresponding option (action), and output the corresponding value estimation.</p>
<p>The corresponding Q-value prediction formula is:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Q_{\theta}^{d}(\mathbf{s}, \mathbf{o})=r+\gamma V_{\theta}^{d}\left(\mathbf{s}^{\prime}\right) \quad V_{\theta}^{d}(\mathbf{s})=\left\{\begin{array}{ll}
V_{\theta}(\mathbf{s}) &amp; \text { if } d=1 \\
\frac{1}{d} V_{\theta}(\mathbf{s})+\frac{d-1}{d} \max _{\mathbf{0}} Q_{\theta}^{d-1}(\mathbf{s}, \mathbf{o}) &amp; \text { if } d&gt;1
\end{array}\right.\)</span></p>
</div></blockquote>
<p>The d-step planning process is shown below:</p>
<blockquote>
<div><img alt="../_images/vpn-planning.png" src="../_images/vpn-planning.png" />
</div></blockquote>
<p>When training, VPN use k-step prediction to train:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\mathbf{s}_{t}^{k}=\left\{\begin{array}{ll}
f_{\theta}^{\text {enc }}\left(\mathbf{x}_{t}\right) &amp; \text { if } k=0 \\
f_{\theta}^{\text {trans }}\left(\mathbf{s}_{t-1}^{k-1}, \mathbf{o}_{t-1}\right) &amp; \text { if } k&gt;0
\end{array} \quad v_{t}^{k}=f_{\theta}^{\text {value }}\left(\mathbf{s}_{t}^{k}\right) \quad r_{t}^{k}, \gamma_{t}^{k}=f_{\theta}^{\text {out }}\left(\mathbf{s}_{t}^{k-1}, \mathbf{o}_{t}\right)\right.\)</span></p>
</div></blockquote>
<p>The entire update flow chart is as follows:</p>
<img alt="../_images/vpn-learning.png" class="align-center" src="../_images/vpn-learning.png" />
</div>
<div class="section" id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<img alt="../_images/vpn-code.png" src="../_images/vpn-code.png" />
</div>
<div class="section" id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>VPN can be combined with:</p>
<blockquote>
<div><ul>
<li><p>Monte Carlo Tree Search(MCTS)</p>
<blockquote>
<div><p>In the VPN paper, the author mentioned that the VPN algorithm is compatible with other tree search algorithms such as MCTS, but in the specific experiment, the paper uses a simple Rollout for simplification.
<a class="reference external" href="https://arxiv.org/abs/1911.08265v2">Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</a>
replaces the simple rollout with MCTS, and achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>TBD</p></li>
</ul>
</div></blockquote>
<p>The network interface VPN used is defined as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>TBD</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<p>Junhyuk Oh, Satinder Singh, Honglak Lee: “Value Prediction Network”, 2017; [<a class="reference external" href="http://arxiv.org/abs/1707.03497">http://arxiv.org/abs/1707.03497</a> arXiv:1707.03497].</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver: “Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model”, 2019; [<a class="reference external" href="http://arxiv.org/abs/1911.08265">http://arxiv.org/abs/1911.08265</a> arXiv:1911.08265]. DOI: [https://dx.doi.org/10.1038/s41586-020-03051-4 10.1038/s41586-020-03051-4].</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mcts.html" class="btn btn-neutral float-right" title="MCTS" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="mbpo.html" class="btn btn-neutral float-left" title="MBPO" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>