


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>分布式强化学习 &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="系统设计" href="../03_system/index_zh.html" />
  <link rel="prev" title="离线强化学习" href="offline_rl_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">用户指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index_zh.html">DI-engine 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index_zh.html">快速开始</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习算法分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index_zh.html">系统设计</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index_zh.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">强化学习教程</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index_zh.html">从 DI-zoo 开始学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index_zh.html">强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index_zh.html">强化学习环境示例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者规范</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index_zh.html">代码规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index_zh.html">代码风格指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index_zh.html">单元测试指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index_zh.html">图像与可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index_zh.html">Github 合作模式</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index_zh.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index_zh.html">强化学习算法分类</a> &gt;</li>
        
      <li>分布式强化学习</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/02_algo/distributed_rl_zh.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="id1">
<h1>分布式强化学习<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<section id="id2">
<h2>问题定义和研究动机<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>分布式强化学习（Distributed RL）是深度强化学习走向大规模应用，解决复杂决策空间和长期规划问题的必经之路。为了解决像星际争霸2（SC2） <a class="footnote-reference brackets" href="#id30" id="id3">1</a> 和 DOTA2 <a class="footnote-reference brackets" href="#id31" id="id4">2</a> 这样超大规模的决策问题，单进程乃至单机器的算力是远远不够的，需要将整个训练管线中的各个部分拓展到各种各样的计算和存储设备上。研究者们希望设计一整套“算法+系统”的方案，能够让 DRL 训练程序便捷地运行在各种不同的计算尺度下，在保证算法优化收敛性的同时，尽可能地提升其中各个环节的效率。</p>
<p>一般来说，一个强化学习训练程序有三类核心模块，用于和环境交互产生数据的 Collector，其中包含环境本身（Env）和产生动作的 Actor，以及使用这些数据进行训练的 Learner，他们各自需要不同数量和类型的计算资源支持。而根据算法和环境类型的不同，又会有一些延伸的辅助模块，例如大部分 off-policy 算法都会需要数据队列（Replay Buffer）来存储训练数据，对于 model-based RL 相关的算法又会有学习环境 dynamics 的相关训练模块，而对于需要大量自我博弈（self-play）的算法，还需要一个中心化的 Coordinator 去控制协调各个组件（例如动态指定自己博弈的双方）。</p>
<p>在系统角度，需要让整个训练程序中的同类模块有足够的并行扩展性，例如可以根据需求增加进行交互的环境数量（消耗更多的CPU），或是增加训练端的吞吐量（通用需要使用更多的GPU），而对于不同的模块，又希望能够尽可能地让所有的模块可以异步执行，并减小模块时间各种通信方式的代价（网络通信，数据库，文件系统）。但总的来说，一个系统的效率优化的理论上限是——Learner 无等待持续高效训练，即在 Learner 一个训练迭代高效完成时，下一个训练迭代的数据已经准备好。</p>
<p>在算法角度，则是希望在保证算法收敛性的情况下，降低算法对数据产生吞吐量的要求（例如容忍更旧更 off-policy 的数据），提高数据探索效率和对于已收集数据的利用效率（例如修改数据采样方法，或是结合一些 RL 中 data-efficiency 相关的研究），从而为系统设计提供更大的空间和可能性。</p>
<p>综上所述，分布式强化学习是一个更加综合的研究子领域，需要深度强化学习算法 + 分布式系统设计的互相感知和协同。</p>
</section>
<section id="id5">
<h2>研究方向<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<section id="id6">
<h3>系统<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<section id="id7">
<h4>整体架构<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>对于常见的决策问题，最常用的两种分布式架构就是 IMPALA <a class="footnote-reference brackets" href="#id32" id="id8">3</a> 和 SEED RL <a class="footnote-reference brackets" href="#id33" id="id9">4</a></p>
<img alt="../_images/impala.png" class="align-center" src="../_images/impala.png" />
<ul class="simple">
<li><p>前者是经典的 Actor-Learner 模式，即数据收集和训练端完全分离，定期从 Learner 到 Actor 传递最新的神经网络模型，而 Actor 在收集到一定量的数据后（即 observations）发送给 Learner。如果有多个 Learner，他们还会定期同步神经网络的梯度（即分布式深度学习中的数据并行模式）。</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/seed_rl.png"><img alt="../_images/seed_rl.png" class="align-center" src="../_images/seed_rl.png" style="width: 649.0px; height: 495.0px;" /></a>
<ul class="simple">
<li><p>后者在前者的基础上，致力于去优化传输模型的损耗，SEED RL将用于推理产生动作的部分剥离出来，也和训练端放在一起，通过高效的 TPU 间通信技术来更新模型，从而大大减少了 IMPALA 中传递模型的代价，而对于环境和推理 Actor 之间的跨机器通信，SEED RL 使用优化过的gRPC方案来传递 observation 和 action，从而并不会有太大的负担。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>这两种方案并没有绝对的谁优谁劣，关键在于对于一个实际的决策问题，究竟是跨机器传输模型的代价更大，还是跨机器传输 observation 和 action 等数据的代价更大，如果是前者，且有比较好的 GPU/TPU 间通信组件，那么 SEED RL 是更好的解决方案，如果是后者，IMPALA是比较稳定的选择。还有，IMPALA 可以累积一批数据后进行数据传递，而 SEED RL 需要每个交互帧都存在数据传输，这属于经典的数据批处理和流处理对比问题，对于目前的机器学习社区前者一般会更简便易用。此外，如果整个训练程序需要更高的自由度和定制化，例如动态控制 Actor 的一些行为，IMPALA 会更为方便一些。</p>
</div>
<p>在上述两种架构之外，还有很多其他的分布式强化学习设计方案，例如引入异步神经网络更新方案的 A3C <a class="footnote-reference brackets" href="#id34" id="id10">5</a> 和 Gossip A2C <a class="footnote-reference brackets" href="#id35" id="id11">6</a>，为了支持大规模自我博弈，设计出复杂 League 机制的 AlphaStar <a class="footnote-reference brackets" href="#id30" id="id12">1</a>，结合 model-based RL 和 MCTS 相关模块的 MuZero <a class="footnote-reference brackets" href="#id36" id="id13">7</a>，这里就不一一叙述了，有兴趣的读者可以参考具体论文或是参考我们的 <a class="reference external" href="../12_policies/index_zh.html">算法攻略合集部分</a>。</p>
</section>
<section id="id15">
<h4>单点效率优化<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h4>
<p>除了整体结构的设计和创新，还有很多对于整个训练程序中某一单点模块进行优化的方法，它们主要是针对某一个子问题，进行专门的定制优化，这里介绍一些主要的方法：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Object</span> <span class="pre">Store</span></code> in Ray/RLLib <a class="footnote-reference brackets" href="#id37" id="id16">8</a>: 对于多进程多机器之间的数据传递，Ray/RLLib中的 Object Store 提供了一种非常便捷高效的方式，任何一个进程，只要知道了这个对象的引用（即reference），就可以通过向 Store 请求获取相应的值，而具体内部的数据传输完全由 Store 进行管理，这样就可以像写本地单进程程序一样，实现一个分布式训练程序。Object Store 的具体实现是结合 redis，plasma和gRPC完成。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sample</span> <span class="pre">Factory</span></code> <a class="footnote-reference brackets" href="#id38" id="id17">9</a>: Sample Factory 针对单台机器规模的 APPO 算法做了定制化优化，精心设计了一种环境和产生动作的策略之间的异步化方案，并利用 shared memory 大幅提高各模块之间的传输效率。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Reverb</span></code> in Acme <a class="footnote-reference brackets" href="#id39" id="id18">10</a>: Reverb 提供了一套高灵活度和高效率的数据操作和管理模块，对于RL，很适合用来实现 replay buffer 相关组件。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">envpool</span></code> <a class="footnote-reference brackets" href="#id40" id="id19">11</a>: envpool 是目前最快的环境向量化并行方案，利用 c++ threadpool 和许多经典 RL 环境的高效实现，提供了强大的异步向量化环境仿真能力。</p></li>
</ul>
</section>
</section>
<section id="id20">
<h3>算法<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<section id="id21">
<h4>降低算法对数据产生吞吐量的要求<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">V-trace</span></code> in IMPALA <a class="footnote-reference brackets" href="#id32" id="id22">3</a>: off-policy 算法可以拓宽可供训练的数据范围，从而一定程度上提高算法对于旧数据的容忍程度，降低 Collector 端产生数据的吞吐压力，但是过于 off-policy 的数据很容易影响算法的收敛性，IMPALA 中针对这个问题，利用重要性采样机制和相应的裁剪手段，设计了分布式训练设置下一种较稳定的算法方案 V-trace，限制 off-policy 数据对优化本身的负面影响。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Reuse</span></code> and <code class="docutils literal notranslate"><span class="pre">Staleness</span></code> in OpenAI FIVE <a class="footnote-reference brackets" href="#id31" id="id23">2</a>: 在 OpenAI 针对 DOTA2 设计的智能体中，他们进行了一些关于数据重用次数（Reuse）和折旧程度（Staleness）的实验，过高的重用次数和过旧的数据都会影响大规模训练中 PPO 算法的稳定性。</p></li>
</ul>
</section>
<section id="id24">
<h4>提高数据探索效率 + 对于已收集数据的利用效率<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Priority</span> <span class="pre">and</span> <span class="pre">Diversity</span></code>——Ape-x <a class="footnote-reference brackets" href="#id41" id="id25">12</a>: Ape-x是一种经典的分布式强化学习方案，其中一个核心做法就是利用 Priority Experience Replay，为不同的数据设置不同的采样优先级，让算法更加关注那些“重要”的数据。此外，Ape-x还在不同的并行 Collector 中设置不同的探索参数（即eps greedy的epsilon）来提升数据多样性。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Representation</span> <span class="pre">Learning</span></code> in RL——CURL <a class="footnote-reference brackets" href="#id42" id="id26">13</a>: 对于一些高维或多模态的输入，可以结合表示学习方法来提升 RL 的数据利用效率，例如对于高维图像输入的控制问题，CURL 引入额外的对比学习过程，RL 在学习到的特征空间上进行决策。而从系统设计来看，表征学习和强化学习训练结合也有很多优化空间，例如两者的异步。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Model-based/MCTS</span> <span class="pre">RL</span></code>——MuZero <a class="footnote-reference brackets" href="#id36" id="id27">7</a>: MuZero将 model-based RL 和 MCTS RL结合在一起来提升整体的训练效率，其中包含诸多独有的模块，例如 MCTS 的搜索过程，数据在训练前的 reanalyze 过程等等，相应也会引出更为复杂和多样的分布式强化学习训练系统。</p></li>
</ul>
</section>
</section>
</section>
<section id="id28">
<h2>未来展望<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h2>
<p>目前，分布式强化学习还只是一个新兴的研究子领域，很多情况下会受限于算力和问题环境，仍然存在很多需要被解决的问题：</p>
<ul class="simple">
<li><p>缺少统一的 benchmark 来评价分布式强化学习算法和系统的效率；</p></li>
<li><p>目前大部分分布式强化学习方案都只适用于一小部分环境和一部分 RL 算法，距离技术的通用化还有很远的路要走；</p></li>
<li><p>当前的系统优化和 RL 算法本身仍然是隔离的，可以考虑感知 RL 优化需求的系统设计，例如动态资源感知和调度</p></li>
</ul>
</section>
<section id="id29">
<h2>参考文献<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id30"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Oriol Vinyals, Igor Babuschkin, David Silver, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nat. 575(7782): 350-354 (2019)</p>
</dd>
<dt class="label" id="id31"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id23">2</a>)</span></dt>
<dd><p>Christopher Berner, Greg Brockman, et al. Dota 2 with Large Scale Deep Reinforcement Learning. CoRR abs/1912.06680 (2019)</p>
</dd>
<dt class="label" id="id32"><span class="brackets">3</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id22">2</a>)</span></dt>
<dd><p>Lasse Espeholt, Hubert Soyer, Rémi Munos, et al. IMPALA. Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. ICML 2018: 1406-1415</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id9">4</a></span></dt>
<dd><p>Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, Marcin Michalski. SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference. ICLR 2020</p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id10">5</a></span></dt>
<dd><p>Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. ICML 2016: 1928-1937</p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id11">6</a></span></dt>
<dd><p>Mahmoud Assran, Joshua Romoff, Nicolas Ballas, Joelle Pineau, Mike Rabbat. Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning. NeurIPS 2019: 13299-13309</p>
</dd>
<dt class="label" id="id36"><span class="brackets">7</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id27">2</a>)</span></dt>
<dd><p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, David Silver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. CoRR abs/1911.08265 (2019)</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id16">8</a></span></dt>
<dd><p>Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph Gonzalez, Ken Goldberg, Ion Stoica. Ray RLLib: A Composable and Scalable Reinforcement Learning Library. CoRR abs/1712.09381 (2017)</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id17">9</a></span></dt>
<dd><p>Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav S. Sukhatme, Vladlen Koltun. Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning. ICML 2020: 7652-7662</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id18">10</a></span></dt>
<dd><p>Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alexander Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Çaglar Gülçehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, Nando de Freitas. Acme: A Research Framework for Distributed Reinforcement Learning. CoRR abs/2006.00979 (2020)</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id19">11</a></span></dt>
<dd><p>Jiayi Weng and Min Lin and Zhongwen Xu and Shuicheng Yan. <a class="reference external" href="https://github.com/sail-sg/envpool">https://github.com/sail-sg/envpool</a></p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id25">12</a></span></dt>
<dd><p>Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, David Silver. Distributed Prioritized Experience Replay. ICLR (Poster) 2018</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id26">13</a></span></dt>
<dd><p>Michael Laskin, Aravind Srinivas, Pieter Abbeel: CURL: Contrastive Unsupervised Representations for Reinforcement Learning. ICML 2020: 5639-5650</p>
</dd>
</dl>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="../03_system/index_zh.html" class="btn btn-neutral float-right" title="系统设计" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="offline_rl_zh.html" class="btn btn-neutral" title="离线强化学习" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">分布式强化学习</a><ul>
<li><a class="reference internal" href="#id2">问题定义和研究动机</a></li>
<li><a class="reference internal" href="#id5">研究方向</a><ul>
<li><a class="reference internal" href="#id6">系统</a><ul>
<li><a class="reference internal" href="#id7">整体架构</a></li>
<li><a class="reference internal" href="#id15">单点效率优化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id20">算法</a><ul>
<li><a class="reference internal" href="#id21">降低算法对数据产生吞吐量的要求</a></li>
<li><a class="reference internal" href="#id24">提高数据探索效率 + 对于已收集数据的利用效率</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id28">未来展望</a></li>
<li><a class="reference internal" href="#id29">参考文献</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>