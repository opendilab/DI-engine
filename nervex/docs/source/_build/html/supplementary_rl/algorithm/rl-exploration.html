

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>RL Exploration &mdash; nerveX 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="强化学习常用的游戏环境/RL Game Environments" href="../game_env.html" />
    <link rel="prev" title="Multi Agent Reinforcement Learning" href="multi-agent.html" />
    <link href="../../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> nerveX
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../feature/index.html">Feature</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Supplementary of RL</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">算法与训练/Algorithm and Training</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="rl-algo.html">RL Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="large-scale-rl.html">Large Scale RL Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="multi-agent.html">Multi Agent Reinforcement Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">RL Exploration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#preparation-facts">0. Preparation Facts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#motivation-on-exploration">1. Motivation on Exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#measurements-on-exploration-and-integrating-into-reward-i-e-the-how-to">2. Measurements on Exploration and Integrating into Reward i.e. the “How to?”</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practices-of-algo-and-env-wip">3. Practices of Algo and Env (WIP.)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-with-drl-methods">4. Integration with DRL methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#environments">5. Environments</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references-remarks">6. References / Remarks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../game_env.html">强化学习常用的游戏环境/RL Game Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bandit/index.html">Bandit</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorial_dev/index.html">Tutorial-Developer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">nerveX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Supplementary of RL</a> &raquo;</li>
        
          <li><a href="index.html">算法与训练/Algorithm and Training</a> &raquo;</li>
        
      <li>RL Exploration</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/supplementary_rl/algorithm/rl-exploration.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rl-exploration">
<h1>RL Exploration<a class="headerlink" href="#rl-exploration" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="preparation-facts">
<h2>0. Preparation Facts<a class="headerlink" href="#preparation-facts" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><dl>
<dt>Terms：</dt><dd><ul>
<li><dl>
<dt>Exploration bonus</dt><dd><ul>
<li><p>定义Exploration增益的常用方式之一，定义generic的Bonus Function，使其根据不同的exploration增益依据, 适应到不同的RL算法中。(<a class="reference external" href="images/RL_survey_2020.png">DRL MDPs Overview</a>)</p></li>
<li><dl>
<dt>如在常见的count-based approach中，Novelty Function基于对某个state的熟悉度通过转化形成对Reward增益的Bonus Function。</dt><dd><ul>
<li><dl>
<dt>一些Bonus的定义例子</dt><dd><img alt="../../_images/Diff_Bonus.png" src="../../_images/Diff_Bonus.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">Thompson Sampling</a></dt><dd><ul class="simple">
<li><dl class="simple">
<dt>TS 可使用先验经验的信息，实现TS重复的流程如下：</dt><dd><ul>
<li><p>设定先验分布：为每一个可能的选项建立一个 Distribution （如选择Beta则先验经验体现在 α与β的初值设定）</p></li>
<li><p>采样：使每个贝塔分布产生一个随机数值，选择结果最大的作为本次选项</p></li>
<li><p>调参：根据实验根据实验结果调整参数（α与β）</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>因而Policy-basedDRL得到样本，训神经网络更新policy的思想，可以用来理解Thompsom Sampling：</dt><dd><ul>
<li><p>TS: prior + sampling ==&gt; posterior distribution</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Stochastic policy vs. Deterministic policy</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Stochastic policy</dt><dd><ul>
<li><p>从state set S​到action set A​的条件概率分布, 在这个S上所有的action都存在一个被选到的概率。在stochastic policy中，action来自policy distribution’s sampling.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Deterministic policy</dt><dd><ul>
<li><p>S-&gt;A的映射。一个state s到对应a​，因policy固定，每一对有映射关系s得到确定的a（概率始终为1）。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://github.com/pliang279/awesome-multimodal-ml#survey-papers">Multimodal</a></dt><dd><ul class="simple">
<li><p>引入多个模态的数据来增强强化学习模型的性能。e.g.智能体捕捉到的环境的声音与的环境图像就可看作是环境信息的两个模态的数据，这两个不同的环境数据来源可以用以提高模型性能。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><a class="reference external" href="https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/">UCB</a></dt><dd><ul>
<li><dl>
<dt>一种设计Bonus的方法： 基于visitation 的频率来增益reward， 属于count based approach中常用的实现exploraion的方法。在AlphaGo MCTS算法中有使用。</dt><dd><img alt="../../_images/UCB.png" src="../../_images/UCB.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><a class="reference external" href="https://www.mikulskibartosz.name/using-boltzmann-distribution-as-exploration-policy-in-tensorflow-agent/">Boltzmann distribution</a></dt><dd><ul>
<li><dl class="simple">
<dt>定义</dt><dd><ul class="simple">
<li><p>源于统计力学，“理想气体”分子间基本没有作用力情况下的分布。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>使用</dt><dd><ul class="simple">
<li><p>在Soft-Q Learning ，在使用stochastic policy的基础上对multimodal，为了更好的收敛Q函数收敛，使用energy-based policy</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Energy-based policy 即熵策略，一种基于能量的表达策略。 在热力学和信息论中，熵为随机程度的度量，因此被用来参与对探索奖励的计算 e.g.最大熵RL</p>
</div>
<img alt="../../_images/energy-based-policy.png" src="../../_images/energy-based-policy.png" />
<img alt="../../_images/equation.svg" src="../../_images/equation.svg" /><img alt="../../_images/equation2.svg" src="../../_images/equation2.svg" /><p>(这也是最大熵RL的optimal policy最优策略的形式)
这样的policy能够为每一个action赋值一个特定的概率符合Q值的分布，也就满足了stochastic policy的需求。</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><a class="reference external" href="https://www.cnblogs.com/sddai/p/10088007.html">SimHash</a></dt><dd><ul>
<li><dl>
<dt>使用Hash进行伪计数（一种先验计数方法，可以理解为观测没发生时可能性的动态变化）的count based方法，SimHash 属于 locality-sensitive hashing（LSH），在visitation基础上体现了包含相似程度层面的exploration approach。</dt><dd><img alt="../../_images/count-hashing-exploration.png" src="../../_images/count-hashing-exploration.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>为什么需要伪计数（Pseudo-Count) 呢？首先理解count-based的探索基于state的频率计算探索奖励，但是在如星际的环境中几乎不存在相同state（即所有state count都是1或0）。因此我们需要伪计数来近似，从而把足够相似的环境认为是可以增加count的。</p>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://victorzhou.com/blog/information-gain/">Info Gain</a></dt><dd><ul class="simple">
<li><dl class="simple">
<dt>理解Information-theoretic exploration即为state += agent.info，且定义信息增益形成对Reward增益的。代表算法如：</dt><dd><ul>
<li><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/1605.09674">VIME（信息最大化探索)</a></dt><dd><ul>
<li><p>Good for sparse-reward exploration problems</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://pathak22.github.io/noreward-rl/">ICM （定义curiosity，实现上是transition dist entropy的近似，使内在奖励函数优化的探索)</a></dt><dd><ul>
<li><p>Error -&gt; Reward</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference external" href="https://arxiv.org/abs/1810.12894">RND （略区别于以上VIME ICM的intrinsic reward设计，使用nn预测误差表示Novelty并与extrinstic结合)</a> （详细介绍见 pt3 error based部分）</dt><dd><ul class="simple">
<li><p>Bonus = the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network</p></li>
<li><p>Flexibly combine intrinsic and extrinsic rewards</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><a class="reference external" href="images/CB.png">Contextural Bandit</a>; <a class="reference external" href="https://cs.uwaterloo.ca/~ppoupart/ICML-07-tutorial-slides/ICML-07-Tutorial-Slides.html">Bayesian RL</a>; <a class="reference external" href="https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture28-pac.pdf">PAC</a>; <a class="reference external" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">POMDP</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Goals:</dt><dd><ol class="arabic simple">
<li><p>Understand &amp; Motivate exploration</p></li>
<li><p>Understand practive of exploration: How to (2.1) derive exploration methods from perspectives to Formula/Model, and (2.2) how to practive in DRL.</p></li>
<li><p>Understand different envs for researching exploration problems.</p></li>
</ol>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="motivation-on-exploration">
<h2>1. Motivation on Exploration<a class="headerlink" href="#motivation-on-exploration" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><dl>
<dt>Why Exploration? See Example:<a class="reference external" href="images/mr.png">Montezuma’s Revenge</a></dt><dd><ul>
<li><dl class="simple">
<dt>Recalling on evaluating performance</dt><dd><ul class="simple">
<li><p>Defining Reward (func)</p></li>
<li><dl class="simple">
<dt>Alternative - mimic expert i.e. behavior cloning</dt><dd><ul>
<li><dl class="simple">
<dt>Difficulties:</dt><dd><ul>
<li><p>Capability difficulties</p></li>
<li><p>Identifying salient parts’ difficulties</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Objective - Reason about what the expert is trying to achieve</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Challenges (hopefuly to be solved by exploration)</dt><dd><ul class="simple">
<li><p>Decide on (Definition / Form of) Reward : How to get strategies without instant reward but big final rewards</p></li>
<li><p>Exploration vs Exploitation : How to decide condition for attempting new behaviors</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Exploration Problems 不过分忽略 AND 不过分关注</dt><dd><ul>
<li><p>Hard-Exploration i.e. exploration in an environment with very sparse or deceptive rewards</p></li>
<li><dl>
<dt>The Noisy TV Problem<span class="classifier">Agent gets reward by a noise (random uncontrollable and unpredictable reward consistently, but fails to proceed to any meaningful progress” “怎么让智能体专心走迷宫而不是分心地看电视不走了？”</span></dt><dd><img alt="../../_images/the-noisy-TV-problem.gif" src="../../_images/the-noisy-TV-problem.gif" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Limitation (on <a class="reference external" href="images/tract.png">Tractability and Optimization</a> )</dt><dd><img alt="../../_images/tract.png" src="../../_images/tract.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="measurements-on-exploration-and-integrating-into-reward-i-e-the-how-to">
<h2>2. Measurements on Exploration and Integrating into Reward i.e. the “How to?”<a class="headerlink" href="#measurements-on-exploration-and-integrating-into-reward-i-e-the-how-to" title="Permalink to this headline">¶</a></h2>
<dl>
<dt>到这里我们已经知道使用探索增益的想要解决的两大问题是什么：如何有效定义{稀疏,复杂,有欺骗性的}奖励 + 摆脱局部最优的困境。接下来进入方法的探讨。</dt><dd><ul>
<li><dl class="simple">
<dt>Classic Approachs</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Statistical Approach:</dt><dd><ul>
<li><p>Epsilon-greedy; UCB; Boltzmann exploration; Thompson sampling</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>More Perspectives and Corresponding Methods (WIP)</dt><dd><ul>
<li><dl>
<dt>Common reward manipulation methods:</dt><dd><ul>
<li><dl>
<dt>Optimism-based exploration i.e. ‘New’ == ‘Good’, typically defining a novelty term (such as UCB) which integrate into bonus (The naive idea is simply to add a bonus to the reward, no need of tuning)</dt><dd><ul>
<li><dl class="simple">
<dt>Count based / Error Based<span class="classifier">count on visitations of states</span></dt><dd><ul class="simple">
<li><p>Naive (Problem: sometimes we never see a state twice. )</p></li>
<li><p>Hashing (e.g. SimHash, to solve the problem that some states are similar to each other)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Prediction Based<span class="classifier">stores all the experiences encountered by the robot, estimate novelty with the prediction error</span></dt><dd><ul>
<li><dl>
<dt>Forward dynamics prediction model i.e.  <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.7661&amp;rep=rep1&amp;type=pdf">Intelligent Adaptive Curiosity</a></dt><dd><img alt="../../_images/IAC.png" src="../../_images/IAC.png" />
</dd>
</dl>
</li>
<li><dl>
<dt>Intrinsic Curiosity Module i.e. <a class="reference external" href="https://arxiv.org/abs/1705.05363">ICM</a></dt><dd><img alt="../../_images/ICM.png" src="../../_images/ICM.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Memory Based :</dt><dd><ul>
<li><dl>
<dt>Episodic Curiosity <a class="reference external" href="https://arxiv.org/abs/1810.02274">(EC)</a></dt><dd><img alt="../../_images/EC.png" src="../../_images/EC.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Thompson sampling style Exploration e.g. Bootstrapped DQN</dt><dd><ul class="simple">
<li><p>Q-value Based: learn distribution over Q-functions or Policy</p></li>
<li><p>sample and act according to sample</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Information-theoretic exploration</dt><dd><ul class="simple">
<li><p>Add an entropy term H(π(a|s)) into the loss function, encouraging the policy to take diverse actions.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Noise Approach:</dt><dd><ul class="simple">
<li><p>Add noise into the observation, action or even parameter space</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Direct Exploration:</dt><dd><ul class="simple">
<li><p>Phase 1 “Go Explore” + Phase 2 “Backward Algorithm”  <a class="reference external" href="https://arxiv.org/abs/2004.12919">(Go-Explore)</a></p></li>
</ul>
<blockquote>
<div><img alt="../../_images/policy-based-Go-Explore.png" src="../../_images/policy-based-Go-Explore.png" />
</div></blockquote>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="practices-of-algo-and-env-wip">
<h2>3. Practices of Algo and Env (WIP.)<a class="headerlink" href="#practices-of-algo-and-env-wip" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><dl>
<dt>Curiosity Algorithm (Optimism-based exploration)</dt><dd><ul>
<li><dl>
<dt>CDP （<a class="reference external" href="http://arxiv.org/pdf/1902.08039v2.pdf">Curiosity-Driven Experience Prioritization via Density Estimation</a>）</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Overview</dt><dd><ul>
<li><p>好奇心驱动优先排序（CDP）框架, 希望平衡地探索memory buffer里的样本</p></li>
<li><p>在（Agent 探索 -&gt; 轨迹收集2Buffer -&gt; 学习）过程中，focus on ucommon events and their tracks to encourage the agent to over-sample those trajectories that have rare achieved goal states</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Algorithm</p></li>
</ul>
<img alt="../../_images/CDP.png" src="../../_images/CDP.png" />
<ul class="simple">
<li><dl class="simple">
<dt>Representation &amp; Manipulation of Exploration</dt><dd><ul>
<li><p>Combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER).</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Environment</dt><dd><ul>
<li><p>OpenAI gym</p></li>
<li><p>6 robot manipulation task</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>BeBold (<a class="reference external" href="https://yuandong-tian.com/BeBold.mp4">Exploration Beyond the Boundary of Explored Regions</a>）</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><a class="reference external" href="https://zhuanlan.zhihu.com/p/337759337">Bebold Overview</a> (&lt;- 作者写的说明文档）</dt><dd><ul>
<li><p>一言以蔽之：定义新旧探索区域的边界并且鼓励边界跨越。具体实现为给定一条智能体走过的路径，估计各状态 s的访问次数 N（s） ，然后把它们的倒数相减，再做一个ReLU截断，就是BeBold。不仅希望像Count-based或者是RND那样去探索那些访问次数[公式]较少的状态，更希望能探索在充分访问区域与未充分访问区域交邻的边界.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="../../_images/bebold.jpg" src="../../_images/bebold.jpg" />
<ul class="simple">
<li><p>Algorithm</p></li>
</ul>
<img alt="../../_images/BeBold.png" src="../../_images/BeBold.png" />
<ul>
<li><dl>
<dt>Representation &amp; Manipulation of Exploration</dt><dd><ul class="simple">
<li><p>探索在充分访问区域与未充分访问区域交邻的边界</p></li>
</ul>
<img alt="../../_images/bb_manipulation.jpg" src="../../_images/bb_manipulation.jpg" />
<ul class="simple">
<li><p>为避免重复游走增加每局访问数（episodic visitation count）的限制，让每局每个状态最多能拿到一次奖励。</p></li>
</ul>
<img alt="../../_images/bb_m2.jpg" src="../../_images/bb_m2.jpg" />
<ul class="simple">
<li><p>用的是Random Network Distillation（RND）的方法，用一个预测网络（predictor network）去拟合另一个固定的随机神经目标网络（random fixed target network）的输出。一个状态 s 被访问的次数越多，则预测网络和目标网络的差值就越小。用两者的差值，就可以反向估计出一个状态的访问次数,即为Novelty function N（s)</p></li>
</ul>
<img alt="../../_images/ablation_bb.jpg" src="../../_images/ablation_bb.jpg" />
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Environment</dt><dd><ul class="simple">
<li><p>Mini Grid</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Error Based Algorithm</dt><dd><ul>
<li><dl>
<dt>RND <a class="reference external" href="https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/">blog</a></dt><dd><ul>
<li><dl>
<dt>Overview</dt><dd><ul>
<li><dl>
<dt>直觉理解即是越可以预知的model的已知来源于更高频率的访问</dt><dd><ul>
<li><dl>
<dt>而这样的model误差越小， the agent’s predictions of the output of a randomly initialized neural network will be less accurate in novel states than in states the agent visited frequently 因此RND用来增益Bonus的Novelty为 the error of predictive models</dt><dd><ul>
<li><dl>
<dt>Trick 1. 游戏结束持续奖励 Continued reward</dt><dd><ul class="simple">
<li><p>Intrinstic return does not goes to zero when episode ends. （IR ER分开训练，有灵活度选择不同的discount factor）</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>为什么要IR在episode结束时持续奖励，不持续奖励会带来什么问题? 对intrinsic reward来说，不持续奖励在episode结束时奖励为0，会激励agent更加保守；对于extrinsic reward，agent可能趋向于在游戏探索到的可以得到Reward的区域累计奖励然后直接gg，且定义上来说ER即是根据环境规则定义的Reward，应该在episode结束时归0。</p>
</div>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Trick 2. Normalization</dt><dd><ul class="simple">
<li><p>The intrinsic reward is normalized by division by a running estimate of the standard deviations of the intrinsic return.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>理论上RND 有助于解决稀疏reward空间的问题。</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Algorithm</dt><dd><ul>
<li><dl class="simple">
<dt>前期分析 - Sources related to noisy TV problem OpenAI定义了一下三个prediction errors的影响来源：</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Prediction error is high due to：</dt><dd><ul>
<li><p>Factor 1: Novel experience</p></li>
<li><p>Factor 2: Stochastic prediction targets</p></li>
<li><p>Factor 3: Information necessary for the prediction is missing or target functions too complex for predictors</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>且认为Factor 1 是有助于探索的因为novelty的必要性且量化了novelty，而2和3使智能体无法区别正常reward和类noisy-TV reward，因此设计了RND w/a new exploration bonus that is based on predicting the output of a fixed and randomly initialized neural network on the next state, given the next state itself.</p>
</li>
<li><p>Design (how to avoid Factor 2 and 3)</p>
<blockquote>
<div><img alt="../../_images/nextstate-vs-rnd-stacked-5.svg" src="../../_images/nextstate-vs-rnd-stacked-5.svg" /><ul class="simple">
<li><p>To avoid 2，就要让神经网络给出确定性的答案，而不是给出多个答案和它们各自的可能性；</p></li>
<li><p>To avoid 3， Target 和 Train使用同样的nn 结构</p></li>
</ul>
</div></blockquote>
</li>
<li><dl>
<dt>RND pseudo (Combined w/ PPO)</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>IR 的产生，对于分布式的训练架构更适合实现在什么位置？</dt><dd><ul>
<li><p>产生：对于每个episode K的iteration都calculate intrinsic reward，并且用于optimization 当前的batch</p></li>
<li><p>位置：在actor收集经验的位置实现</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="../../_images/RND_pseudo_code.png" src="../../_images/RND_pseudo_code.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Prediction Network如何训练，在什么时候，用什么数据？首先，产生一个随机生成但确定的 target network 和一个被智能体收集数据集训练的a predictor network，然后对每个观测的状态计算MSE，在这个过程中，随机网络像训练网络蒸馏，而在新颖度更高的地方有更高的prediction error。</p>
</div>
<ul class="simple">
<li></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Representation &amp; Manipulation of Exploration （内部算法为PPO）</dt><dd><ul>
<li><p>将IR和ER标准化后相加得到奖励</p>
<blockquote>
<div><img alt="../../_images/ext.svg" src="../../_images/ext.svg" /></div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Environment</dt><dd><ul class="simple">
<li><p>MontezumaRevenge</p></li>
<li><p>RND + PPO</p></li>
<li><p><a class="reference external" href="https://github.com/openai/random-network-distillation">开源</a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Direct Exploration  approach 1. 分离return 和explore</dt><dd><ul>
<li><dl>
<dt>Go Explore <a class="reference external" href="https://arxiv.org/abs/1901.10995">Paper1</a> <a class="reference external" href="https://arxiv.org/pdf/2004.12919.pdf">Paper2</a></dt><dd><ul>
<li><dl class="simple">
<dt>Overview</dt><dd><ul class="simple">
<li><p>GE算法区别于之前的探索强化学习算法最大的特点即是把returning 和 exploring 更大限度的分开——先形成一个return policy再在此基础上explore。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Algorithm</dt><dd><ul>
<li><dl>
<dt>回到下图。在return policy (Go phase 或者 phase 1）阶段在存档中guided by heuristic地选择一个和promising cells关联的state 以minimized explore降低了return failure，然后在下一个阶段(explore phase 或者 phase 2）采用purely exploratory policy，即随机采样或者从另一个policy中采样.</dt><dd><ul>
<li><dl>
<dt>步骤如下</dt><dd><ul class="simple">
<li><p>选择state：用概率方法在state set存档中取s, guided by heuristics 以更倾向于选择和promising cells关联的s.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>这里的heuristic更倾向于如何选择s呢？1. 被更少被访问过以及经验访问的return是好的 2.附近有更多未被探索过的s 3.是高level的s；</p>
</div>
<ul>
<li><dl class="simple">
<dt>从初始状态开始前往选择的s</dt><dd><ul class="simple">
<li><p>按照存档中的行动序列行动即可，轨迹中途经的各个状态也会被记录下来，如果要前往这些状态也可以复用前半部分的轨迹。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>从当前状态出发</dt><dd><ul class="simple">
<li><p>Explore from that state by taking random actions or sampling from a policy 做K步的随机行动采样，保持大概率重复上一步。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>存储</dt><dd><ul class="simple">
<li><p>Map every state encountered during returning and exploring to a low-dimensional cell representation.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>更新</dt><dd><ul class="simple">
<li><p>Add states that map to new cells to the archive and update other archive entries.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>是每次迭代都会更新吗？以下两种情况会有更新存档的必要：1.到达了新的s 2.到达了存档中的s，而走到该状态的路径更优于存档中已有的路径。因此我们知道在上一步不仅需要记录Corresponding reward on path of reaching the each state,也需要记录path的长度，用于这一步的更新。</p>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="../../_images/policy-based-Go-Explore.png" src="../../_images/policy-based-Go-Explore.png" />
<ul class="simple">
<li><p>GE算法的一个结果是两个状态之间变化轨迹更新，如果途径状态不同那么会保存新的轨迹而不会抛弃旧的轨迹。在文章中总结为维护存档。</p></li>
</ul>
<img alt="../../_images/ge_detachment.png" src="../../_images/ge_detachment.png" />
<img alt="../../_images/ge_overview.png" src="../../_images/ge_overview.png" />
<ul class="simple">
<li><p>总结Go explore 算法Key Principal： (1) remember good exploration stepping stones, (2) first return to a state, then explore and, (3) first solve a problem, then robustify (if necessary).</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Representation &amp; Manipulation of Exploration</dt><dd><ul class="simple">
<li><p>总结Go explore 算法对状态处理的特点：把状态表达为低维度的cell以及domain knowledge的引入，前者的实现为对基本状态的灰度化和低像素化后存储，而后者表现为在上面提取的状态的基础上还使用编写的程序从图中提取一些与游戏相关的信息作为状态特征，比如智能体的x, y坐标，当前房间的位置，当前处于的关卡数等。</p></li>
</ul>
<img alt="../../_images/ge_grayscale.png" src="../../_images/ge_grayscale.png" />
</dd>
</dl>
</li>
<li><dl>
<dt>Results</dt><dd><ul class="simple">
<li><p>不使用 domain knowledge 的结果就超越了人类专家的水平。使用了专家知识之后，效果提升。</p></li>
</ul>
<img alt="../../_images/ge_mr_result.png" src="../../_images/ge_mr_result.png" />
<img alt="../../_images/ge_domain.png" src="../../_images/ge_domain.png" />
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Environment</dt><dd><ul class="simple">
<li><p>Montezuma’s Revenge</p></li>
<li><p>Pitfall (此处不具体展示）</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Direct Exploration  approach 2. Reward-Free Exploration / 对纯粹充分探索的尝试</dt><dd><ul>
<li><dl>
<dt><a class="reference external" href="https://arxiv.org/abs/2002.02794">Reward-Free Exploration</a></dt><dd><ul>
<li><dl class="simple">
<dt>Overview</dt><dd><ul class="simple">
<li><p>直观理解，用无奖励环境探索（在状态空间上纯探索），得到探索策略后进行采样，之后对采样的数据进行近似（对于转化矩阵）并求解.</p></li>
<li><p>适用于对智能体行为有较理想预期的强化学习，只需考虑Reward Function的设计使智能体达到预期.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Algorithm</dt><dd><ul>
<li><dl>
<dt>Protocol 1 i.e. Guide of Phase 1 - Exploration</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>a reward-free version of the MDP</dt><dd><ul>
<li><p>the agent collects a dataset ‘D’ of visisted states, actions, and transitions</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<img alt="../../_images/D.png" src="../../_images/D.png" />
<img alt="../../_images/rf_p1.png" src="../../_images/rf_p1.png" />
<p>第一个For Loop完成了在状态空间的无奖励探索，得到了Policy Set（使用针对每个状态的奖励函数，即每个奖励函数对于非关联状态的奖励为0，然后用内部算法计算出目标Policy Set）；第二个For Loop则是由Policy Set得到D的过程.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Phase 2 - Planning</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Compute Optimal Policy and measure with value function</dt><dd><ul>
<li><p>Using reward function  r(·, ·) , compute an optimal policy using the dataset D.</p></li>
<li><p>Using value function V (·; r), evaluate with # of episodes K required in the phase1 for such optimal.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Algo + pseudo code of RF Explore + Plan</p>
<blockquote>
<div><img alt="../../_images/rf_algo.png" src="../../_images/rf_algo.png" />
<img alt="../../_images/rf_exploration.png" src="../../_images/rf_exploration.png" />
<img alt="../../_images/rf_planning.png" src="../../_images/rf_planning.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>文章使用用 Euler 是因为该算法得到的策略集的性能和最优价值函数的大小有关，即，如果这个最优价值函数本身就不特别好（比如，即使最优策略也访问不到那些 reward 比较大但是 insignificant 的状态），那么得到的策略和最优价值函数的差距也不会特别大。 （Lemma 3.4 in Paper, also proved in Lemma 3.3 that this algorithm is applicable that it can sufficiently explore all arbitrary significant states) 而文章后来提到的MaxEnt (uniformly explore all states including insignificants) 等因为没有提出insignificant cases效果不如RF</p>
</div>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Representation &amp; Manipulation &amp; Env（理论研究）</dt><dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>这是一个理论研究论文，因而策略集的性能和复杂度没有实验支持。了解 reward function  r(·, ·)在phase1的状态空间的无奖励探索Loop中被用到，而value function V (·; r)是作为一个量化衡量性能的工具（不在算法内部）即可。</p>
</div>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Direct Exploration  approach 3. MaxRényi / 对纯粹充分探索的尝试</dt><dd><ul>
<li><dl>
<dt><a class="reference external" href="https://arxiv.org/abs/2002.02794">MaxRényi Exploration</a></dt><dd><ul>
<li><dl class="simple">
<dt><a class="reference external" href="https://zhuanlan.zhihu.com/p/350594029">Overview</a> （&lt;– 作者写的专栏）</dt><dd><ul class="simple">
<li><p>基于Direct Explore第二部分RF 框架的实践</p></li>
<li><p>从鼓励探索上来说，以前的很多方法都采用了最大化熵的正则方法来鼓励探索。本文作者提出Renyi 熵函数（一种广义熵函数）会得到更好的探索效果。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Algorithm</dt><dd><ul>
<li><dl>
<dt>Design （内部函数依然是PPO）</dt><dd><ul>
<li><ol class="arabic simple">
<li><p>Adopted RF framwork</p></li>
</ol>
<blockquote>
<div><img alt="../../_images/me_framework.png" src="../../_images/me_framework.png" />
</div></blockquote>
</li>
<li><ol class="arabic simple" start="2">
<li><p>The Choice of objective function</p></li>
</ol>
<blockquote>
<div><img alt="../../_images/of.png" src="../../_images/of.png" />
<ul>
<li><p>（1）选择优化state-action distribution 的函数，而不只是 state distribution 的函数。原因是奖励函数一般是基于 state-action pair 定义的，因此我们希望鼓励数据集去访问各种不同的 state-action pair，而不仅仅是访问不同的状态。这里举了一个例子，说明即使最大化 state distribution 的熵函数，也可能使得某些 state-action pair 完全访问不到。</p>
<blockquote>
<div><img alt="../../_images/sa.png" src="../../_images/sa.png" />
</div></blockquote>
</li>
<li><p>（2） MDP只要给定一个包含所有 state-action pair 的数据集，就一定能够成功离线规划得到最优的策略。因此，目标函数的选择就有了一个完全量化可以写出的形式，即希望产生的策略能够以最少的采样次数获得这样完整的数据集。</p>
<blockquote>
<div><img alt="../../_images/intractable.png" src="../../_images/intractable.png" />
<p>(然而是intractable的）Renyi 是对以上形式的近似，必要共同特征是在边缘都有类似 barrier 的结构，这样的结构能够促使智能体去更多地访问那些很难被访问到的 state-action pair.
.. image:: images/approx.png</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Algo pseudo code (w/ PPO as value function)</p>
<blockquote>
<div><img alt="../../_images/ppo_renyi.png" src="../../_images/ppo_renyi.png" />
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Representation &amp; Manipulation &amp; Env</dt><dd><ul class="simple">
<li><p>Exp. w/ MiniGrid - Multirooms</p></li>
</ul>
<img alt="../../_images/multi.png" src="../../_images/multi.png" />
<ul class="simple">
<li><p>Exp. w/ Montezuma’s Revenge</p></li>
</ul>
<img alt="../../_images/mr_renyi.png" src="../../_images/mr_renyi.png" />
<img alt="../../_images/renyimr1.png" src="../../_images/renyimr1.png" />
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="integration-with-drl-methods">
<h2>4. Integration with DRL methods<a class="headerlink" href="#integration-with-drl-methods" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>ICM</dt><dd><ul>
<li><p>&amp; DQN</p></li>
<li><p>&amp; PPO</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="environments">
<h2>5. Environments<a class="headerlink" href="#environments" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>Can pip</dt><dd><ul>
<li><p>Toy Version Montezuma’s Revenge</p></li>
<li><p><a class="reference external" href="https://gym.openai.com/envs/MontezumaRevenge-v0/">MontezumaRevenge</a></p></li>
<li><p><a class="reference external" href="https://github.com/maximecb/gym-minigrid">Gym-MiniGrid</a></p></li>
<li><p><a class="reference external" href="https://github.com/mwydmuch/ViZDoom">VizDoom</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><p><a class="reference external" href="https://github.com/qqadssp/RandomMazeEnvironment">Noisy-TV (not official)</a></p></li>
<li><p>Noisy-TV (to be found)</p></li>
<li><p>Other Atari game with sparce rewards</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="references-remarks">
<h2>6. References / Remarks<a class="headerlink" href="#references-remarks" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>Resources:</dt><dd><ol class="arabic simple">
<li><p><a class="reference external" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_exploration.pdf">berkeley DRL17 L13 Exploration</a></p></li>
<li><p><a class="reference external" href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html">lilianweng blog: Exploration Strategies in DRL</a></p></li>
<li><p><a class="reference external" href="https://d4mucfpksywv.cloudfront.net/emergent-tool-use/paper/Multi_Agent_Emergence_2019.pdf">Open AI 捉迷藏</a></p></li>
</ol>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../game_env.html" class="btn btn-neutral float-right" title="强化学习常用的游戏环境/RL Game Environments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="multi-agent.html" class="btn btn-neutral float-left" title="Multi Agent Reinforcement Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, X-Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>