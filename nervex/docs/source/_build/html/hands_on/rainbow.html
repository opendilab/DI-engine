

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Rainbow &mdash; nerveX 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="A2C" href="a2c.html" />
    <link rel="prev" title="C51" href="c51_qrdqn_iqn.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nerveX
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Hands on RL</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dqn.html">DQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c51_qrdqn_iqn.html">C51</a></li>
<li class="toctree-l2"><a class="reference internal" href="c51_qrdqn_iqn.html#qrdqn">QRDQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="c51_qrdqn_iqn.html#iqn">IQN</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Rainbow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#double-q-learning">Double Q-learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prioritized-experience-replay-per">Prioritized Experience Replay(PER)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dueling-network">Dueling Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-step-learning">Multi-step Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#noisy-net">Noisy Net</a></li>
<li class="toctree-l3"><a class="reference internal" href="#extensions">Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="a2c.html">A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppo.html">PPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppg.html">PPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="impala.html">Impala</a></li>
<li class="toctree-l2"><a class="reference internal" href="ddpg.html">DDPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="td3.html">TD3</a></li>
<li class="toctree-l2"><a class="reference internal" href="sac.html">SAC</a></li>
<li class="toctree-l2"><a class="reference internal" href="qmix.html">QMIX</a></li>
<li class="toctree-l2"><a class="reference internal" href="coma.html">COMA</a></li>
<li class="toctree-l2"><a class="reference internal" href="atoc.html">ATOC</a></li>
<li class="toctree-l2"><a class="reference internal" href="collaq.html">CollaQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="vpn.html">VPN</a></li>
<li class="toctree-l2"><a class="reference internal" href="mcts.html">MCTS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supplementary_rl/index.html">Supplementary of RL</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nerveX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Hands on RL</a> &raquo;</li>
        
      <li>Rainbow</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/hands_on/rainbow.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rainbow">
<h1>Rainbow<a class="headerlink" href="#rainbow" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Rainbow was proposed in <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>. It combines many independent improvements to DQN, including: target network(double DQN), priority, dueling head, multi-step TD-loss, C51 and noisy net.</p>
</div>
<div class="section" id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Rainbow is a <strong>model-free</strong> and <strong>value-based</strong> RL algorithm.</p></li>
<li><p>Rainbow only support <strong>discrete action spaces</strong>.</p></li>
<li><p>Rainbow is an <strong>off-policy</strong> algorithm.</p></li>
<li><p>Usually, Rainbow use <strong>eps-greedy</strong>, <strong>multinomial sample</strong> or <strong>noisy net</strong> for exploration.</p></li>
<li><p>Rainbow can be equipped with RNN.</p></li>
<li><p>The nerveX implementation of Rainbow supports <strong>multi-discrete</strong> action space.</p></li>
</ol>
</div>
<div class="section" id="double-q-learning">
<h2>Double Q-learning<a class="headerlink" href="#double-q-learning" title="Permalink to this headline">¶</a></h2>
<p>Double Q-learning maintains a target q network, which is periodically updated with the current q network. Double Q-learning decouples the over-estimation of q-value by selects action with the current q network but estimate the q-value with the target network, formally:</p>
<a class="reference internal image-reference" href="../_images/double.png"><img alt="../_images/double.png" class="align-center" src="../_images/double.png" style="height: 40px;" /></a>
</div>
<div class="section" id="prioritized-experience-replay-per">
<h2>Prioritized Experience Replay(PER)<a class="headerlink" href="#prioritized-experience-replay-per" title="Permalink to this headline">¶</a></h2>
<p>DQN samples uniformly from the replay buffer. Ideally, we want to sample more frequently those transitions from which there is much to learn. As a proxy for learning potential, prioritized experience replay samples transitions with probability relative to the last encountered absolute TD error, formally:</p>
<a class="reference internal image-reference" href="../_images/priority.png"><img alt="../_images/priority.png" class="align-center" src="../_images/priority.png" style="height: 50px;" /></a>
<p>The original paper of PER, the authors show that PER achieve improvements on most of the 57 Atari games, especially on Gopher, Atlantis, James Bond 007, Space Invaders, etc.</p>
</div>
<div class="section" id="dueling-network">
<h2>Dueling Network<a class="headerlink" href="#dueling-network" title="Permalink to this headline">¶</a></h2>
<p>The dueling network is a neural network architecture designed for value based RL. It features two streams of computation, the value and advantage
streams, sharing a convolutional encoder, and merged by a special aggregator. This corresponds to the following factorization of action values:</p>
<a class="reference internal image-reference" href="../_images/dueling.png"><img alt="../_images/dueling.png" class="align-center" src="../_images/dueling.png" style="height: 80px;" /></a>
<p>The network architecture of Rainbow is a dueling network architecture adapted for use with return distributions. The network has a shared representation, which is then fed into a value stream <span class="math notranslate nohighlight">\(v_\eta\)</span> with <span class="math notranslate nohighlight">\(N_{atoms}\)</span> outputs, and into an advantage stream <span class="math notranslate nohighlight">\(a_{\psi}\)</span> with <span class="math notranslate nohighlight">\(N_{atoms} \times N_{actions}\)</span> outputs, where <span class="math notranslate nohighlight">\(a_{\psi}^i(a)\)</span> will denote the output corresponding to atom i and action a. For each atom <span class="math notranslate nohighlight">\(z_i\)</span>, the value and advantage streams are aggregated, as in dueling DQN, and then passed through a softmax layer to obtain the normalized parametric distributions used to estimate the returns’ distributions:</p>
<a class="reference internal image-reference" href="../_images/dueling_distribution.png"><img alt="../_images/dueling_distribution.png" class="align-center" src="../_images/dueling_distribution.png" style="height: 160px;" /></a>
</div>
<div class="section" id="multi-step-learning">
<h2>Multi-step Learning<a class="headerlink" href="#multi-step-learning" title="Permalink to this headline">¶</a></h2>
<p>A multi-step variant of DQN is then defined by minimizing the alternative loss:</p>
<a class="reference internal image-reference" href="../_images/nstep_td1.png"><img alt="../_images/nstep_td1.png" class="align-center" src="../_images/nstep_td1.png" style="height: 40px;" /></a>
<p>where the truncated n-step return is defined as:</p>
<a class="reference internal image-reference" href="../_images/nstep_reward.png"><img alt="../_images/nstep_reward.png" class="align-center" src="../_images/nstep_reward.png" style="height: 80px;" /></a>
<p>In the paper <a class="reference external" href="https://acsweb.ucsd.edu/~wfedus/pdf/replay.pdf">Revisiting Fundamentals of Experience Replay</a>, the authors analyze that a greater capacity of replay buffer substantially increase the performance when multi-step learning is used, and they think the reason is that multi-step learning brings larger variance, which is compensated by a larger replay buffer.</p>
</div>
<div class="section" id="noisy-net">
<h2>Noisy Net<a class="headerlink" href="#noisy-net" title="Permalink to this headline">¶</a></h2>
<p>Noisy Nets use a noisy linear layer that combines a deterministic and noisy stream:</p>
<a class="reference internal image-reference" href="../_images/noisy_net.png"><img alt="../_images/noisy_net.png" class="align-center" src="../_images/noisy_net.png" style="height: 40px;" /></a>
<p>Over time, the network can learn to ignore the noisy stream, but at different rates in different parts of the state space, allowing state-conditional exploration with a form of self-annealing. It usually achieves improvements against epsilon-greedy when the action space is large, e.g. Montezuma’s Revenge, because epsilon-greedy tends to quickly converge to a one-hot distribution before the rewards of the large numbers of actions are collected enough.
In our implementation, the noises are resampled before each forward both during data collection and training. When double Q-learning is used, the target network also resamples the noises before each forward. During the noise sampling, the nosies are first sampled form N(0,1), then their magnitudes are modulated via a sqrt function with their signs preserved, i.e. x -&gt; x.sign() * x.sqrt().</p>
</div>
<div class="section" id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Rainbow can be combined with:</dt><dd><ul class="simple">
<li><p>RNN</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<dl class="py class">
<dt>
<em class="property">class </em><code class="sig-prename descclassname">nervex.policy.rainbow.</code><code class="sig-name descname">RainbowDQNPolicy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cfg</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>type<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">enable_field</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nervex/policy/rainbow.html#RainbowDQNPolicy"><span class="viewcode-link">[source]</span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><dl class="simple">
<dt>Rainbow DQN contain several improvements upon DQN, including:</dt><dd><ul class="simple">
<li><p>target network</p></li>
<li><p>dueling architecture</p></li>
<li><p>prioritized experience replay</p></li>
<li><p>n_step return</p></li>
<li><p>noise net</p></li>
<li><p>distribution net</p></li>
</ul>
</dd>
</dl>
<p>Therefore, the RainbowDQNPolicy class inherit upon DQNPolicy class</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>rainbow</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.v_min</span></code></p></td>
<td><p>float</p></td>
<td><p>-10</p></td>
<td><div class="line-block">
<div class="line">Value of the smallest atom</div>
<div class="line">in the support set.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.v_max</span></code></p></td>
<td><p>float</p></td>
<td><p>10</p></td>
<td><div class="line-block">
<div class="line">Value of the largest atom</div>
<div class="line">in the support set.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.n_atom</span></code></p></td>
<td><p>int</p></td>
<td><p>51</p></td>
<td><div class="line-block">
<div class="line">Number of atoms in the support set</div>
<div class="line">of the value distribution.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">.start</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.05</p></td>
<td><div class="line-block">
<div class="line">Start value for epsilon decay. It’s</div>
<div class="line">small because rainbow use noisy net.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">.end</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.05</p></td>
<td><div class="line-block">
<div class="line">End value for epsilon decay.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.97,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">may be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>3,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>3</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>

<p>The network interface of Rainbow used is defined as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>TODO</p></li>
</ul>
</div></blockquote>
<p>The Benchmark result of Rainbow implemented in nerveX is shown in <a class="reference external" href="../feature/algorithm_overview.html">Benchmark</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="a2c.html" class="btn btn-neutral float-right" title="A2C" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="c51_qrdqn_iqn.html" class="btn btn-neutral float-left" title="C51" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, X-Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>