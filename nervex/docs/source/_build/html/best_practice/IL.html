

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Imitation Learning &mdash; nerveX 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inverse RL" href="IRL.html" />
    <link rel="prev" title="Log and Tensorboard" href="log.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nerveX
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Best Practice</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="nstep_td.html">N-step TD</a></li>
<li class="toctree-l2"><a class="reference internal" href="priority.html">How to Use PER(Prioritized Experience Replay)</a></li>
<li class="toctree-l2"><a class="reference internal" href="log.html">Log and Tensorboard</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Imitation Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#guideline">Guideline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#demo">Demo</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="IRL.html">Inverse RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnn.html">How to use RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="random_seed.html">Random seed</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_discrete.html">Multi-Discrete Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_wrapper.html">How to Customize Model Wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="env_wrapper.html">How to Customize an Env Wrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="customization1_dynamic_update_step.html">Customization 1: Dynamic Update Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_generated_folders.html">How to understand training generated folders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_buffer.html">How to use multiple buffers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="random_collect_size.html">How to randomly collect some data sample at the beginning?</a></li>
<li class="toctree-l2"><a class="reference internal" href="irregular_learner_log.html">How to correctly print irregular log?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supplementary_rl/index.html">Supplementary of RL</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nerveX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Best Practice</a> &raquo;</li>
        
      <li>Imitation Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/best_practice/IL.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="imitation-learning">
<h1>Imitation Learning<a class="headerlink" href="#imitation-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="guideline">
<h2>Guideline<a class="headerlink" href="#guideline" title="Permalink to this headline">¶</a></h2>
<p>In some environments where the rewards are sparse (e.g. a game where we
only receive a reward when the game is won or lost), the normal RL
approach can be very struggle. A feasible solution to this problem is
imitation learning (IL). In IL instead of trying to learn from the
sparse rewards or manually specifying a reward function, an expert
(typically a human) provides us with a set of demonstrations. The agent
then tries to learn the optimal policy by following, imitating the
expert’s decisions.</p>
<p>The simplest idea of imitation learning is behavioral cloning(BC). BC
can be described as the following steps:</p>
<ul class="simple">
<li><p>Collect demonstrations (<span class="math notranslate nohighlight">\(\tau^{*}\)</span> trajectories) from expert</p></li>
<li><p>Treat the demonstrations as i.i.d state-action pairs:
<span class="math notranslate nohighlight">\((s_0^*,a_0^*),(s_1^*,a_1^*),...\)</span></p></li>
<li><p>Learn <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> policy using supervised learning by
minimizing the loss function <span class="math notranslate nohighlight">\(L(a^*,\pi_{\theta}(s))\)</span></p></li>
</ul>
<p>Behavioral cloning can be quite problematic. The main reason for this is
the i.i.d. assumption: while supervised learning assumes that the
state-action pairs are distributed i.i.d., in MDP an action in a given
state induces the next state, which breaks the previous assumption. This
also means, that errors made in different states add up, therefore a
mistake made by the agent can easily put it into a state that the expert
has never visited and the agent has never trained on. In such states,
the behavior is undefined and this can lead to catastrophic failures.</p>
<p>For the majority of the cases, behavioral cloning can be quite
problematic. But due to the clarity of behavioral cloning, our demo of
imitation learning will be given in BC.</p>
</div>
<div class="section" id="demo">
<h2>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h2>
<p>You can use either expert model or demonstrations to perform imitation
learning. Usually you need to define an imitation learning policy. For
policy registration, you can refer to
<a class="reference external" href="../feature/policy_overview.html">policy Overview</a></p>
<p><strong>Use Demonstrations to IL</strong></p>
<p>NerveX provides serial entry for IL implementation. By specify the
prepared expert data <code class="docutils literal notranslate"><span class="pre">expert_data_path</span></code>, you can deploy IL by the
following codes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">converge_stop_flag</span> <span class="o">=</span> <span class="n">serial_pipeline_il</span><span class="p">(</span><span class="n">il_config</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">314</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">expert_data_path</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Use Expert Model to IL</strong></p>
<p>NerveX provides data collector functions in
<code class="docutils literal notranslate"><span class="pre">nervex/entry/application_entry.py</span></code>. You can give a policy configure
to train an RL model from scratch(or load an existing model), then use
the model to generate data for IL. This pipline can be describe as the
following codes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">expert_policy</span> <span class="o">=</span> <span class="n">serial_pipeline</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># collect expert demo data</span>
<span class="n">collect_count</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">expert_data_path</span> <span class="o">=</span> <span class="s1">&#39;expert_data.pkl&#39;</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">expert_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">collect_config</span> <span class="o">=</span> <span class="p">[</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">cartpole_ppo_config</span><span class="p">),</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">cartpole_ppo_create_config</span><span class="p">)]</span>
<span class="n">collect_demo_data</span><span class="p">(</span>
    <span class="n">collect_config</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">expert_data_path</span><span class="o">=</span><span class="n">expert_data_path</span><span class="p">,</span> <span class="n">collect_count</span><span class="o">=</span><span class="n">collect_count</span>
<span class="p">)</span>
<span class="c1"># il training</span>
<span class="n">_</span><span class="p">,</span> <span class="n">converge_stop_flag</span> <span class="o">=</span> <span class="n">serial_pipeline_il</span><span class="p">(</span><span class="n">il_config</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">314</span><span class="p">,</span> <span class="n">data_path</span><span class="o">=</span><span class="n">expert_data_path</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Online IL through Seiral Pipline</strong></p>
<p>NerveX’s <cite>serial_entry_il</cite> provides a sub-implementation of serial pipline,
in which there is no collectors (or use collectors only to collect data at the beginning
of training). However, many IL algorithms (Dagger, SQIL, etc.) need to collect demonstration
as well as training IL model. In this case, nerveX can use <cite>serial_entry</cite> to perform this
pipline. Users can define a new IL policy, the collect model of this policy is the expert
policy, and the learn model can be any supervised learning model or other IL learn model.
More details about the policy defination of nerveX can be found in
<a class="reference external" href="../feature/policy_overview.html">policy Overview</a>
NerveX also provide a demo of this policy in <cite>nervex/policy/il.py</cite>. It provides a supervised
learning pipline to imitate from an expert model online on Google Research football environments.</p>
<p>NerveX provide a full demo of using PPO as both the expert policy to
generate data and the IL policy to implement behavioral cloning. You can
refer to <code class="docutils literal notranslate"><span class="pre">nervex/entry/tests/test_serial_entry_il.py</span></code> for more
details.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="IRL.html" class="btn btn-neutral float-right" title="Inverse RL" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="log.html" class="btn btn-neutral float-left" title="Log and Tensorboard" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, X-Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>