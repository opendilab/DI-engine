

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quick Start &mdash; nerveX 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Concept" href="../key_concept/index.html" />
    <link rel="prev" title="Installation" href="../installation/index.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nerveX
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#config-and-entry">Config and entry</a></li>
<li class="toctree-l2"><a class="reference internal" href="#set-up-environments">Set up Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#set-up-policy-and-nn-model">Set up Policy and NN model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#set-up-execution-modules">Set up execution modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#train-and-evaluate-the-policy">Train and evaluate the policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-features">Advanced features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#epsilon-greedy">Epsilon Greedy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#visualization-logging">Visualization &amp; Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-saving-checkpoints">Loading &amp; Saving checkpoints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
<li class="toctree-l1"><a class="reference internal" href="../supplementary_rl/index.html">Supplementary of RL</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nerveX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Quick Start</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/quick_start/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quick-start">
<h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline">¶</a></h1>
<p>(Based on commit 0c3529)</p>
<div class="toctree-wrapper compound">
</div>
<p>Here we show how to easily deploy a Reinforcement Learning experiment on a simple <cite>CartPole</cite>
environment using nerveX.</p>
<p>NerveX provides config-wise and code-wise specifications to build RL experiments.
Both are commonly used by existing RL platforms. In this section we use the code-level
entry to clarify the training procedure and defined modules, with the hyperparameters
for training details and NN models pre-defined in a config file.</p>
<div class="section" id="config-and-entry">
<h2>Config and entry<a class="headerlink" href="#config-and-entry" title="Permalink to this headline">¶</a></h2>
<p>NerveX recommends using a config <cite>dict</cite> defined in a python file as input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cartpole_dqn_default_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">manager</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">collect</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">learn</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="nb">eval</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">other</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">replay_buffer</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
            <span class="o">...</span>
        <span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>A config file contains two main namespaces, <code class="docutils literal notranslate"><span class="pre">env</span></code> and <code class="docutils literal notranslate"><span class="pre">policy</span></code>. Some sub-namespace belong to certain modules in nerveX.
The module can be specialized defined by users or just use our pre-defined modules. Here is a example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nervex.config</span> <span class="kn">import</span> <span class="n">compile_config</span>
<span class="kn">from</span> <span class="nn">nervex.envs</span> <span class="kn">import</span> <span class="n">BaseEnvManager</span><span class="p">,</span> <span class="n">NervexEnvWrapper</span>
<span class="kn">from</span> <span class="nn">nervex.model</span> <span class="kn">import</span> <span class="n">DQN</span><span class="p">,</span> <span class="n">VAC</span>
<span class="kn">from</span> <span class="nn">nervex.policy</span> <span class="kn">import</span> <span class="n">DQNPolicy</span><span class="p">,</span> <span class="n">PPOPolicy</span>
<span class="kn">from</span> <span class="nn">nervex.worker</span> <span class="kn">import</span> <span class="n">BaseLearner</span><span class="p">,</span> <span class="n">SampleCollector</span><span class="p">,</span> <span class="n">BaseSerialEvaluator</span><span class="p">,</span> <span class="n">AdvancedReplayBuffer</span>
<span class="kn">from</span> <span class="nn">app_zoo.classic_control.cartpole.config.cartpole_dqn_config</span> <span class="kn">import</span> <span class="n">cartpole_dqn_config</span>
<span class="c1"># from app_zoo.classic_control.cartpole.config.cartpole_ppo_config import cartpole_ppo_config  # ppo config</span>

<span class="c1"># compile config</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">compile_config</span><span class="p">(</span>
    <span class="n">cartpole_dqn_config</span><span class="p">,</span>
    <span class="n">BaseEnvManager</span><span class="p">,</span>
    <span class="n">DQNPolicy</span><span class="p">,</span>
    <span class="n">BaseLearner</span><span class="p">,</span>
    <span class="n">SampleCollector</span><span class="p">,</span>
    <span class="n">BaseSerialEvaluator</span><span class="p">,</span>
    <span class="n">AdvancedReplayBuffer</span><span class="p">,</span>
    <span class="n">save_cfg</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the specific config example and how to construct config, you can refer to <code class="docutils literal notranslate"><span class="pre">app_zoo/classic_control/cartpole/config/cartpole_dqn_config.py</span></code> and <code class="docutils literal notranslate"><span class="pre">app_zoo/classic_control/cartpole/entry/cartpole_dqn_main.py</span></code></p>
</div>
<p>For more design details, please refer to the <a class="reference external" href="../key_concept/index.html#config">Config</a>.</p>
<p>When you are ready with config, you can you construct your RL training/evaluation entry program referring to the following guides step by step.</p>
</div>
<div class="section" id="set-up-environments">
<h2>Set up Environments<a class="headerlink" href="#set-up-environments" title="Permalink to this headline">¶</a></h2>
<p>NerveX redefines RL environment interfaces derived from the widely used <a class="reference external" href="https://github.com/openai/gym">OpenAI Gym</a>.
For junior users, an environment wrapper (by default <code class="xref py py-class docutils literal notranslate"><span class="pre">NervexEnvWrapper</span></code>) is provided
to simply wrap the gym env into NerveX form env.
For advanced users, it is suggested to check our <a class="reference external" href="../key_concept/index.html#env">Environment</a> doc for details</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Env</span> <span class="pre">Manager</span></code> is used to manage multiple environments, single-process serially
or multi-process parallelly. The interfaces of <cite>env manager</cite> are similar to those of a simple gym env. Here we show a case
of using <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEnvManager</span></code> to build environments for collection and evaluation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="k">def</span> <span class="nf">wrapped_cartpole_env</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">NervexEnvWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">))</span>

<span class="n">collector_env_num</span><span class="p">,</span> <span class="n">evaluator_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">collector_env_num</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">evaluator_env_num</span>
<span class="n">collector_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">collector_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
<span class="n">evaluator_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="set-up-policy-and-nn-model">
<h2>Set up Policy and NN model<a class="headerlink" href="#set-up-policy-and-nn-model" title="Permalink to this headline">¶</a></h2>
<p>NerveX supports most of the common policies used in RL training. Each is defined as a <code class="xref py py-class docutils literal notranslate"><span class="pre">Policy</span></code>
class. The details of optimiaztion algorithm, data pre-processing and post-processing, control of multiple networks
are encapsulated inside. Users only need to build a PyTorch network structure and pass into the policy.
NerveX also provides default networks to simply apply to the environment. For some complex RL methods, it is required to set some
properties (such as <cite>Actor</cite> and <cite>Critic</cite>) in your defined model.</p>
<p>For example, a <cite>DQN</cite> policy and <cite>PPO</cite> policy for CartPole can be defined as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">VAC</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">PPOPolicy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="set-up-execution-modules">
<h2>Set up execution modules<a class="headerlink" href="#set-up-execution-modules" title="Permalink to this headline">¶</a></h2>
<p>NerveX needs to build some execution components to manage an RL training procedure.
A <code class="xref py py-class docutils literal notranslate"><span class="pre">Collector</span></code> is used to sample and provide data for training.
A <code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code> is used to receive training data and conduct
the training (including updating networks, strategy and experience pool, etc.).
An <code class="xref py py-class docutils literal notranslate"><span class="pre">Evaluator</span></code> is build to perform the evaluation when needed.
And other components like <code class="xref py py-class docutils literal notranslate"><span class="pre">Replay</span> <span class="pre">Buffer</span></code> may be required for the
training process. All these module can be customized by config or rewritten by the user.</p>
<p>An example of setting up all the above is showed as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="n">tb_logger</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;./log/&#39;</span><span class="p">,</span> <span class="s1">&#39;your_experiment_name&#39;</span><span class="p">))</span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">BaseLearner</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">learner</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">learn_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">)</span>
<span class="n">collector</span> <span class="o">=</span> <span class="n">SampleCollector</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span> <span class="n">collector_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BaseSerialEvaluator</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">evaluator_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">)</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">AdvancedReplayBuffer</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="train-and-evaluate-the-policy">
<h2>Train and evaluate the policy<a class="headerlink" href="#train-and-evaluate-the-policy" title="Permalink to this headline">¶</a></h2>
<p>The training loop in nerveX can be customized arbitrarily. Usually the training process may consist of
collecting data, updating policy, updating related modules and evaluation.</p>
<p>Here we provide examples of off-policy training (<cite>DQN</cite>) and on-policy training (<cite>PPO</cite>) for a <cite>CartPole</cite>
environment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nervex.rl_utils</span> <span class="kn">import</span> <span class="n">get_epsilon_greedy_fn</span>

<span class="c1"># DQN training loop</span>
<span class="n">eps_cfg</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">eps</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="n">get_epsilon_greedy_fn</span><span class="p">(</span><span class="n">eps_cfg</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">end</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">decay</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">train_iter</span><span class="o">=</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">eps</span><span class="p">})</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">cur_collector_envstep</span><span class="o">=</span><span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">update_per_collect</span><span class="p">):</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PPO training loop</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">train_iter</span><span class="o">=</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">cur_collector_envstep</span><span class="o">=</span><span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">update_per_collect</span><span class="p">):</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The users can refer to the complete demo in <code class="docutils literal notranslate"><span class="pre">app_zoo/classic_control/cartpole/entry/cartpole_dqn_main.py</span></code> and <code class="docutils literal notranslate"><span class="pre">app_zoo/classic_control/cartpole/entry/cartpole_ppo_main.py</span></code> .</p>
</div>
</div>
<div class="section" id="advanced-features">
<h2>Advanced features<a class="headerlink" href="#advanced-features" title="Permalink to this headline">¶</a></h2>
<p>Some advanced features in RL training which well supported by nerveX are listed below.</p>
<div class="section" id="epsilon-greedy">
<h3>Epsilon Greedy<a class="headerlink" href="#epsilon-greedy" title="Permalink to this headline">¶</a></h3>
<p>An easy way of deploying epsilon greedy exploration when sampling data has already been shown above. It is
called by the <cite>epsilon_greedy</cite> function each step. And you can select your own decay strategy, such as envstep and train_iter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nervex.rl_utils</span> <span class="kn">import</span> <span class="n">get_epsilon_greedy_fn</span>

<span class="n">eps_cfg</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">eps</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="n">get_epsilon_greedy_fn</span><span class="p">(</span><span class="n">eps_cfg</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">end</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">decay</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="visualization-logging">
<h3>Visualization &amp; Logging<a class="headerlink" href="#visualization-logging" title="Permalink to this headline">¶</a></h3>
<p>Some environments have a renderd surface or visualization. NerveX adds a switch to save these replays.
After training, the users need to indicate <code class="docutils literal notranslate"><span class="pre">env.replay_path</span></code> in config and add the next two lines after creating environments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">evaluator_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">replay_path</span> <span class="o">=</span> <span class="s1">&#39;./video&#39;</span>
<span class="n">evaluator_env</span><span class="o">.</span><span class="n">enable_save_replay</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">replay_path</span><span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BaseSerialEvaluator</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">evaluator_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">)</span>
<span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<p>A simple demo for replaying CartPole Env evaluation is shown follow.</p>
<img alt="../_images/openaigym.video.gif" class="align-center" src="../_images/openaigym.video.gif" />
<p>Similar with other Deep Learning platforms, nerveX uses tensorboard to record key parameters and results during
training. In addition to the default logging parameters, users can add their own logging parameters as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tb_logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
</pre></div>
</div>
<p>If you want to know more details about default information recorded in tensorboard, see our
<a class="reference external" href="./tb_demo.html">tensorboard and logging demo</a> for a
DQN experiment.</p>
</div>
<div class="section" id="loading-saving-checkpoints">
<h3>Loading &amp; Saving checkpoints<a class="headerlink" href="#loading-saving-checkpoints" title="Permalink to this headline">¶</a></h3>
<p>It is usually needed to save and resume an experiments with model checkpoints. NerveX saves and loads checkpoints
in the same way as PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ckpt_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/your/ckpt&#39;</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="s1">&#39;last_iter&#39;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
    <span class="n">last_iter</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;last_iter&#39;</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">last_iter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">last_iter</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> load ckpt in </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">))</span>

<span class="o">...</span>

<span class="n">dirname</span> <span class="o">=</span> <span class="s1">&#39;./ckpt_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">exsit_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ckpt_name</span> <span class="o">=</span> <span class="s1">&#39;iteration_</span><span class="si">{}</span><span class="s1">.pth.tar&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">last_iter</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">ckpt_name</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;last_iter&#39;</span><span class="p">:</span> <span class="n">learner</span><span class="o">.</span><span class="n">last_iter</span><span class="o">.</span><span class="n">val</span><span class="p">})</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> save ckpt in </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">path</span><span class="p">))</span>
</pre></div>
</div>
<p>To deploy this in a more elegant way, nerveX is configured to use
<a class="reference internal" href="../api_doc/worker/learner/learner.html#nervex.worker.learner.learner_hook.LearnerHook" title="nervex.worker.learner.learner_hook.LearnerHook"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span> <span class="pre">Hooks</span></code></a> to handle these cases. The saving hook is
automatically frequently called after training iterations. And to load &amp; save checkpoints at the beginning and
in the end, users can simply add one line code before &amp; after training as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">call_hook</span><span class="p">(</span><span class="s1">&#39;before_run&#39;</span><span class="p">)</span>

<span class="c1"># training loop</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="o">...</span>

<span class="n">learner</span><span class="o">.</span><span class="n">call_hook</span><span class="p">(</span><span class="s1">&#39;after_run&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, please take a look to <a class="reference external" href="../feature/wrapper_hook_overview.html">Wrapper &amp; Hook Overview</a> doc.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../key_concept/index.html" class="btn btn-neutral float-right" title="Key Concept" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../installation/index.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, X-Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>