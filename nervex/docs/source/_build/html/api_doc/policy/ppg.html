

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>PPG &mdash; nerveX 0.2.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Model" href="../model/index.html" />
    <link rel="prev" title="CollaQ" href="collaq.html" />
    <link href="../../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> nerveX
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Doc</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../config/index.html">Config</a></li>
<li class="toctree-l2"><a class="reference internal" href="../env/index.html">Env</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Policy</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dqn.html">DQN</a></li>
<li class="toctree-l3"><a class="reference internal" href="rainbow.html">Rainbow</a></li>
<li class="toctree-l3"><a class="reference internal" href="r2d2.html">R2D2</a></li>
<li class="toctree-l3"><a class="reference internal" href="a2c.html">A2C</a></li>
<li class="toctree-l3"><a class="reference internal" href="ddpg.html">DDPG</a></li>
<li class="toctree-l3"><a class="reference internal" href="qmix.html">QMIX</a></li>
<li class="toctree-l3"><a class="reference internal" href="coma.html">COMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="collaq.html">CollaQ</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">PPG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ppgpolicy">PPGPolicy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model/index.html">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reward_model/index.html">Reward Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../league/index.html">League</a></li>
<li class="toctree-l2"><a class="reference internal" href="../worker/replay_buffer/index.html">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torch_utils/index.html">Torch Utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utils/index.html">Utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../interaction/index.html">Interaction</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../feature/index.html">Feature</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../supplementary_rl/index.html">Supplementary of RL</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorial_dev/index.html">Tutorial-Developer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">nerveX</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">API Doc</a> &raquo;</li>
        
          <li><a href="index.html">Policy</a> &raquo;</li>
        
      <li>PPG</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_doc/policy/ppg.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="ppg">
<h1>PPG<a class="headerlink" href="#ppg" title="Permalink to this headline">¶</a></h1>
<div class="section" id="ppgpolicy">
<h2>PPGPolicy<a class="headerlink" href="#ppgpolicy" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="nervex.policy.ppg.PPGPolicy">
<em class="property">class </em><code class="sig-prename descclassname">nervex.policy.ppg.</code><code class="sig-name descname">PPGPolicy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cfg</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>type<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">enable_field</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of PPG algorithm.</p>
</dd>
<dt>Interface:</dt><dd><p>_init_learn, _data_preprocess_learn, _forward_learn, _state_dict_learn, _load_state_dict_learn            _init_collect, _forward_collect, _process_transition, _get_train_sample, _get_batch_size, _init_eval,            _forward_eval, default_model, _monitor_vars_learn, learn_aux</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>ppg</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><ol class="arabic simple" start="4">
<li></li>
</ol>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">IS_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling</div>
<div class="line">Weight to correct biased update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">IS weight</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>5</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.value_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">weight</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><div class="line-block">
<div class="line">The loss weight of value network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">policy network weight</div>
<div class="line">is set to 1</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.entropy_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">weight</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.01</p></td>
<td><div class="line-block">
<div class="line">The loss weight of entropy</div>
<div class="line">regularization</div>
</div>
</td>
<td><div class="line-block">
<div class="line">policy network weight</div>
<div class="line">is set to 1</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.clip_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ratio</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.2</p></td>
<td><div class="line-block">
<div class="line">PPO clip ratio</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.adv_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">norm</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use advantage norm in</div>
<div class="line">a whole training batch</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.aux_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>5</p></td>
<td><div class="line-block">
<div class="line">The frequency(normal update times)</div>
<div class="line">of auxiliary phase training</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.aux_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">train_epoch</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>6</p></td>
<td><div class="line-block">
<div class="line">The training epochs of auxiliary</div>
<div class="line">phase</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.aux_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">bc_weight</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">The loss weight of behavioral_cloning</div>
<div class="line">in auxiliary phase</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.dis</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">count_factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.99</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">may be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.gae_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">lambda</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.95</p></td>
<td><div class="line-block">
<div class="line">GAE lambda factor for the balance</div>
<div class="line">of bias and variance(1-step td and mc)</div>
</div>
</td>
<td></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._data_preprocess_learn">
<code class="sig-name descname">_data_preprocess_learn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Any<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._data_preprocess_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._data_preprocess_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Preprocess the data to fit the required data format for learning, including             collate(stack data into batch), ignore done(in some fake terminate env),            prepare loss weight per training sample, and cpu tensor to cuda.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): the data collected from collect function</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the processed data, including at least [‘done’, ‘weight’]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._forward_collect">
<code class="sig-name descname">_forward_collect</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">dict</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._forward_collect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward function for collect mode</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, including at least [‘obs’].</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): The collected data</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._forward_eval">
<code class="sig-name descname">_forward_eval</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">dict</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._forward_eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward function for eval mode, similar to <code class="docutils literal notranslate"><span class="pre">self._forward_collect</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, including at least [‘obs’].</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, including at least inferred action according to input obs.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._forward_learn">
<code class="sig-name descname">_forward_learn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">dict</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._forward_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Forward and backward function of learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): Dict type data, a batch of data for training, values are torch.Tensor or                 np.ndarray or dict/list combinations.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): Dict type data, a info dict indicated training result, which will be                 recorded in text log and tensorboard, values are python scalar or a list of scalars.</p></li>
</ul>
</dd>
<dt>ArgumentsKeys:</dt><dd><ul class="simple">
<li><p>necessary: ‘obs’, ‘logit’, ‘action’, ‘value’, ‘reward’, ‘done’</p></li>
</ul>
</dd>
<dt>ReturnsKeys:</dt><dd><ul>
<li><p>necessary: current lr, total_loss, policy_loss, value_loss, entropy_loss,                         adv_abs_max, approx_kl, clipfrac                        aux_value_loss, auxiliary_loss, behavioral_cloning_loss</p>
<blockquote>
<div><ul class="simple">
<li><p>current_lr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Current learning rate</p></li>
<li><p>total_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The calculated loss</p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The policy(actor) loss of ppg</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The value(critic) loss of ppg</p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The entropy loss</p></li>
<li><p>auxiliary_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The auxiliary loss, we use the value function loss                     as the auxiliary objective, thereby sharing features between the policy and value function                    while minimizing distortions to the policy</p></li>
<li><p>aux_value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The auxiliary value loss, we need to train the value network extra                     during the auxiliary phase, it’s the value loss we train the value network during auxiliary phase</p></li>
<li><p>behavioral_cloning_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The behavioral cloning loss, used to optimize the auxiliary                     objective while otherwise preserving the original policy</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._get_batch_size">
<code class="sig-name descname">_get_batch_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._get_batch_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._get_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get learn batch size. In the PPG algorithm, different networks require different data.            We need to get data[‘policy’] and data[‘value’] to train policy net and value net,            this function is used to get the batch size of data[‘policy’] and data[‘value’].</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict[str,</span> <span class="pre">int]</span></code>): Dict type data, including str type batch size and int type batch size.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._get_train_sample">
<code class="sig-name descname">_get_train_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">collections.deque</span></em><span class="sig-paren">)</span> &#x2192; Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._get_train_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the trajectory and calculate GAE, return one data to cache for next time calculation</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): The trajectory’s cache</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): The training samples generated</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._init_collect">
<code class="sig-name descname">_init_collect</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._init_collect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Collect mode init method. Called by <code class="docutils literal notranslate"><span class="pre">self.__init__</span></code>.
Init unroll length, adder, collect model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._init_eval">
<code class="sig-name descname">_init_eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._init_eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Evaluate mode init method. Called by <code class="docutils literal notranslate"><span class="pre">self.__init__</span></code>.
Init eval model with argmax strategy.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._init_learn">
<code class="sig-name descname">_init_learn</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._init_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Learn mode init method. Called by <code class="docutils literal notranslate"><span class="pre">self.__init__</span></code>.
Init the optimizer, algorithm config and the main model.</p>
</dd>
<dt>Arguments:</dt><dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The _init_learn method takes the argument from the self._cfg.learn in the config file</p>
</div>
<ul class="simple">
<li><p>learning_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The learning rate fo the optimizer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._load_state_dict_learn">
<code class="sig-name descname">_load_state_dict_learn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._load_state_dict_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of policy learn state saved before.                When the value is distilled into the policy network, we need to make sure the policy                 network does not change the action predictions, we need two optimizers,                 _optimizer_ac is used in policy net, and _optimizer_aux_critic is used in value net.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">nervex.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._monitor_vars_learn">
<code class="sig-name descname">_monitor_vars_learn</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._monitor_vars_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return variables’ name if variables are to used in monitor.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>vars (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): Variables’ name list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._process_transition">
<code class="sig-name descname">_process_transition</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">model_output</span><span class="p">:</span> <span class="n">dict</span></em>, <em class="sig-param"><span class="n">timestep</span><span class="p">:</span> <span class="n">collections.namedtuple</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._process_transition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generate dict type transition data from inputs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): Env observation</p></li>
<li><p>model_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Output of collect model, including at least [‘action’]</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): Output after env step, including at least [‘obs’, ‘reward’, ‘done’]                       (here ‘obs’ indicates obs after env step).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type transition data.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy._state_dict_learn">
<code class="sig-name descname">_state_dict_learn</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy._state_dict_learn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy.default_model">
<code class="sig-name descname">default_model</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy.default_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default model setting for demostration.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): model name and mode import_names</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="nervex.policy.ppg.PPGPolicy.learn_aux">
<code class="sig-name descname">learn_aux</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../../_modules/nervex/policy/ppg.html#PPGPolicy.learn_aux"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nervex.policy.ppg.PPGPolicy.learn_aux" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The auxiliary phase training, where the value is distilled into the policy network</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>aux_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor,</span> <span class="pre">torch.Tensor]</span></code>): including average auxiliary loss                average behavioral cloning loss, and average auxiliary value loss</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../model/index.html" class="btn btn-neutral float-right" title="Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="collaq.html" class="btn btn-neutral float-left" title="CollaQ" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, X-Lab.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>