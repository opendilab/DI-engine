common:
    save_path: '.'
env:
    screen_resolution: [128, 128]
    default_step_mul: 8  # not used
    game_steps_per_episode: 30
    disable_fog: False
    realtime: False  # realtime mode, incompatible with action_delays
    obs_stat_type: 'self_online'  # self_online, replay_online, replay_last
    pseudo_reward_type: 'global'  # global, immediate
    pseudo_reward_prob: 0.25
    ignore_camera: True
    begin_num: 20
    crop_map_to_playable_area: False
    action_delays: [0.5, 0.25, 0.25]  # list of probablities of delay from 1 to n or null for delay=1
    map_name_list: ['KairosJunction']
# for testing actor worker
train:
    trajectory_len: 16
actor:
    use_cuda: False
    temperature: 0.8
    heartbeats_thread: True
    heartbeats_freq: 10
    max_connection_retries: 20
    print_freq: 4
    load_model_freq: 4
coordinator:
    use_fake_data: True
    fake_model_path: do_not_load
    fake_stat_path: do_not_load

# values in this section is mean to be shared by multiple components
system:
    resume_dir: '.'    # placeholder, it's set by coordinator_start.py
    learner_port: 18293               # learner port
    learner_heartbeats_thread: False  # if use thread to send heartbeats to coordinator in learner
    learner_heartbeats_freq: 10       # send heartbeats each `freq`
    coordinator_ip: 10.198.6.31        # learner and coordinator are in same cluster
    coordinator_port: 18294
    coordinator_check_dead_learner_freq: 300
    coordinator_resume_path: 
    # coordinator_resume_path: './api-log/coordinator.resume.2020-05-04-23-18-31'
    # manager_ip can be set to 'auto' or the real IP of the manager instance
    # this value is used by actors to find what they should connect and report to
    # if it's set to 'auto', the manager node of the Slurm cluster will be used as the manager
    manager_ip: 'auto'
    manager_port: 18295
    manager_resume_path: 
    league_manager_port: 18296
    auto_run_actor: False
    use_partitions: ['VI_SP_Y_V100_A', 'VI_SP_Y_V100_B']
    actor_num: [2, 2]
    use_ceph: True  # TODO: there is no support for lustre file on the actor side
    # The paths must end with a /
    ceph_traj_path: 's3://alphastar_fake_data/'
    ceph_model_path: 's3://alphastar_fake_data/'
    ceph_stat_path: 's3://alphastar_fake_data/'

replay_buffer:
    meta_maxlen: 4096
    max_reuse: 2
    min_sample_ratio: 1
    alpha: 0.6
    beta: 0.5
    cache_maxlen: 2
    timeout: 8  # times of the seconds of per learning iteration
